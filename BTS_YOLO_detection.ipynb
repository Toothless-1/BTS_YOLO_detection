{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10287488,"sourceType":"datasetVersion","datasetId":6366491}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport os\nimport PIL\nimport skimage\nfrom skimage import io\nimport numpy as np\nfrom PIL import Image\nimport shutil \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms.functional as FT\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchinfo import summary\nimport copy\nimport datetime\nimport random\nimport traceback\nfrom IPython.display import display, clear_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nseed = 42\nimport cv2\nimport xml.etree.ElementTree as ET\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:01:46.393588Z","iopub.execute_input":"2024-12-24T20:01:46.393954Z","iopub.status.idle":"2024-12-24T20:01:52.015679Z","shell.execute_reply.started":"2024-12-24T20:01:46.393907Z","shell.execute_reply":"2024-12-24T20:01:52.014777Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torchvision import models\nresnet50_model = models.resnet50(pretrained=True)\nfor param in resnet50_model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:01:52.017316Z","iopub.execute_input":"2024-12-24T20:01:52.017831Z","iopub.status.idle":"2024-12-24T20:01:53.146084Z","shell.execute_reply.started":"2024-12-24T20:01:52.017795Z","shell.execute_reply":"2024-12-24T20:01:53.145245Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 224MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"summary(resnet50_model, input_size=(16, 3, 448, 448))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:01:53.147204Z","iopub.execute_input":"2024-12-24T20:01:53.147613Z","iopub.status.idle":"2024-12-24T20:01:54.163371Z","shell.execute_reply.started":"2024-12-24T20:01:53.147574Z","shell.execute_reply":"2024-12-24T20:01:54.162459Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet                                   [16, 1000]                --\n├─Conv2d: 1-1                            [16, 64, 224, 224]        9,408\n├─BatchNorm2d: 1-2                       [16, 64, 224, 224]        128\n├─ReLU: 1-3                              [16, 64, 224, 224]        --\n├─MaxPool2d: 1-4                         [16, 64, 112, 112]        --\n├─Sequential: 1-5                        [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-1                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-1                  [16, 64, 112, 112]        4,096\n│    │    └─BatchNorm2d: 3-2             [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-3                    [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-4                  [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-5             [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-6                    [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-7                  [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-8             [16, 256, 112, 112]       512\n│    │    └─Sequential: 3-9              [16, 256, 112, 112]       16,896\n│    │    └─ReLU: 3-10                   [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-2                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-11                 [16, 64, 112, 112]        16,384\n│    │    └─BatchNorm2d: 3-12            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-13                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-14                 [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-15            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-16                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-17                 [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-18            [16, 256, 112, 112]       512\n│    │    └─ReLU: 3-19                   [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-3                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-20                 [16, 64, 112, 112]        16,384\n│    │    └─BatchNorm2d: 3-21            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-22                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-23                 [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-24            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-25                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-26                 [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-27            [16, 256, 112, 112]       512\n│    │    └─ReLU: 3-28                   [16, 256, 112, 112]       --\n├─Sequential: 1-6                        [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-4                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-29                 [16, 128, 112, 112]       32,768\n│    │    └─BatchNorm2d: 3-30            [16, 128, 112, 112]       256\n│    │    └─ReLU: 3-31                   [16, 128, 112, 112]       --\n│    │    └─Conv2d: 3-32                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-33            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-34                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-35                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-36            [16, 512, 56, 56]         1,024\n│    │    └─Sequential: 3-37             [16, 512, 56, 56]         132,096\n│    │    └─ReLU: 3-38                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-5                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-39                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-40            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-41                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-42                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-43            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-44                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-45                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-46            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-47                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-6                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-48                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-49            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-50                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-51                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-52            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-53                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-54                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-55            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-56                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-7                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-57                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-58            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-59                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-60                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-61            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-62                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-63                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-64            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-65                   [16, 512, 56, 56]         --\n├─Sequential: 1-7                        [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-8                   [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-66                 [16, 256, 56, 56]         131,072\n│    │    └─BatchNorm2d: 3-67            [16, 256, 56, 56]         512\n│    │    └─ReLU: 3-68                   [16, 256, 56, 56]         --\n│    │    └─Conv2d: 3-69                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-70            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-71                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-72                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-73            [16, 1024, 28, 28]        2,048\n│    │    └─Sequential: 3-74             [16, 1024, 28, 28]        526,336\n│    │    └─ReLU: 3-75                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-9                   [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-76                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-77            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-78                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-79                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-80            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-81                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-82                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-83            [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-84                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-10                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-85                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-86            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-87                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-88                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-89            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-90                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-91                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-92            [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-93                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-11                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-94                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-95            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-96                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-97                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-98            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-99                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-100                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-101           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-102                  [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-12                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-103                [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-104           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-105                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-106                [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-107           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-108                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-109                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-110           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-111                  [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-13                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-112                [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-113           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-114                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-115                [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-116           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-117                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-118                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-119           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-120                  [16, 1024, 28, 28]        --\n├─Sequential: 1-8                        [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-14                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-121                [16, 512, 28, 28]         524,288\n│    │    └─BatchNorm2d: 3-122           [16, 512, 28, 28]         1,024\n│    │    └─ReLU: 3-123                  [16, 512, 28, 28]         --\n│    │    └─Conv2d: 3-124                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-125           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-126                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-127                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-128           [16, 2048, 14, 14]        4,096\n│    │    └─Sequential: 3-129            [16, 2048, 14, 14]        2,101,248\n│    │    └─ReLU: 3-130                  [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-15                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-131                [16, 512, 14, 14]         1,048,576\n│    │    └─BatchNorm2d: 3-132           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-133                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-134                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-135           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-136                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-137                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-138           [16, 2048, 14, 14]        4,096\n│    │    └─ReLU: 3-139                  [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-16                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-140                [16, 512, 14, 14]         1,048,576\n│    │    └─BatchNorm2d: 3-141           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-142                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-143                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-144           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-145                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-146                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-147           [16, 2048, 14, 14]        4,096\n│    │    └─ReLU: 3-148                  [16, 2048, 14, 14]        --\n├─AdaptiveAvgPool2d: 1-9                 [16, 2048, 1, 1]          --\n├─Linear: 1-10                           [16, 1000]                2,049,000\n==========================================================================================\nTotal params: 25,557,032\nTrainable params: 25,557,032\nNon-trainable params: 0\nTotal mult-adds (G): 261.61\n==========================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 11380.85\nParams size (MB): 102.23\nEstimated Total Size (MB): 11521.61\n=========================================================================================="},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:01:54.165124Z","iopub.execute_input":"2024-12-24T20:01:54.165728Z","iopub.status.idle":"2024-12-24T20:01:54.169607Z","shell.execute_reply.started":"2024-12-24T20:01:54.165689Z","shell.execute_reply":"2024-12-24T20:01:54.168667Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"images = [image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))]\nannots = []\nfor image in images:\n    annot = image[:-4] + '.txt'\n    annots.append(annot)\n    \nimages = pd.Series(images, name='images')\nannots = pd.Series(annots, name='annots')\ndf = pd.concat([images, annots], axis=1)\ndf = pd.DataFrame(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:00.085615Z","iopub.execute_input":"2024-12-24T20:02:00.086405Z","iopub.status.idle":"2024-12-24T20:02:01.282877Z","shell.execute_reply.started":"2024-12-24T20:02:00.086342Z","shell.execute_reply":"2024-12-24T20:02:01.281808Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:02.403085Z","iopub.execute_input":"2024-12-24T20:02:02.403455Z","iopub.status.idle":"2024-12-24T20:02:02.421852Z","shell.execute_reply.started":"2024-12-24T20:02:02.403422Z","shell.execute_reply":"2024-12-24T20:02:02.420948Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"           images        annots\n0      jhope0.png    jhope0.txt\n1      jhope1.png    jhope1.txt\n2     jhope10.png   jhope10.txt\n3    jhope100.png  jhope100.txt\n4    jhope101.png  jhope101.txt\n..            ...           ...\n803      v291.png      v291.txt\n804      v292.png      v292.txt\n805      v293.png      v293.txt\n806      v294.png      v294.txt\n807      v295.png      v295.txt\n\n[808 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>images</th>\n      <th>annots</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>jhope0.png</td>\n      <td>jhope0.txt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>jhope1.png</td>\n      <td>jhope1.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>jhope10.png</td>\n      <td>jhope10.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jhope100.png</td>\n      <td>jhope100.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>jhope101.png</td>\n      <td>jhope101.txt</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>803</th>\n      <td>v291.png</td>\n      <td>v291.txt</td>\n    </tr>\n    <tr>\n      <th>804</th>\n      <td>v292.png</td>\n      <td>v292.txt</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>v293.png</td>\n      <td>v293.txt</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>v294.png</td>\n      <td>v294.txt</td>\n    </tr>\n    <tr>\n      <th>807</th>\n      <td>v295.png</td>\n      <td>v295.txt</td>\n    </tr>\n  </tbody>\n</table>\n<p>808 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:06.335315Z","iopub.execute_input":"2024-12-24T20:02:06.335670Z","iopub.status.idle":"2024-12-24T20:02:06.615934Z","shell.execute_reply.started":"2024-12-24T20:02:06.335640Z","shell.execute_reply":"2024-12-24T20:02:06.615038Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train, test = train_test_split(df, test_size = 0.2, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:08.618020Z","iopub.execute_input":"2024-12-24T20:02:08.618376Z","iopub.status.idle":"2024-12-24T20:02:08.625745Z","shell.execute_reply.started":"2024-12-24T20:02:08.618339Z","shell.execute_reply":"2024-12-24T20:02:08.624773Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"img_transforms  = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[-0.0932, -0.0971, -0.1260], std=[0.5091, 0.4912, 0.4931])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:09.972021Z","iopub.execute_input":"2024-12-24T20:02:09.972860Z","iopub.status.idle":"2024-12-24T20:02:09.977127Z","shell.execute_reply.started":"2024-12-24T20:02:09.972828Z","shell.execute_reply":"2024-12-24T20:02:09.976294Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!mkdir train test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:12.240279Z","iopub.execute_input":"2024-12-24T20:02:12.241119Z","iopub.status.idle":"2024-12-24T20:02:13.286191Z","shell.execute_reply.started":"2024-12-24T20:02:12.241085Z","shell.execute_reply":"2024-12-24T20:02:13.284954Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!mkdir train/jhope train/jin train/jimin train/jungkook train/suga train/rm train/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:13.956898Z","iopub.execute_input":"2024-12-24T20:02:13.957254Z","iopub.status.idle":"2024-12-24T20:02:14.994231Z","shell.execute_reply.started":"2024-12-24T20:02:13.957227Z","shell.execute_reply":"2024-12-24T20:02:14.992873Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!mkdir test/jhope test/jin test/jimin test/jungkook test/suga test/rm test/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:16.813521Z","iopub.execute_input":"2024-12-24T20:02:16.813888Z","iopub.status.idle":"2024-12-24T20:02:17.845316Z","shell.execute_reply.started":"2024-12-24T20:02:16.813857Z","shell.execute_reply":"2024-12-24T20:02:17.844114Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\nclass_names = dict((i, names[i]) for i in range(7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:18.929284Z","iopub.execute_input":"2024-12-24T20:02:18.929909Z","iopub.status.idle":"2024-12-24T20:02:18.934714Z","shell.execute_reply.started":"2024-12-24T20:02:18.929873Z","shell.execute_reply":"2024-12-24T20:02:18.933813Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"for class_name in class_names:\n    for i in range(train.shape[0]):\n        temp = train.iloc[i,:][0].split('.')[0]\n        flags = list(map(str.isalpha, list(temp)))\n        temp_str = temp[:flags.index(False)]\n        if temp_str == class_names.get(class_name):\n            shutil.copy(os.path.join(files_dir, train.iloc[i,:][0]), \\\n                        os.path.join('/kaggle/working/train'+'/'+class_names.get(class_name), train.iloc[i,:][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:20.664401Z","iopub.execute_input":"2024-12-24T20:02:20.664773Z","iopub.status.idle":"2024-12-24T20:02:23.650969Z","shell.execute_reply.started":"2024-12-24T20:02:20.664741Z","shell.execute_reply":"2024-12-24T20:02:23.650203Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/588640762.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  temp = train.iloc[i,:][0].split('.')[0]\n/tmp/ipykernel_30/588640762.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  shutil.copy(os.path.join(files_dir, train.iloc[i,:][0]), \\\n/tmp/ipykernel_30/588640762.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  os.path.join('/kaggle/working/train'+'/'+class_names.get(class_name), train.iloc[i,:][0]))\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!ls train/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:23.652472Z","iopub.execute_input":"2024-12-24T20:02:23.652755Z","iopub.status.idle":"2024-12-24T20:02:24.682095Z","shell.execute_reply.started":"2024-12-24T20:02:23.652728Z","shell.execute_reply":"2024-12-24T20:02:24.680992Z"}},"outputs":[{"name":"stdout","text":"v0.png\t  v11.png   v16.png   v18.png\tv197.png  v209.png  v270.png  v287.png\nv10.png   v110.png  v169.png  v180.png\tv198.png  v21.png   v272.png  v288.png\nv101.png  v112.png  v170.png  v183.png\tv199.png  v212.png  v273.png  v289.png\nv102.png  v114.png  v172.png  v185.png\tv2.png\t  v213.png  v274.png  v293.png\nv103.png  v115.png  v173.png  v187.png\tv20.png   v218.png  v275.png  v294.png\nv104.png  v118.png  v174.png  v19.png\tv200.png  v22.png   v278.png  v295.png\nv105.png  v119.png  v175.png  v192.png\tv201.png  v23.png   v280.png\nv106.png  v12.png   v176.png  v193.png\tv204.png  v25.png   v281.png\nv107.png  v14.png   v177.png  v194.png\tv206.png  v26.png   v283.png\nv109.png  v15.png   v179.png  v196.png\tv208.png  v27.png   v286.png\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"for class_name in class_names:\n    for i in range(test.shape[0]):\n        temp = test.iloc[i,:][0].split('.')[0]\n        flags = list(map(str.isalpha, list(temp)))\n        temp_str = temp[:flags.index(False)]\n        if temp_str == class_names.get(class_name):\n            shutil.copy(os.path.join(files_dir, test.iloc[i,:][0]), \\\n                        os.path.join('/kaggle/working/test'+'/'+class_names.get(class_name), test.iloc[i,:][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:25.351530Z","iopub.execute_input":"2024-12-24T20:02:25.351899Z","iopub.status.idle":"2024-12-24T20:02:26.148593Z","shell.execute_reply.started":"2024-12-24T20:02:25.351866Z","shell.execute_reply":"2024-12-24T20:02:26.147583Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/598401277.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  temp = test.iloc[i,:][0].split('.')[0]\n/tmp/ipykernel_30/598401277.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  shutil.copy(os.path.join(files_dir, test.iloc[i,:][0]), \\\n/tmp/ipykernel_30/598401277.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  os.path.join('/kaggle/working/test'+'/'+class_names.get(class_name), test.iloc[i,:][0]))\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!ls test/suga","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:27.790512Z","iopub.execute_input":"2024-12-24T20:02:27.791152Z","iopub.status.idle":"2024-12-24T20:02:28.830564Z","shell.execute_reply.started":"2024-12-24T20:02:27.791117Z","shell.execute_reply":"2024-12-24T20:02:28.829633Z"}},"outputs":[{"name":"stdout","text":"suga113.png  suga183.png  suga26.png   suga277.png  suga316.png\nsuga13.png   suga201.png  suga268.png  suga28.png   suga378.png\nsuga16.png   suga214.png  suga270.png  suga309.png  suga381.png\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_data = ImageFolder(root='/kaggle/working/train', transform=img_transforms)\nvalidation_data = ImageFolder(root='/kaggle/working/test', transform=img_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:30.976640Z","iopub.execute_input":"2024-12-24T20:02:30.977555Z","iopub.status.idle":"2024-12-24T20:02:30.985966Z","shell.execute_reply.started":"2024-12-24T20:02:30.977502Z","shell.execute_reply":"2024-12-24T20:02:30.984889Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_data, batch_size=16, shuffle=True, num_workers=2)\nvalidation_dataloader = DataLoader(dataset=validation_data, batch_size=16, shuffle=True, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:32.936609Z","iopub.execute_input":"2024-12-24T20:02:32.936954Z","iopub.status.idle":"2024-12-24T20:02:32.941991Z","shell.execute_reply.started":"2024-12-24T20:02:32.936911Z","shell.execute_reply":"2024-12-24T20:02:32.940976Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=16,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0, \n                    plot=False):\n    \"\"\"\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer, factor=0.5, patience=4)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n\n    best_val_loss = float('inf')\n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n\n# Dynamic plot\n    if plot:\n        plot_epoch_data = []\n        plot_train_loss = []\n        plot_val_loss = []\n\n        fig, ax = plt.subplots()\n        line_train, = ax.plot([], [], 'r-')\n        line_val, = ax.plot([], [], 'b-')\n        ax.legend(['train', 'val'])\n        ax.set_xlim(0, epoch_n)\n\n        def add_point(epoch_i, train_loss, val_loss):\n            max_loss = max(ax.viewLim.y1 / 1.1, train_loss, val_loss)\n            ax.set_ylim(0, max_loss * 1.1)\n            \n            plot_epoch_data.append(epoch_i)\n            plot_train_loss.append(train_loss)\n            plot_val_loss.append(val_loss)\n            line_train.set_data(plot_epoch_data, plot_train_loss)\n            line_val.set_data(plot_epoch_data, plot_val_loss)\n            clear_output(wait=True)\n            display(fig)\n\n\n    for epoch_i in range(epoch_n):\n        try:\n            epoch_start = datetime.datetime.now()\n            \n\n            print('Эпоха {}'.format(epoch_i))\n\n            model.train()\n            mean_train_loss = 0\n            train_batches_n = 0\n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                mean_train_loss += float(loss)\n                train_batches_n += 1\n\n            mean_train_loss /= train_batches_n\n\n            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n            print('Среднее значение функции потерь на обучении', mean_train_loss)\n\n\n\n            model.eval()\n            mean_val_loss = 0\n            val_batches_n = 0\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n\n                    mean_val_loss += float(loss)\n                    val_batches_n += 1\n\n            mean_val_loss /= val_batches_n\n\n            if plot:\n                add_point(epoch_i, mean_train_loss, mean_val_loss)\n            else:\n                pass\n            \n            print('Среднее значение функции потерь на валидации', mean_val_loss)\n\n            if mean_val_loss < best_val_loss:\n                best_epoch_i = epoch_i\n                best_val_loss = mean_val_loss\n                best_model = copy.deepcopy(model)\n                print('Новая лучшая модель! На эпохе {}'.format(epoch_i))\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(mean_val_loss)\n\n            print()\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n        finally:\n            if plot:\n                plt.close(fig)\n\n    return best_val_loss, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:55.944280Z","iopub.execute_input":"2024-12-24T20:02:55.944639Z","iopub.status.idle":"2024-12-24T20:02:55.967627Z","shell.execute_reply.started":"2024-12-24T20:02:55.944600Z","shell.execute_reply":"2024-12-24T20:02:55.966656Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:02:57.922453Z","iopub.execute_input":"2024-12-24T20:02:57.923143Z","iopub.status.idle":"2024-12-24T20:02:57.927503Z","shell.execute_reply.started":"2024-12-24T20:02:57.923109Z","shell.execute_reply":"2024-12-24T20:02:57.926332Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"best_loss, best_model = train_eval_loop(model=resnet50_model, \n                train_dataset=train_data, \n                val_dataset=validation_data, \n                criterion=nn.CrossEntropyLoss(),\n                lr=1e-3, \n                epoch_n=100, \n                batch_size=16,\n                device=device, \n                early_stopping_patience=10, \n                l2_reg_alpha=0.8,\n                max_batches_per_epoch_train=10000,\n                max_batches_per_epoch_val=1000,\n                data_loader_ctor=DataLoader,\n                optimizer_ctor=torch.optim.Adam,\n                lr_scheduler_ctor=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                shuffle_train=True,\n                dataloader_workers_n=2,\n                plot=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:06:51.581874Z","iopub.execute_input":"2024-12-24T20:06:51.582253Z","iopub.status.idle":"2024-12-24T20:06:51.611194Z","shell.execute_reply.started":"2024-12-24T20:06:51.582223Z","shell.execute_reply":"2024-12-24T20:06:51.610164Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m best_loss, best_model \u001b[38;5;241m=\u001b[39m train_eval_loop(model\u001b[38;5;241m=\u001b[39mresnet50_model, \n\u001b[1;32m      2\u001b[0m                 train_dataset\u001b[38;5;241m=\u001b[39mtrain_data, \n\u001b[1;32m      3\u001b[0m                 val_dataset\u001b[38;5;241m=\u001b[39mvalidation_data, \n\u001b[1;32m      4\u001b[0m                 criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[1;32m      5\u001b[0m                 lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, \n\u001b[1;32m      6\u001b[0m                 epoch_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m      7\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      8\u001b[0m                 device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[1;32m      9\u001b[0m                 early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     10\u001b[0m                 l2_reg_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     11\u001b[0m                 max_batches_per_epoch_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m     12\u001b[0m                 max_batches_per_epoch_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     13\u001b[0m                 data_loader_ctor\u001b[38;5;241m=\u001b[39mDataLoader,\n\u001b[1;32m     14\u001b[0m                 optimizer_ctor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam,\n\u001b[0;32m---> 15\u001b[0m                 lr_scheduler_ctor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\\\n\u001b[1;32m     16\u001b[0m                                                                              factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     17\u001b[0m                 shuffle_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                 dataloader_workers_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     19\u001b[0m                 plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mTypeError\u001b[0m: Adam.__init__() missing 1 required positional argument: 'params'"],"ename":"TypeError","evalue":"Adam.__init__() missing 1 required positional argument: 'params'","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"predicted_labels = []\nactual_labels = []\n\nresnet50_model.eval()  # Set the model to evaluation mode\nwith torch.inference_mode():  # Ensure no gradients are computed\n    for images, labels in validation_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = resnet50_model(images)\n        _, predicted = torch.max(outputs, 1)\n        predicted_labels.extend(predicted.cpu().numpy())\n        actual_labels.extend(labels.cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T19:59:43.171966Z","iopub.execute_input":"2024-12-24T19:59:43.172260Z","iopub.status.idle":"2024-12-24T19:59:43.662166Z","shell.execute_reply.started":"2024-12-24T19:59:43.172229Z","shell.execute_reply":"2024-12-24T19:59:43.661229Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\nclassification_report, confusion_matrix\n\n# Вычисление тестовых метрик\naccuracy = accuracy_score(actual_labels, predicted_labels)\nprecision = precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\nrecall = recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\nf1 = f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\n\n# Принт метрик\nprint(f\"Model Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Model Precision: {precision * 100:.2f}%\")\nprint(f\"Model Recall: {recall * 100:.2f}%\")\nprint(f\"Model F1 Score: {f1 * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T19:59:43.663802Z","iopub.execute_input":"2024-12-24T19:59:43.664694Z","iopub.status.idle":"2024-12-24T19:59:43.680065Z","shell.execute_reply.started":"2024-12-24T19:59:43.664646Z","shell.execute_reply":"2024-12-24T19:59:43.679160Z"}},"outputs":[{"name":"stdout","text":"Model Accuracy: 17.90%\nModel Precision: 17.42%\nModel Recall: 17.90%\nModel F1 Score: 16.66%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"cm = confusion_matrix(actual_labels, predicted_labels)\nclass_names = validation_dataloader.dataset.classes\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T19:59:43.681765Z","iopub.execute_input":"2024-12-24T19:59:43.682038Z","iopub.status.idle":"2024-12-24T19:59:44.068283Z","shell.execute_reply.started":"2024-12-24T19:59:43.682012Z","shell.execute_reply":"2024-12-24T19:59:44.067415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x1000 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAxMAAANXCAYAAABOkwIqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACCN0lEQVR4nOzdeVxUZf//8feAOCAICK64gIrihmhpZe5pi0suVKaZkWal2W1Gt5mVaylluZtbmprZdrtVmqblQqWVe5lLam65CyKuqDC/P/w135nQHI4MZxhfzx7zeDjXzJzz5nIkPlyfc43FZrPZBAAAAAA55GN2AAAAAAD5E8UEAAAAAEMoJgAAAAAYQjEBAAAAwBCKCQAAAACGUEwAAAAAMIRiAgAAAIAhFBMAAAAADKGYAAAAAGAIxQQAXMOuXbt03333KSQkRBaLRQsXLszV4+/bt08Wi0UzZ87M1ePmZ02aNFGTJk3MjgEAyAGKCQAea8+ePXr22WdVoUIF+fv7Kzg4WPXr19fYsWN14cIFt547ISFBv/32m4YNG6bZs2erTp06bj1fXnryySdlsVgUHBx8zXnctWuXLBaLLBaL3n333Rwf//Dhwxo8eLA2b96cC2kBAJ6sgNkBAOBaFi9erEceeURWq1VPPPGEatSooUuXLumHH35Q37599fvvv2vq1KluOfeFCxe0du1avfbaa3r++efdco7IyEhduHBBfn5+bjn+jRQoUEDnz5/XV199pQ4dOjg9NmfOHPn7++vixYuGjn348GENGTJEUVFRqlWrlsuvW7ZsmaHzAQDMQzEBwOPs3btXHTt2VGRkpFasWKFSpUrZH+vVq5d2796txYsXu+38J06ckCSFhoa67RwWi0X+/v5uO/6NWK1W1a9fX5988km2YuLjjz9Wq1atNG/evDzJcv78eRUqVEgFCxbMk/MBAHIPbU4APM6IESN09uxZTZ8+3amQ+Ft0dLReeOEF+/0rV67ojTfeUMWKFWW1WhUVFaVXX31VGRkZTq+LiopS69at9cMPP+iOO+6Qv7+/KlSooA8//ND+nMGDBysyMlKS1LdvX1ksFkVFRUm62h70958dDR48WBaLxWls+fLlatCggUJDQxUUFKSYmBi9+uqr9sevd83EihUr1LBhQwUGBio0NFRt27bV9u3br3m+3bt368knn1RoaKhCQkLUtWtXnT9//voT+w+PPfaYlixZorS0NPvYunXrtGvXLj322GPZnp+amqr//ve/io2NVVBQkIKDg9WiRQtt2bLF/pxVq1apbt26kqSuXbva26X+/jqbNGmiGjVqaMOGDWrUqJEKFSpkn5d/XjORkJAgf3//bF///fffryJFiujw4cMuf60AAPegmADgcb766itVqFBBd999t0vP7969uwYOHKjbbrtNo0ePVuPGjZWUlKSOHTtme+7u3bv18MMP695779XIkSNVpEgRPfnkk/r9998lSfHx8Ro9erQkqVOnTpo9e7bGjBmTo/y///67WrdurYyMDA0dOlQjR45UmzZt9OOPP/7r67799lvdf//9On78uAYPHqzExEStWbNG9evX1759+7I9v0OHDjpz5oySkpLUoUMHzZw5U0OGDHE5Z3x8vCwWi+bPn28f+/jjj1WlShXddttt2Z7/559/auHChWrdurVGjRqlvn376rffflPjxo3tP9hXrVpVQ4cOlSQ988wzmj17tmbPnq1GjRrZj5OSkqIWLVqoVq1aGjNmjJo2bXrNfGPHjlWxYsWUkJCgzMxMSdKUKVO0bNkyjR8/XhERES5/rQAAN7EBgAc5ffq0TZKtbdu2Lj1/8+bNNkm27t27O43/97//tUmyrVixwj4WGRlpk2RLTk62jx0/ftxmtVptL730kn1s7969Nkm2d955x+mYCQkJtsjIyGwZBg0aZHP8djp69GibJNuJEyeum/vvc8yYMcM+VqtWLVvx4sVtKSkp9rEtW7bYfHx8bE888US283Xr1s3pmO3bt7eFh4df95yOX0dgYKDNZrPZHn74YVuzZs1sNpvNlpmZaStZsqRtyJAh15yDixcv2jIzM7N9HVar1TZ06FD72Lp167J9bX9r3LixTZJt8uTJ13yscePGTmPffPONTZLtzTfftP3555+2oKAgW7t27W74NQIA8gYrEwA8Snp6uiSpcOHCLj3/66+/liQlJiY6jb/00kuSlO3aimrVqqlhw4b2+8WKFVNMTIz+/PNPw5n/6e9rLb744gtlZWW59JojR45o8+bNevLJJxUWFmYfr1mzpu6991771+moR48eTvcbNmyolJQU+xy64rHHHtOqVat09OhRrVixQkePHr1mi5N09ToLH5+r/9vIzMxUSkqKvYVr48aNLp/TarWqa9euLj33vvvu07PPPquhQ4cqPj5e/v7+mjJlisvnAgC4F8UEAI8SHBwsSTpz5oxLz9+/f798fHwUHR3tNF6yZEmFhoZq//79TuPlypXLdowiRYro1KlTBhNn9+ijj6p+/frq3r27SpQooY4dO+rzzz//18Li75wxMTHZHqtatapOnjypc+fOOY3/82spUqSIJOXoa2nZsqUKFy6szz77THPmzFHdunWzzeXfsrKyNHr0aFWqVElWq1VFixZVsWLF9Ouvv+r06dMun7N06dI5utj63XffVVhYmDZv3qxx48apePHiLr8WAOBeFBMAPEpwcLAiIiK0devWHL3unxdAX4+vr+81x202m+Fz/N3P/7eAgAAlJyfr22+/VZcuXfTrr7/q0Ucf1b333pvtuTfjZr6Wv1mtVsXHx2vWrFlasGDBdVclJGn48OFKTExUo0aN9NFHH+mbb77R8uXLVb16dZdXYKSr85MTmzZt0vHjxyVJv/32W45eCwBwL4oJAB6ndevW2rNnj9auXXvD50ZGRiorK0u7du1yGj927JjS0tLsOzPlhiJFijjtfPS3f65+SJKPj4+aNWumUaNGadu2bRo2bJhWrFihlStXXvPYf+fcuXNntsd27NihokWLKjAw8Oa+gOt47LHHtGnTJp05c+aaF63/be7cuWratKmmT5+ujh076r777lPz5s2zzYmrhZ0rzp07p65du6patWp65plnNGLECK1bty7Xjg8AuDkUEwA8zssvv6zAwEB1795dx44dy/b4nj17NHbsWElX23QkZdtxadSoUZKkVq1a5VquihUr6vTp0/r111/tY0eOHNGCBQucnpeamprttX9/eNs/t6v9W6lSpVSrVi3NmjXL6YfzrVu3atmyZfav0x2aNm2qN954QxMmTFDJkiWv+zxfX99sqx7/+9//dOjQIaexv4ueaxVeOdWvXz8dOHBAs2bN0qhRoxQVFaWEhITrziMAIG/xoXUAPE7FihX18ccf69FHH1XVqlWdPgF7zZo1+t///qcnn3xSkhQXF6eEhARNnTpVaWlpaty4sX755RfNmjVL7dq1u+62o0Z07NhR/fr1U/v27dW7d2+dP39ekyZNUuXKlZ0uQB46dKiSk5PVqlUrRUZG6vjx45o4caLKlCmjBg0aXPf477zzjlq0aKF69erpqaee0oULFzR+/HiFhIRo8ODBufZ1/JOPj49ef/31Gz6vdevWGjp0qLp27aq7775bv/32m+bMmaMKFSo4Pa9ixYoKDQ3V5MmTVbhwYQUGBurOO+9U+fLlc5RrxYoVmjhxogYNGmTfqnbGjBlq0qSJBgwYoBEjRuToeACA3MfKBACP1KZNG/366696+OGH9cUXX6hXr1565ZVXtG/fPo0cOVLjxo2zP3fatGkaMmSI1q1bpz59+mjFihXq37+/Pv3001zNFB4ergULFqhQoUJ6+eWXNWvWLCUlJenBBx/Mlr1cuXL64IMP1KtXL7333ntq1KiRVqxYoZCQkOsev3nz5lq6dKnCw8M1cOBAvfvuu7rrrrv0448/5vgHcXd49dVX9dJLL+mbb77RCy+8oI0bN2rx4sUqW7as0/P8/Pw0a9Ys+fr6qkePHurUqZNWr16do3OdOXNG3bp1U+3atfXaa6/Zxxs2bKgXXnhBI0eO1E8//ZQrXxcAwDiLLSdX6gEAAADA/8fKBAAAAABDKCYAAAAAGEIxAQAAAMAQigkAAADACyUnJ+vBBx9URESELBaLFi5ceN3n9ujRQxaLJdtW6zdCMQEAAAB4oXPnzikuLk7vvffevz5vwYIF+umnnxQREZHjc/A5EwAAAIAXatGihVq0aPGvzzl06JD+85//6JtvvjH0Qa8UEwAAAEA+kZGRoYyMDKcxq9Uqq9Wa42NlZWWpS5cu6tu3r6pXr24oj1cWExE95psdwevNeqGR2RG8WvnwQLMjeL35vx82O4LXi6+e8+VyALeO6OIBZke4roDaz5sd4br6tS2qIUOGOI0NGjRIgwcPzvGx3n77bRUoUEC9e/c2nMcriwkAAADAG/Xv31+JiYlOY0ZWJTZs2KCxY8dq48aNslgshvNwATYAAACQT1itVgUHBzvdjBQT33//vY4fP65y5cqpQIECKlCggPbv36+XXnpJUVFRLh+HlQkAAADAkcX7f9/epUsXNW/e3Gns/vvvV5cuXdS1a1eXj0MxAQAAAHihs2fPavfu3fb7e/fu1ebNmxUWFqZy5copPDzc6fl+fn4qWbKkYmJiXD4HxQQAAADghdavX6+mTZva7/99rUVCQoJmzpyZK+egmAAAAAAc3cQFyZ6kSZMmstlsLj9/3759OT6H9zeEAQAAAHALigkAAAAAhtDmBAAAADi6BXZzyi3MFAAAAABDKCYAAAAAGEKbEwAAAODIS3ZzygusTAAAAAAwhGICAAAAgCG0OQEAAACO2M3JZcwUAAAAAEMoJgAAAAAYQpsTAAAA4IjdnFzGygQAAAAAQygmAAAAABhCmxMAAADgiN2cXMZMAQAAADCEYgIAAACAIbQ5AQAAAI7YzcllrEwAAAAAMIRiAgAAAIAhtDkBAAAAjtjNyWXMFAAAAABDKCYAAAAAGEKbEwAAAOCI3ZxcxsoEAAAAAEMoJgAAAAAYQpsTAAAA4IjdnFzGTAEAAAAwhGICAAAAgCG0OQEAAACO2M3JZaxMAAAAADDE9GJi9uzZql+/viIiIrR//35J0pgxY/TFF1+YnAwAAADAvzG1mJg0aZISExPVsmVLpaWlKTMzU5IUGhqqMWPGmBkNAAAAtyqLj+fePIypicaPH6/3339fr732mnx9fe3jderU0W+//WZiMgAAAAA3YmoxsXfvXtWuXTvbuNVq1blz50xIBAAAAMBVpu7mVL58eW3evFmRkZFO40uXLlXVqlVNSgUAAIBbmge2E3kqU4uJxMRE9erVSxcvXpTNZtMvv/yiTz75RElJSZo2bZqZ0QAAAADcgKnFRPfu3RUQEKDXX39d58+f12OPPaaIiAiNHTtWHTt2NDMaAAAAgBsw/UPrOnfurM6dO+v8+fM6e/asihcvbnYkAAAA3Mp8+NA6V5leTEjS8ePHtXPnTkmSxWJRsWLFTE4EAAAA4EZMvbrkzJkz6tKliyIiItS4cWM1btxYERERevzxx3X69GkzowEAAAC4AVOLie7du+vnn3/W4sWLlZaWprS0NC1atEjr16/Xs88+a2Y0AAAA3KrM/mC6fPShdaa2OS1atEjffPONGjRoYB+7//779f777+uBBx4wMVnuuzM6XM/dV1mx5UJVMjRA3Sat1dItR+yPt6gVoScalVdsuVCFBVl175vf6fe/WJ3JLcvmzdaXsyerSetH9HD3PmbH8Qqfz56uNcnf6a/9+1TQalXVGnHq2rOPypSLMjuaVzl36qTWLZihv35fryuXMhRcrJQaJryoYpGVzY6W7/Eedj/m2P2YY5jN1PImPDxcISEh2cZDQkJUpEgRExK5TyFrAf3+12m9+umW6zzuq192p2j4gt/zOJn3279ru3785guVjoo2O4pX+W3zBrVq/6hGTvlQb46erCtXruj1xJ66eOGC2dG8Rsa5M1r0zn/l4+ur+58fqocGTdYdDz8ta6HCZkfzCryH3Y85dj/mGGYzdWXi9ddfV2JiombPnq2SJUtKko4ePaq+fftqwIABZkbLdSt/P6aVvx+77uPzfj4oSSoTXiivIt0SMi6c18zRQ9SpVz8t/XyW2XG8yhsjJzrdT3x1qB5rc49279ymGrVuNymVd/l12VwFhhVTo4RE+1jhoiVNTORdeA+7H3Psfsyxm1jYzclVphYTkyZN0u7du1WuXDmVK1dOknTgwAFZrVadOHFCU6ZMsT9348aNZsVEPvbZ1JGqcXs9VYmrSzHhZufOnZUkBQVnX22EMQe2/KTS1W7Xd1OH6+iu31QoNFxVG7VWlYbe1QbqKXgPux9z7H7MMfKaqcVEu3btzDw9vNz677/VwT1/6OV3+TR1d8vKytLUce+oWmwtRVWgnSy3nDl5VDuSF6tG8/aKe+BRndz/h376fLJ8CxRQpXrNzY7nVXgPux9z7H7MMcxgajExaNCgmz5GRkaGMjIynMZsmZdl8fW76WMj/zp14pjmTRuj54eMkV9Bq9lxvN6kUUnav3e33nlvptlRvIrNZlPRyEqq0+5JSVLRchV16vB+bU/+mmIil/Eedj/m2P2Y41zkgbsmeSqP+NC6DRs2aPv27ZKk6tWrq3bt2i6/NikpSUOGDHEaC7q9gwrXeTRXMyJ/ObBnp86cPqW3E7vZx7KyMrVn22Ylfz1fY/63Uj6+viYm9B6TRifpl7XJenv8BypavITZcbxKQEgRhZYq6zQWWrKs9m380aRE3on3sPsxx+7HHMMsphYTx48fV8eOHbVq1SqFhoZKktLS0tS0aVN9+umnLn0Sdv/+/ZWYmOg0FvPSEnfERT4SE3e7Xh0722nso/HDVKJ0pO6Nf5xCIhfYbDZNHvOW1iavUNK4aSoZUdrsSF6nRMVqOn3skNPY6WOHFBRe3KRE3oX3sPsxx+7HHMNspq7h/Oc//9GZM2f0+++/KzU1Vampqdq6davS09PVu3dvl45htVoVHBzsdPPEFqdCVl9VLxOi6mWuXhBVtmigqpcJUekiAZKk0EJ+ql4mRJVLXd3ysWKJIFUvE6JiwbToGOEfEKiIyApOt4LWAAUWDlZEZAWz43mFiaOGa+Wyxeo7MEkBhQKVmnJSqSknlZFx0exoXqNGs/Y6/ucObV7ymdKPH9aeX1Zq5w9LVLVxa7OjeQXew+7HHLsfc+wmFovn3jyMxWaz2cw6eUhIiL799lvVrVvXafyXX37Rfffdp7S0NEPHjegxPxfS5a56lYtqXmKjbOOfrd2vF2dtUId65TQmoU62x0cu2q6Ri7bnRcQcmfVC9q/F04157XmVKR+dLz60rnx4oNkRbqhVw1rXHO/Tf4jubdk2b8MYMP/3w2ZHcMmBX3/W+oUzlX78sIKKllSNZu3zzW5O8dUjzI7wr/L7ezg/YI7dLz/PcXTxALMjXFfAvW+bHeG6LizvZ3YEJ6YWE4ULF9b333+vWrVqOY1v2rRJjRs3Vnp6uqHjemIx4W3yYzGRn+SHYiK/yy/FRH7m6cUEAHNRTBjjacWEqW1O99xzj1544QUdPvx//1M/dOiQXnzxRTVr1szEZAAAALhlWXw89+ZhTE00YcIEpaenKyoqShUrVlTFihVVvnx5paena/z48WZGAwAAAHADpu7mVLZsWW3cuFHffvutduzYIUmqWrWqmjdn/3QAAADA05n+ORMWi0X33nuv7r33XrOjAAAAAB65a5KnMr2Y+O677/Tdd9/p+PHjysrKcnrsgw8+MCkVAAAAgBsxtZgYMmSIhg4dqjp16qhUqVKyUAUCAAAA+YapxcTkyZM1c+ZMdenSxcwYAAAAwP/xwF2TPJWpM3Xp0iXdfffdZkYAAAAAYJCpxUT37t318ccfmxkBAAAAgEF53uaUmJho/3NWVpamTp2qb7/9VjVr1pSfn5/Tc0eNGpXX8QAAAHCr4zpel+V5MbFp0yan+7Vq1ZIkbd26Na+jAAAAALgJeV5MrFy5Mq9PCQAAAMAN8ryYiI+P18yZMxUcHKz4+Ph/fW5QUJCqV6+uHj16KCQkJI8SAgAA4JbGbk4uy/NiIiQkxP55EjcqEDIyMjR58mT9+OOP+vLLL/MiHgAAAAAX5XkxMWPGjGv++Xq2bdumunXrujMSAAAAAANM/dA6V8TExGjNmjVmxwAAAMCtgt2cXObxDWG+vr6Ki4szOwYAAACAf/D4YgIAAACAZ/L4NicAAAAgT7Gbk8uYKQAAAACGUEwAAAAAMIQ2JwAAAMARbU4uY6YAAAAAGEIxAQAAAMAQ2pwAAAAAR3xonctYmQAAAABgCMUEAAAAAENocwIAAAAcsZuTy5gpAAAAAIZQTAAAAAAwhDYnAAAAwBG7ObmMlQkAAAAAhlBMAAAAADCENicAAADAEbs5uYyZAgAAAGAIxQQAAAAAQ2hzAgAAAByxm5PLWJkAAAAAYAjFBAAAAABDaHMCAAAAHFhoc3IZKxMAAAAADKGYAAAAAGAIbU4AAACAA9qcXMfKBAAAAABDKCYAAAAAGEKbEwAAAOCILieXsTIBAAAAeKHk5GQ9+OCDioiIkMVi0cKFC+2PXb58Wf369VNsbKwCAwMVERGhJ554QocPH87ROSgmAAAAAC907tw5xcXF6b333sv22Pnz57Vx40YNGDBAGzdu1Pz587Vz5061adMmR+egzQkAAABw4C27ObVo0UItWrS45mMhISFavny509iECRN0xx136MCBAypXrpxL56CYAAAAAPKJjIwMZWRkOI1ZrVZZrdabPvbp06dlsVgUGhrq8mu8spgIK1rY7Ahe740lO82O4NWebRxpdgSvVzr45r/p4t8FB3jl/2JwCwkO8DM7ApBNUlKShgwZ4jQ2aNAgDR48+KaOe/HiRfXr10+dOnVScHCwy6/jOz0AAADgwJPbnPr376/ExESnsZtdlbh8+bI6dOggm82mSZMm5ei1FBMAAABAPpFbLU1/+7uQ2L9/v1asWJGjVQmJYgIAAAC4Jf1dSOzatUsrV65UeHh4jo9BMQEAAAA48OQ2p5w4e/asdu/ebb+/d+9ebd68WWFhYSpVqpQefvhhbdy4UYsWLVJmZqaOHj0qSQoLC1PBggVdOgfFBAAAAOCF1q9fr6ZNm9rv/32tRUJCggYPHqwvv/xSklSrVi2n161cuVJNmjRx6RwUEwAAAIAXatKkiWw223Uf/7fHXEUxAQAAADjwljanvOBjdgAAAAAA+RPFBAAAAABDaHMCAAAAHNHl5DJWJgAAAAAYQjEBAAAAwBDanAAAAAAH7ObkOlYmAAAAABhCMQEAAADAENqcAAAAAAe0ObmOlQkAAAAAhlBMAAAAADCENicAAADAAW1OrmNlAgAAAIAhFBMAAAAADKHNCQAAAHBAm5PrWJkAAAAAYAjFBAAAAABDaHMCAAAAHNHl5DJWJgAAAAAYQjEBAAAAwBDanAAAAAAH7ObkOlYmAAAAABhCMQEAAADAENqcAAAAAAe0ObmOlQkAAAAAhlBMAAAAADCENicAAADAAW1OrmNlAgAAAIAhFBMAAAAADKHNCQAAAHBEl5PLWJkAAAAAYAjFBAAAAABDTG1zOnfunN566y199913On78uLKyspwe//PPP01KBgAAgFsVuzm5ztRionv37lq9erW6dOmiUqVK8RcHAAAA5COmFhNLlizR4sWLVb9+fTNjAAAAADDA1GKiSJEiCgsLMzMCAAAA4IRuGdeZegH2G2+8oYEDB+r8+fNmxgAAAABggKkrEyNHjtSePXtUokQJRUVFyc/Pz+nxjRs3mpQMAAAAwI2YWky0a9fOzNMDAAAA2dDm5DpTi4lBgwaZeXoAAAAAN4EPrQMAAABgSJ6vTISFhemPP/5Q0aJFVaRIkX9dRkpNTc3DZO51e1SoujaIUrWIYBUPtqr3nM1asf2E03N6Nauoh+uUVmH/Atp0IE1vfLlDB1K4OP1mBPj5qnv9cmoYHaYihfy06/g5jVu5VzuOnTU7Wr63au4sJc//0GksvFRZ9Ro505xAXog5dr/NG9frk9kztHP7NqWcPKFh745VoybNzI7lVZhj9/v04zmaNWO6Tp48ocoxVfTKqwMUW7Om2bHyNdqcXJfnxcTo0aNVuHBhSdKYMWPy+vSmCfDz1c6jZ7RgwyGN7Vwr2+PdGkap811l9dq833Xo1AU937yipiTUVttxa3XpSlb2A8Il/e6LVvnwQhq2ZJdOnruk+6oW06iHq+uJWZt08uwls+Ple8XKRKnLq+/Y7/v4+JqYxjsxx+518cIFRVeKUas27fVa3z5mx/FKzLF7LV3ytd4dkaTXBw1RbGyc5syepZ7PPqUvFi1VeHi42fFwC8jzYiIhIeGaf/Z2P+xK0Q+7Uq77eJe7y2nqqr1auePqasWrc3/X6lcaqVnVYlry27G8iulVChbwUaNK4Xr1i+3acihdkjRj7UHdXSFM7WqW1LQ1B0xOmP/5+PoqKJTPinEn5ti97qrfUHfVb2h2DK/GHLvX7FkzFP9wB7Vr/5Ak6fVBQ5ScvEoL58/TU08/Y3I63ApMvQD7b8ePH9fx48eVleX8G/iat8gSXZkiASpW2Kq1e/6v2DibcUW//pWuuLKhFBMG+VosKuBjybayk3ElS7Glg01K5V1Sjx7SqOc6qIBfQZWpVE3NOj6lkKIlzI7lVZhjANdz+dIlbd/2u556+ln7mI+Pj+666279umWTicm8AF1OLjO1mNiwYYMSEhK0fft22Ww2p8csFosyMzNNSpa3igYVlCSl/KPtJuVshooWLmhGJK9w4XKmth5OV8JdZbU/9YJOnb+kZlWKqXqpwjqUdtHsePle6egqavvsywqPKKMzp1KVPP9DzRzaRz3eni5rQCGz43kF5hjAvzmVdkqZmZnZ2pnCw8O1d++fJqXCrcbUYqJbt26qXLmypk+frhIlShi62CUjI0MZGRlOY1lXLsmnAD+EQ3pzyS69cn+0FjxbV1eybNp1/Ky+23lCMcWDzI6W71Wqdaf9zyXKVVSZ6Koa2/sxbftplWo3bWliMu/BHAMAPJ2pxcSff/6pefPmKTo62vAxkpKSNGTIEKexYg0fV/FGT9xsvDzz94XA4UEFnS4KDg+yaueRM2bF8gqHT19U78+3yr+AjwKtvko5d1mDW8Xo8GlWJnKbf2CQwkuVUeqxw2ZH8VrMMQBHRUKLyNfXVykpztdkpqSkqGjRoial8g7s5uQ6Uz9nolmzZtqyZctNHaN///46ffq0063o3R1zKWHe+OvUBZ04k6G7Kv7fMmWg1Vc1ywRry8E084J5kYtXspRy7rKCrL6qGxmqH/Z4z7bDnuLSxQtKPXaYi4XdiDkG4MivYEFVrVZdP/+01j6WlZWln39eq5pxtU1MhluJqSsT06ZNU0JCgrZu3aoaNWrIz8/P6fE2bdrc8BhWq1VWq9VpzBNbnAIK+qpcWID9fukiAYopGaTTF67o6OmLmr3mgJ5pUl77U85f3Rq2WUUdP5Oh7/7xWRTImbqRobJYpIOpF1Q61F89G0XpwKkL+vr342ZHy/eWzZmsyrfVU2jREjpzKkWr5s6Uj4+Patx9j9nRvAZz7H7nz5/XoYP/t7PbkUOHtGvnDgWHhKhEyVImJvMezLF7dUnoqgGv9lP16jVUI7amPpo9SxcuXFC79vFmR8MtwtRiYu3atfrxxx+1ZMmSbI952wXYNUoHa8ZTdez3+7WMkSQt3HhYr8//XR98v08BBX01uG1VFfYvoI0H0tRj1iY+Y+ImBVl99UyDSBULsurMxStavTtF7/+wX5lZthu/GP/qTMoJzR8/TBfOpqtQcIjKVa6hbkMnKDA41OxoXoM5dr+d27aqd49u9vsTRo+QJD3Quq1eGzzMrFhehTl2rwdatNSp1FRNnDBOJ0+eUEyVqpo4ZZrCaXO6KbQ5uc5i++c2SnkoKipKrVu31oABA1SiRO5tdVjj9eW5dixcW1gYO8m407ONI82OANy0eyuzhS3yt+AAvxs/CYb5e8QHFFxbmecWmh3huv6a2M7sCE5MvWYiJSVFL774Yq4WEgAAAADyhqk1YXx8vFauXKmKFSuaGQMAAACwo83JdaYWE5UrV1b//v31ww8/KDY2NtsF2L179zYpGQAAAIAbMX03p6CgIK1evVqrV692esxisVBMAAAAAB7M1GJi7969Zp4eAAAAyI4uJ5eZegE2AAAAgPwrz1cmEhMT9cYbbygwMFCJiYn/+txRo0blUSoAAAAAOZXnxcSmTZt0+fJl+58BAAAAT8JuTq7L82Ji5cqV1/wzAAAAgPwlz4uJ+Ph4zZw5U8HBwYqPj//X5wYFBal69erq0aOHQkJC8ighAAAAAFfkeTEREhJiXzq6UYGQkZGhyZMn68cff9SXX36ZF/EAAABwi6PNyXV5XkzMmDHjmn++nm3btqlu3brujAQAAADAAI/fGjYmJkZr1qwxOwYAAACAfzD1Q+tc4evrq7i4OLNjAAAA4BZBm5PrPH5lAgAAAIBnopgAAAAAYIjHtzkBAAAAeYk2J9exMgEAAADAEIoJAAAAAIbQ5gQAAAA4osvJZaxMAAAAADCEYgIAAACAIbQ5AQAAAA7Yzcl1rEwAAAAAMIRiAgAAAIAhtDkBAAAADmhzch0rEwAAAAAMoZgAAAAAYAhtTgAAAIADupxcx8oEAAAAAEMoJgAAAAAYQpsTAAAA4IDdnFzHygQAAAAAQygmAAAAABhCmxMAAADggC4n17EyAQAAAMAQigkAAAAAhtDmBAAAADhgNyfXsTIBAAAAwBCKCQAAAACG0OYEAAAAOKDLyXWsTAAAAAAwhGICAAAAgCG0OQEAAAAOfHzoc3IVKxMAAAAADKGYAAAAAGAIxQQAAADgwGLx3FtOJCcn68EHH1RERIQsFosWLlzo9LjNZtPAgQNVqlQpBQQEqHnz5tq1a1eOzkExAQAAAHihc+fOKS4uTu+99941Hx8xYoTGjRunyZMn6+eff1ZgYKDuv/9+Xbx40eVzcAE2AAAA4IVatGihFi1aXPMxm82mMWPG6PXXX1fbtm0lSR9++KFKlCihhQsXqmPHji6dg5UJAAAAwIHFYvHYW0ZGhtLT051uGRkZOf4a9+7dq6NHj6p58+b2sZCQEN15551au3aty8fxypWJPYu/MDuC16vSu5vZEYCbcmfZcLMjeL0tf502O4JXKx8eaHYEr5d+4YrZEbxadPEAsyPkS0lJSRoyZIjT2KBBgzR48OAcHefo0aOSpBIlSjiNlyhRwv6YK7yymAAAAAC8Uf/+/ZWYmOg0ZrVaTUpDMQEAAAA4yemuSXnJarXmSvFQsmRJSdKxY8dUqlQp+/ixY8dUq1Ytl4/DNRMAAADALaZ8+fIqWbKkvvvuO/tYenq6fv75Z9WrV8/l47AyAQAAAHihs2fPavfu3fb7e/fu1ebNmxUWFqZy5cqpT58+evPNN1WpUiWVL19eAwYMUEREhNq1a+fyOSgmAAAAAAcWT+5zyoH169eradOm9vt/X2uRkJCgmTNn6uWXX9a5c+f0zDPPKC0tTQ0aNNDSpUvl7+/v8jkoJgAAAAAv1KRJE9lstus+brFYNHToUA0dOtTwObhmAgAAAIAhrEwAAAAADrylzSkvsDIBAAAAwBCKCQAAAACG0OYEAAAAOKDLyXWsTAAAAAAwhGICAAAAgCG0OQEAAAAO2M3JdaxMAAAAADCEYgIAAACAIbQ5AQAAAA7ocnIdKxMAAAAADKGYAAAAAGAIbU4AAACAA3Zzch0rEwAAAAAMoZgAAAAAYAhtTgAAAIADupxcx8oEAAAAAEMoJgAAAAAYQpsTAAAA4IDdnFzHygQAAAAAQygmAAAAABhCmxMAAADggC4n17EyAQAAAMAQigkAAAAAhtDmBAAAADhgNyfXsTIBAAAAwBCKCQAAAACG0OYEAAAAOKDLyXWsTAAAAAAwhGICAAAAgCG0OQEAAAAO2M3JdaxMAAAAADCEYgIAAACAIbQ5AQAAAA7ocnIdKxMAAAAADKGYAAAAAGAIbU4AAACAA3Zzch0rEwAAAAAMoZgAAAAAYAhtTgAAAIADupxcx8oEAAAAAENMX5nYtWuXVq5cqePHjysrK8vpsYEDB5qUKvfVv62iXnyiuW6rVk6lioWow4tT9dWqX6/53HGvddTTDzdQ33fmasLHq/I2qBfxsUgdapVSw4phCg3w06nzl7Vqd4rmbjlqdjSvsGruLCXP/9BpLLxUWfUaOdOcQF7o89nTtSb5O/21f58KWq2qWiNOXXv2UZlyUWZH80rL5s3Wl7Mnq0nrR/Rw9z5mx/EKvIfdjzmG2UwtJt5//3317NlTRYsWVcmSJZ2unLdYLF5VTAQGWPXbH4f04Rdr9dmoZ677vDZNa+qO2CgdPp6Wd+G8VLvYErqvSjFN+H6fDqZdVMXwQurVMFLnL2Xq6+0nzI7nFYqViVKXV9+x3/fx8TUxjff5bfMGtWr/qCpXra7MzEzNmjJeryf21OTZ8+UfEGB2PK+yf9d2/fjNFyodFW12FK/Ce9j9mGP3YDcn15laTLz55psaNmyY+vXrZ2aMPLHsx21a9uO2f31ORLEQjer3iB587j0tGN8zj5J5r5jiQVp3IE0b/0qXJJ04e0kNKhRRdLFAiWIiV/j4+iooNMzsGF7rjZETne4nvjpUj7W5R7t3blONWreblMr7ZFw4r5mjh6hTr35a+vkss+N4Fd7D7sccw2ymXjNx6tQpPfLII2ZG8BgWi0XT33xCo2d9p+1/0oaTG3YeP6vYUoVVKtgqSYosEqAqJYK06a/TJifzHqlHD2nUcx007oXHNX/CcJ0+eczsSF7t3LmzkqSg4BCTk3iXz6aOVI3b66lKXF2zo3g93sPuxxwjr5m6MvHII49o2bJl6tGjh5kxPMJLXe/VlcwsvffJKrOjeI0Fvx5TgJ+vxsZXU5bt6jUUn2w4rO//PGV2NK9QOrqK2j77ssIjyujMqVQlz/9QM4f2UY+3p8saUMjseF4nKytLU8e9o2qxtRRVgVac3LL++291cM8fevndaWZH8Xq8h92POc49tDm5ztRiIjo6WgMGDNBPP/2k2NhY+fn5OT3eu3fvGx4jIyNDGRkZTmO2rExZ8lHvdu2qZdWrUxPd/djbZkfxKneXL6KGFcM0dvU+HUy7oKiwQup6RxmlXris1btTzY6X71Wqdaf9zyXKVVSZ6Koa2/sxbftplWo3bWliMu80aVSS9u/drXfem2l2FK9x6sQxzZs2Rs8PGSO/glaz43g93sPuxxzDDKYWE1OnTlVQUJBWr16t1atXOz1msVhcKiaSkpI0ZMgQpzHfEnXlV+qOXM3qTvVrV1TxsCD98fVQ+1iBAr56KzFez3duqiqtBpmYLv/qUre0Fv56VD/uvboSceDURRULKqj42JIUE27gHxik8FJllHrssNlRvM6k0Un6ZW2y3h7/gYoWL2F2HK9xYM9OnTl9Sm8ndrOPZWVlas+2zUr+er7G/G+lfHzzzy+mPBnvYfdjjmEWU4uJvXv33vQx+vfvr8TERKex4g3z1wXdHy9epxU/73Qa+2piL328+Bd9+MVPJqXK/6y+PsqyOY9lZdn4IBo3uXTxglKPHVZsg+ZmR/EaNptNk8e8pbXJK5Q0bppKRpQ2O5JXiYm7Xa+One009tH4YSpROlL3xj9OIZELeA+7H3PsHvys4DrTP2fiZlmtVlmtzsvTntjiFBhQUBXLFrPfjyodrpqVS+tU+nkdPHpKqafPOT3/8pVMHTuZrl37j+d1VK+x/uBpPRRXUifPXdLBtIsqHxag1jWKa+WuFLOjeYVlcyar8m31FFq0hM6cStGquTPl4+OjGnffY3Y0rzFx1HCt/naJBgwfo4BCgUpNOSlJCgwKktXqb3K6/M8/IFARkRWcxgpaAxRYODjbOIzhPex+zDHMlufFRGJiot544w0FBgZmW1H4p1GjRuVRKve7rVqklk17wX5/xH8fkiTN/vInPTPoI7NiebXpPx1Ux9si9HS9sgr2v/qhdct3ntTczeyWlRvOpJzQ/PHDdOFsugoFh6hc5RrqNnSCAoNDzY7mNb5e+D9J0iu9uzuN9+k/RPe2bGtGJCBHeA+7H3MMs1lsNpvtxk/LPU2bNtWCBQsUGhqqpk2b/utzV65caegcAbWfN/Q6uK5V7243fhIMa1+zuNkRvN6dZcPNjuD19qacu/GTYFj58ECzIwA3Jbq4536oXpMxa8yOcF2r+txtdgQneb4y4VggGC0WAAAAAJgvz4uJ+Ph4zZw5U8HBwYqPj//X5wYFBal69erq0aOHQkL48BUAAADAk+R5MRESEmL/IJAbFQgZGRmaPHmyfvzxR3355Zd5EQ8AAAC3OHZzcl2eFxMzZsy45p+vZ9u2bapbt647IwEAAAAwwMfsADcSExOjNWs89yIYAAAA4Fbl8Z8z4evrq7i4OLNjAAAA4BZhoc/JZR6/MgEAAADAM1FMAAAAADDE49ucAAAAgLxEl5PrWJkAAAAAYAjFBAAAAABDaHMCAAAAHPjQ5+QyViYAAAAAGEIxAQAAAMAQ2pwAAAAAB3Q5uY6VCQAAAACGUEwAAAAAMIQ2JwAAAMCBhT4nl7EyAQAAAMAQigkAAAAAhlBMAAAAADCEayYAAAAABz5cMuEyViYAAAAAGEIxAQAAAMAQ2pwAAAAAB2wN6zpWJgAAAAAYQjEBAAAAwBDanAAAAAAHdDm5jpUJAAAAAIZQTAAAAAAwhDYnAAAAwIFF9Dm5ipUJAAAAAIZQTAAAAAAwhDYnAAAAwIEPXU4uY2UCAAAAgCEUEwAAAAAMoc0JAAAAcGDhU+tcxsoEAAAAAEMoJgAAAAAYQpsTAAAA4IAuJ9exMgEAAADAEIoJAAAAAIbQ5gQAAAA48KHPyWWsTAAAAAAwhGICAAAAgCG0OQEAAAAO6HJyHSsTAAAAgJfJzMzUgAEDVL58eQUEBKhixYp64403ZLPZcvU8rEwAAAAAXubtt9/WpEmTNGvWLFWvXl3r169X165dFRISot69e+faeSgmAAAAAAcWL+hzWrNmjdq2batWrVpJkqKiovTJJ5/ol19+ydXz0OYEAAAA5BMZGRlKT093umVkZGR73t13363vvvtOf/zxhyRpy5Yt+uGHH9SiRYtczeOVKxPdBvYyO4LXu7tcYbMjeLU7y4abHcHrlQkLMDuC12OO3Sv9wmWzI3i94AA/syMA2SQlJWnIkCFOY4MGDdLgwYOdxl555RWlp6erSpUq8vX1VWZmpoYNG6bOnTvnah6vLCYAAAAAozy5y6l///5KTEx0GrNardme9/nnn2vOnDn6+OOPVb16dW3evFl9+vRRRESEEhISci0PxQQAAACQT1it1msWD//Ut29fvfLKK+rYsaMkKTY2Vvv371dSUlKuFhNcMwEAAAB4mfPnz8vHx/lHfV9fX2VlZeXqeViZAAAAABz4eHKfk4sefPBBDRs2TOXKlVP16tW1adMmjRo1St26dcvV81BMAAAAAF5m/PjxGjBggJ577jkdP35cERERevbZZzVw4MBcPQ/FBAAAAOBlChcurDFjxmjMmDFuPQ/FBAAAAOAg/zc55R0uwAYAAABgCMUEAAAAAENypc0pLS1NoaGhuXEoAAAAwFQWL9jNKa/keGXi7bff1meffWa/36FDB4WHh6t06dLasmVLroYDAAAA4LlyXExMnjxZZcuWlSQtX75cy5cv15IlS9SiRQv17ds31wMCAAAA8Ew5bnM6evSovZhYtGiROnTooPvuu09RUVG68847cz0gAAAAkJd86HJyWY5XJooUKaKDBw9KkpYuXarmzZtLkmw2mzIzM3M3HQAAAACPleOVifj4eD322GOqVKmSUlJS1KJFC0nSpk2bFB0dnesBAQAAAHimHBcTo0ePVlRUlA4ePKgRI0YoKChIknTkyBE999xzuR4QAAAAyEvs5uS6HBcTfn5++u9//5tt/MUXX8yVQAAAAADyB5eKiS+//NLlA7Zp08ZwGAAAAAD5h0vFRLt27Vw6mMVi4SJsAAAA5Gt0ObnOpWIiKyvL3TkAAAAA5DM53hrW0cWLF3MrBwAAAIB8JsfFRGZmpt544w2VLl1aQUFB+vPPPyVJAwYM0PTp03M9IAAAAJCXLBaLx948TY6LiWHDhmnmzJkaMWKEChYsaB+vUaOGpk2blqvhAAAAAHiuHBcTH374oaZOnarOnTvL19fXPh4XF6cdO3bkajgAAAAAnivHxcShQ4eu+UnXWVlZunz5co6O9cknn1z3sb59++Y0GgAAAHDTfCyee/M0OS4mqlWrpu+//z7b+Ny5c1W7du0cHatnz55asmRJtvEXX3xRH330UU6jAQAAAMhDOf4E7IEDByohIUGHDh1SVlaW5s+fr507d+rDDz/UokWLcnSsOXPmqFOnTlq0aJEaNGggSfrPf/6j+fPna+XKlTmNBgAAACAP5Xhlom3btvrqq6/07bffKjAwUAMHDtT27dv11Vdf6d57783RsVq1aqWJEyeqTZs22rBhg5577jl7IVGlSpWcRgMAAABumtk7NuWn3ZxyvDIhSQ0bNtTy5ctzJcBjjz2mtLQ01a9fX8WKFdPq1auveU0GAAAAAM9iqJiQpPXr12v79u2Srl5Hcfvtt7v0usTExGuOFytWTLfddpsmTpxoHxs1apTReAAAAADcLMfFxF9//aVOnTrpxx9/VGhoqCQpLS1Nd999tz799FOVKVPmX1+/adOma45HR0crPT3d/rgnLuMAAADA+/FTqOtyXEx0795dly9f1vbt2xUTEyNJ2rlzp7p27aru3btr6dKl//p6LqwGAAAAvEOOi4nVq1drzZo19kJCkmJiYjR+/Hg1bNjQcJC//vpLkm64sgEAAADAM+R4N6eyZcte88PpMjMzFRERkaNjZWVlaejQoQoJCVFkZKQiIyMVGhqqN954Q1lZWTmNBgAAANw0H4vFY2+eJsfFxDvvvKP//Oc/Wr9+vX1s/fr1euGFF/Tuu+/m6FivvfaaJkyYoLfeekubNm3Spk2bNHz4cI0fP14DBgzIaTQAAAAAechis9lsN3pSkSJFnC6IPnfunK5cuaICBa52Sf3958DAQKWmprp88oiICE2ePFlt2rRxGv/iiy/03HPP6dChQy4fy1GvBdsNvQ6uu7tcYbMjeLU7y4abHcHrlQkLMDsCcFPSL2TvEkDuCg7wMzuCV/M3vKeo+3X/bKvZEa5r2qM1zI7gxKW/xjFjxrjl5Kmpqdf8cLoqVarkqCgBAAAAcosHdhN5LJeKiYSEBLecPC4uThMmTNC4ceOcxidMmKC4uDi3nBMAAABA7ripBaaLFy/q0qVLTmPBwcEuv37EiBFq1aqVvv32W9WrV0+StHbtWh08eFBff/31zUQDAAAA4GY5vgD73Llzev7551W8eHEFBgaqSJEiTrecaNy4sf744w+1b99eaWlpSktLU3x8vHbu3HlT28wCAAAARlksFo+9eZocr0y8/PLLWrlypSZNmqQuXbrovffe06FDhzRlyhS99dZbOQ4QERGhYcOG5fh1AAAAAMyV42Liq6++0ocffqgmTZqoa9euatiwoaKjoxUZGak5c+aoc+fOOTpeWlqapk+fru3br+7AVL16dXXr1k0hISE5jQYAAAAgD+W4zSk1NVUVKlSQdPX6iL93XWrQoIGSk5NzdKz169erYsWKGj16tFJTU5WamqpRo0apYsWK2rhxY06jAQAAADfNYvHcm6fJcTFRoUIF7d27V9LVLVw///xzSVdXLEJDQ3N0rBdffFFt2rTRvn37NH/+fM2fP1979+5V69at1adPn5xGAwAAAJCHclxMdO3aVVu2bJEkvfLKK3rvvffk7++vF198UX379s3RsdavX69+/frZP/xOkgoUKKCXX37Z6RO2AQAAAHieHF8z8eKLL9r/3Lx5c+3YsUMbNmxQdHS0atasmaNjBQcH68CBA9k+uO7gwYMqXJhPWAYAAEDe8/HEfiIPleOViX+KjIxUfHy8wsLC9Mwzz+TotY8++qieeuopffbZZzp48KAOHjyoTz/9VN27d1enTp1uNhoAAAAAN7qpD61zlJKSounTp2vq1Kkuv+bdd9+VxWLRE088oStXrkiS/Pz81LNnT0PbzAIAAADIO7lWTBhhsVg0duxYJSUlac+ePZKkihUrqlChQjp58qSsVquZ8dxq6H0VFR5YMNv46j9T9fmWYyYk8j6r5s5S8vwPncbCS5VVr5EzzQnkZT6fPV1rkr/TX/v3qaDVqqo14tS1Zx+VKRdldjSv8+nHczRrxnSdPHlClWOq6JVXByg2h22luD7m1702b1yvT2bP0M7t25Ry8oSGvTtWjZo0MzuWV+E9nPvocnKdqcVEx44dNXfuXBUqVEixsbH28WPHjqlZs2baunWrienca8SqffJxeKOWCraqd4NIbTp0xrxQXqhYmSh1efUd+30fH18T03iX3zZvUKv2j6py1erKzMzUrCnj9XpiT02ePV/+AQFmx/MaS5d8rXdHJOn1QUMUGxunObNnqeezT+mLRUsVHh5udrx8j/l1v4sXLii6UoxatWmv1/r2MTuO1+E9DLPd9DUTN+PAgQPq3r2709iRI0fUpEmTbBdle5uzlzKVnvF/txolC+vE2UvadfK82dG8io+vr4JCw+y3QsF8GGJueWPkRN3bsq0iy0erQnSMEl8dqhPHjmj3zm1mR/Mqs2fNUPzDHdSu/UOqGB2t1wcNkb+/vxbOn2d2NK/A/LrfXfUb6unneqtR0+ZmR/FKvIdhNpdXJuLj4//18bS0tByf/Ouvv1ajRo2UmJioUaNG6fDhw2ratKni4uL06aef5vh4+ZWvRbqjbLBW7E41O4rXST16SKOe66ACfgVVplI1Nev4lEKKljA7llc6d+6sJCmIgi3XXL50Sdu3/a6nnn7WPubj46O77rpbv27ZZGIy78D8Ir/jPew+FvqcXOZyMRES8u8/IISEhOiJJ57I0cmLFSumZcuWqUGDBpKkRYsW6bbbbtOcOXPk42PqokmeiosorAA/X/104LTZUbxK6egqavvsywqPKKMzp1KVPP9DzRzaRz3eni5rQCGz43mVrKwsTR33jqrF1lJUhWiz43iNU2mnlJmZma1VITw8XHv3/mlSKu/B/CK/4z0MT+ByMTFjxgy3BChbtqyWL1+uhg0b6t5779Xs2bNzVA1mZGQoIyPDaSzz8iX5+mW/uNlT1YsM1bZjZ3X64hWzo3iVSrXutP+5RLmKKhNdVWN7P6ZtP61S7aYtTUzmfSaNStL+vbv1znszzY4CAADyUJ7/+r9IkSIKCwtzut111106ffq0vvrqK4WHh9vHXZGUlKSQkBCn24Z5rm9Pa7awgAKqUjxQa/anmR3F6/kHBim8VBmlHjtsdhSvMml0kn5Zm6yksdNUtDgtZLmpSGgR+fr6KiUlxWk8JSVFRYsWNSmV92B+kd/xHnYfHw++eZo8381pzJgxuXq8/v37KzEx0Wns5aV7c/Uc7nRXZKjOZGRq69GzZkfxepcuXlDqscOKbcBFgLnBZrNp8pi3tDZ5hZLGTVPJiNJmR/I6fgULqmq16vr5p7W6p9nV921WVpZ+/nmtOnZ63OR0+R/zi/yO9zA8QZ4XEwkJCbl6PKvVmu3zKPJLi5NFV1ucfj6Qpiyb2Wm8z7I5k1X5tnoKLVpCZ06laNXcmfLx8VGNu+8xO5pXmDhquFZ/u0QDho9RQKFApaaclCQFBgXJavU3OZ336JLQVQNe7afq1WuoRmxNfTR7li5cuKB27f99Uwy4hvl1v/Pnz+vQwQP2+0cOHdKunTsUHBKiEiVLmZjMO/AehtlM/ZyJ9PT0a45bLBZZrVYVLJg/igKjYooHKqyQn9bu58JrdziTckLzxw/ThbPpKhQconKVa6jb0AkKDA41O5pX+Hrh/yRJr/R23t65T/8hurdlWzMieaUHWrTUqdRUTZwwTidPnlBMlaqaOGWawmlhyBXMr/vt3LZVvXt0s9+fMHqEJOmB1m312uBhZsXyGryH3YPdnFxnsdlspv1O3MfH51//ssqUKaMnn3xSgwYNytHuTr0WbM+NePgXd5crbHYEr3ZnWT5oyN3KhPHBesjf0i9cNjuC1wsO8DM7glfzN/VX2v+u98IdZke4rnHtPOuz2Fz6a/zyyy9dPmCbNm1cfu7MmTP12muv6cknn9Qdd9whSfrll180a9Ysvf766zpx4oTeffddWa1Wvfrqqy4fFwAAAID7uVRMtGvXzqWDWSwWZWZmunzyWbNmaeTIkerQoYN97MEHH1RsbKymTJmi7777TuXKldOwYcMoJgAAAJAnfOhycplLvUNZWVku3XJSSEjSmjVrVLt27WzjtWvX1tq1ayVJDRo00IEDB7I9BwAAAIC5TN2utmzZspo+fXq28enTp6ts2bKSru6VXKRIkbyOBgAAAOAGDF36cu7cOa1evVoHDhzQpUuXnB7r3bu3y8d599139cgjj2jJkiWqW7euJGn9+vXasWOH5s6dK0lat26dHn30USMxAQAAgByjzcl1OS4mNm3apJYtW+r8+fM6d+6cwsLCdPLkSRUqVEjFixfPUTHRpk0b7dixQ1OmTNEff/whSWrRooUWLlyoqKgoSVLPnj1zGhEAAABAHshxMfHiiy/qwQcf1OTJkxUSEqKffvpJfn5+evzxx/XCCy/kOED58uX11ltv5fh1AAAAAMyV42Ji8+bNmjJlinx8fOTr66uMjAxVqFBBI0aMUEJCguLjc/aJi2lpafrll190/PhxZWVlOT32xBNP5DQeAAAAcFP40DrX5biY8PPzs3+AXPHixXXgwAFVrVpVISEhOnjwYI6O9dVXX6lz5846e/asgoODnf7iLBYLxQQAAADgwXJcTNSuXVvr1q1TpUqV1LhxYw0cOFAnT57U7NmzVaNGjRwd66WXXlK3bt00fPhwFSpUKKdRAAAAAJgox1vDDh8+XKVKlZIkDRs2TEWKFFHPnj114sQJTZ06NUfHOnTokHr37k0hAQAAAI/hY/Hcm6fJ8cpEnTp17H8uXry4li5davjk999/v9avX68KFSoYPgYAAAAAcxj6nInc0qpVK/Xt21fbtm1TbGys/Pz8nB5v06aNSckAAAAA3EiOi4ny5cv/6xXuf/75p8vHevrppyVJQ4cOzfaYxWJRZmZmTuMBAAAAN4XNnFyX42KiT58+TvcvX76sTZs2aenSperbt2+OjvXPrWABAAAA5B85Liau98F07733ntavX3/TgQAAAADkD7l2zUSLFi3Uv39/zZgxw+XXXKu9ydHAgQNvNhYAAACQIz70Obks14qJuXPnKiwsLEevWbBggdP9y5cva+/evSpQoIAqVqxIMQEAAAB4MEMfWud4AbbNZtPRo0d14sQJTZw4MUfH2rRpU7ax9PR0Pfnkk2rfvn1OowEAAADIQzkuJtq2betUTPj4+KhYsWJq0qSJqlSpctOBgoODNWTIED344IPq0qXLTR8PAAAAyIkcf6rzLSzHxcTgwYPdEMPZ6dOndfr0abefBwAAAIBxOS4mfH19deTIERUvXtxpPCUlRcWLF8/RZ0OMGzfO6b7NZtORI0c0e/ZstWjRIqfRAAAAAOShHBcTNpvtmuMZGRkqWLBgjo41evRop/t/t0wlJCSof//+OY0GAAAA3DQ2c3Kdy8XE36sIFotF06ZNU1BQkP2xzMxMJScn5/iaib179+bo+QAAAAA8h8vFxN+rCDabTZMnT5avr6/9sYIFCyoqKkqTJ0++4XHi4+M1c+ZMBQcHKz4+/l+fGxQUpOrVq6tHjx4KCQlxNSoAAACAPOByMfH3KkLTpk01f/58FSlSxNAJQ0JC7LtB3ahAyMjI0OTJk/Xjjz/qyy+/NHQ+AAAAICf40DrX5fiaiZUrV97UCR0/IduVT8vetm2b6tate1PnBAAAAJD7cryN7kMPPaS333472/iIESP0yCOP5EooRzExMVqzZk2uHxcAAADAzclxMZGcnKyWLVtmG2/RooWSk5NzJZQjX19fxcXF5fpxAQAAgGuxWDz35mlyXEycPXv2mlvA+vn5KT09PVdCAQAAAPB8OS4mYmNj9dlnn2Ub//TTT1WtWrVcCQUAAADA8+X4AuwBAwYoPj5ee/bs0T333CNJ+u677/TJJ5/of//7X64HBAAAAPKSjwe2E3mqHBcTDz74oBYuXKjhw4dr7ty5CggIUM2aNfXtt9+qcePG7sgIAAAAwAPluJiQpFatWqlVq1bZxrdu3aoaNWrcdCgAAAAAns9QMeHozJkz+uSTTzRt2jRt2LBBmZmZuZELAAAAMAUfWue6HF+A/bfk5GQ98cQTKlWqlN59913dc889+umnn3IzGwAAAAAPlqOViaNHj2rmzJmaPn260tPT1aFDB2VkZGjhwoXs5AQAAADcYlxemXjwwQcVExOjX3/9VWPGjNHhw4c1fvx4d2YDAAAA8pzZH0yXnz60zuWViSVLlqh3797q2bOnKlWq5M5MAAAAAPIBl1cmfvjhB505c0a333677rzzTk2YMEEnT550ZzYAAAAAHszlYuKuu+7S+++/ryNHjujZZ5/Vp59+qoiICGVlZWn58uU6c+aMO3MCAAAAecLH4rk3T5Pj3ZwCAwPVrVs3/fDDD/rtt9/00ksv6a233lLx4sXVpk0bd2QEAAAA4IEMbw0rSTExMRoxYoT++usvffLJJ7mVCQAAAEA+cNMfWidJvr6+ateundq1a5cbhwMAAABMY5EH9hN5qJtamQAAAABw66KYAAAAAGAIxQQAAADgwOwdm3JrN6dDhw7p8ccfV3h4uAICAhQbG6v169fn6lzlyjUTAAAAADzHqVOnVL9+fTVt2lRLlixRsWLFtGvXLhUpUiRXz0MxAQAAAHiZt99+W2XLltWMGTPsY+XLl8/189DmBAAAADgwu5Xp324ZGRlKT093umVkZGT7Gr788kvVqVNHjzzyiIoXL67atWvr/fffz/W5sthsNluuH9Vku49fMDuC10u/cNnsCF4tOMDP7Ahej/ew+5UJCzA7gldLv3DF7AjATYku7rnfI0as3GN2hOs6v3q2hgwZ4jQ2aNAgDR482GnM399fkpSYmKhHHnlE69at0wsvvKDJkycrISEh1/JQTMAQfhBzL4oJ9+M97H4UE+5FMYH8jmLCmBfuLpNtJcJqtcpqtTqNFSxYUHXq1NGaNWvsY71799a6deu0du3aXMvDNRMAAACAA4vFcz+07lqFw7WUKlVK1apVcxqrWrWq5s2bl6t5uGYCAAAA8DL169fXzp07ncb++OMPRUZG5up5KCYAAAAAL/Piiy/qp59+0vDhw7V79259/PHHmjp1qnr16pWr56HNCQAAAHCQ0w+H80R169bVggUL1L9/fw0dOlTly5fXmDFj1Llz51w9D8UEAAAA4IVat26t1q1bu/UctDkBAAAAMISVCQAAAMCBB2/m5HFYmQAAAABgCMUEAAAAAENocwIAAAAc+NDn5DJWJgAAAAAYQjEBAAAAwBDanAAAAAAH3vChdXmFlQkAAAAAhlBMAAAAADCENicAAADAAZs5uY6VCQAAAACGUEwAAAAAMIQ2JwAAAMCBj+hzchUrEwAAAAAMoZgAAAAAYAhtTgAAAIADdnNyHSsTAAAAAAyhmAAAAABgCG1OAAAAgAMf2pxcxsoEAAAAAEMoJgAAAAAYQpsTAAAA4MCH7ZxcxsoEAAAAAEMoJgAAAAAYQpsTAAAA4IAuJ9exMgEAAADAEIoJAAAAAIbQ5gQAAAA4YDcn17EyAQAAAMAQigkAAAAAhtDmBAAAADigy8l1rEwAAAAAMIRiAgAAAIAhtDkBAAAADvhtu+uYKwAAAACGUEwAAAAAMIQ2JwAAAMCBhe2cXMbKBAAAAABDKCYAAAAAGEKbEwAAAOCAJifXsTIBAAAAwBBWJkzy+ezpWpP8nf7av08FrVZVrRGnrj37qEy5KLOjeY3lX83V8kXzdPLYEUlSmcgKiu/8lGrdUd/kZN6B97D78R52v80b1+uT2TO0c/s2pZw8oWHvjlWjJs3MjuU1+D7hfswxzOYRxcTFixc1fvx4rVy5UsePH1dWVpbT4xs3bjQpmfv8tnmDWrV/VJWrVldmZqZmTRmv1xN7avLs+fIPCDA7nlcIK1pcnZ56XiVLl5VsNiUvX6x3B/9XSRM/UtmoimbHy/d4D7sf72H3u3jhgqIrxahVm/Z6rW8fs+N4Hb5PuB9z7B4+7ObkMovNZrOZHaJz585atmyZHn74YZUoUSLbdlyDBg3K0fF2H7+Qm/HyxOlTqXqszT16e/x01ah1u9lxbij9wmWzIxjS/aFm6ty9t5q2aGt2lH8VHOBndoQc4z2cN/LLe1iSyoTlrx9kGtapka9WJtIvXDE7Qo7lt+8T+VF+muPo4p77PeKjDX+ZHeG6Hr+9jNkRnHjEysSiRYv09ddfq379W3fp/ty5s5KkoOAQk5N4p6zMTP2U/J0yLl5QpWqxZsfxSryH3Yv3MLwB3yfcjzlGXvOIYqJ06dIqXLiw2TFMk5WVpanj3lG12FqKqhBtdhyvcmDvbg18oZsuX7ok/4AAJQ56R2UiK5gdy+vwHnYf3sPwFnyfcD/mOPfQ5OQ6j9jNaeTIkerXr5/279+f49dmZGQoPT3d6ZaRkeGGlO4zaVSS9u/drX6D3zY7iteJKBOptybN0RvjZqh564c06Z3B+mv/n2bH8jq8h92H9zC8Bd8n3I85hhk8opioU6eOLl68qAoVKqhw4cIKCwtzuv2bpKQkhYSEON2mjHsnj5LfvEmjk/TL2mQljZ2mosVLmB3H6xTw81PJ0mVVoXJVdXrqeUVWqKSlCz41O5ZX4T3sXryH4Q34PuF+zDHM4hFtTp06ddKhQ4c0fPjwa16A/W/69++vxMREp7GDp7Ou82zPYbPZNHnMW1qbvEJJ46apZERpsyPdErKybLp8+ZLZMbwC72Fz8B5GfsL3Cfdjjt2DzZxc5xHFxJo1a7R27VrFxcXl+LVWq1VWq9V57KLn7+Y0cdRwrf52iQYMH6OAQoFKTTkpSQoMCpLV6m9yOu/wyfQJqlX3bhUtXlIXLpzXjyuWavuvG/TK8PFmR/MKvIfdj/ew+50/f16HDh6w3z9y6JB27dyh4JAQlShZysRk3oHvE+7HHMNsHrE17G233aaJEyfqrrvuypXj5YetYVs1rHXN8T79h+jelp6/5WN+2FZzysg3tHXzOqWlnlShQkEqVyFaD3ZIUM3b7zQ72g3lh61heQ+7X35+D0v5Y2vYTet/Ue8e3bKNP9C6rV4bPMyERK7LD1vD5vfvE/lBfp5jT94a9uONnrs17GO3edbWsB5RTCxbtkxDhgzRsGHDFBsbKz8/5x+kgoODc3S8/FBM5Hf54Qex/Cw/FBP5He9h98sPxUR+lh+KCeDfeHIx8cmmQ2ZHuK5OtT2rlc0j2pweeOABSVKzZs4fFGSz2WSxWJSZmWlGLAAAAAD/wvRi4vLlq78dnDx5smJiYkxOAwAAAMBVphcTfn5+Cg8PV9OmTVWpUiWz4wAAAOAW5xGfnZBPeMRcPf7445o+fbrZMQAAAADkgOkrE5J05coVffDBB/r22291++23KzAw0OnxUaNGmZQMAAAAwPV4RDGxdetW3XbbbZKkP/74w+mxnHyAHQAAAHCz+PnTdR5RTKxcudLsCAAAAAByyCOumQAAAACQ/3jEygQAAADgKWhych0rEwAAAAAMoZgAAAAAYAhtTgAAAIADdnNyHSsTAAAAAAyhmAAAAABgCG1OAAAAgAN+2+465goAAACAIRQTAAAAAAyhzQkAAABwwG5OrmNlAgAAAIAhFBMAAAAADKHNCQAAAHBAk5PrWJkAAAAAYAjFBAAAAABDaHMCAAAAHLCZk+tYmQAAAABgCMUEAAAAAENocwIAAAAc+LCfk8tYmQAAAABgCMUEAAAAAENocwIAAAAcsJuT61iZAAAAAGAIxQQAAAAAQ2hzAgAAABxY2M3JZaxMAAAAADCEYgIAAACAIbQ5AQAAAA7Yzcl1rEwAAAAAMIRiAgAAAIAhtDkBAAAADnzYzcllrEwAAAAAMIRiAgAAAIAhtDkBAAAADtjNyXWsTAAAAAAwhGICAAAAgCG0OQEAAAAOaHNyHSsTAAAAAAyhmAAAAAC83FtvvSWLxaI+ffrk6nFpcwIAAAAcWLzsQ+vWrVunKVOmqGbNmrl+bFYmAAAAAC919uxZde7cWe+//76KFCmS68enmAAAAADyiYyMDKWnpzvdMjIyrvv8Xr16qVWrVmrevLlb8nhlm1P6hctmR/B6KecvmR3BqwUH+JkdweuVCQswO4LX433sXsyv+83b8pfZEbxadPEyZke4Lh8P7nJKSkrSkCFDnMYGDRqkwYMHZ3vup59+qo0bN2rdunVuy+OVxQQAAADgjfr376/ExESnMavVmu15Bw8e1AsvvKDly5fL39/fbXkoJgAAAIB8wmq1XrN4+KcNGzbo+PHjuu222+xjmZmZSk5O1oQJE5SRkSFfX9+bzkMxAQAAADjwht2cmjVrpt9++81prGvXrqpSpYr69euXK4WERDEBAAAAeJ3ChQurRo0aTmOBgYEKDw/PNn4z2M0JAAAAgCGsTAAAAAAOLPm/y+maVq1alevHZGUCAAAAgCEUEwAAAAAMoc0JAAAAcOANuznlFVYmAAAAABhCMQEAAADAENqcAAAAAAc+dDm5jJUJAAAAAIZQTAAAAAAwhDYnAAAAwAG7ObmOlQkAAAAAhlBMAAAAADCENicAAADAgYUuJ5exMgEAAADAEIoJAAAAAIbQ5gQAAAA4oMvJdaxMAAAAADCEYgIAAACAIbQ5AQAAAA582M7JZaxMAAAAADCEYgIAAACAIbQ5AQAAAA5ocnIdKxMAAAAADKGYAAAAAGAIbU4AAACAI/qcXMbKBAAAAABDKCYAAAAAGEKbEwAAAODAQp+Ty1iZAAAAAGAIxQQAAAAAQ2hzAgAAABxY6HJyGSsTAAAAAAyhmAAAAABgCG1OAAAAgAO6nFzHygQAAAAAQzxqZeLixYu6dOmS01hwcLBJaQAAAAD8G9NXJs6fP6/nn39exYsXV2BgoIoUKeJ0AwAAAPKUxYNvHsb0YqJv375asWKFJk2aJKvVqmnTpmnIkCGKiIjQhx9+aHY8AAAAANdhepvTV199pQ8//FBNmjRR165d1bBhQ0VHRysyMlJz5sxR586dzY4IAAAA4BpMX5lITU1VhQoVJF29PiI1NVWS1KBBAyUnJ5sZDQAAALcgiwf/52lMLyYqVKigvXv3SpKqVKmizz//XNLVFYvQ0FATkwEAAAD4N6YXE127dtWWLVskSa+88oree+89+fv768UXX1Tfvn1NTgcAAADgeky/ZuLFF1+0/7l58+basWOHNmzYoOjoaNWsWdPEZAAAALgVWTyvm8hjmV5M/FNkZKQiIyPNjuF2y7+aq+WL5unksSOSpDKRFRTf+SnVuqO+ycm807J5s/Xl7Mlq0voRPdy9j9lxvMLns6drTfJ3+mv/PhW0WlW1Rpy69uyjMuWizI7mNTZvXK9PZs/Qzu3blHLyhIa9O1aNmjQzO5bX+fTjOZo1Y7pOnjyhyjFV9MqrAxTLL7NyFXPsPqvmzlLyfOfdL8NLlVWvkTPNCYRbjunFxLhx4645brFY5O/vr+joaDVq1Ei+vr55nMy9wooWV6ennlfJ0mUlm03Jyxfr3cH/VdLEj1Q2qqLZ8bzK/l3b9eM3X6h0VLTZUbzKb5s3qFX7R1W5anVlZmZq1pTxej2xpybPni//gACz43mFixcuKLpSjFq1aa/X+vYxO45XWrrka707IkmvDxqi2Ng4zZk9Sz2ffUpfLFqq8PBws+N5BebY/YqViVKXV9+x3/fx8a6fmeDZTC8mRo8erRMnTuj8+fP2D6k7deqUChUqpKCgIB0/flwVKlTQypUrVbZsWZPT5p7b6zVyuv9o1+e0fNE87d6+lWIiF2VcOK+Zo4eoU69+Wvr5LLPjeJU3Rk50up/46lA91uYe7d65TTVq3W5SKu9yV/2Guqt+Q7NjeLXZs2Yo/uEOatf+IUnS64OGKDl5lRbOn6ennn7G5HTegTl2Px9fXwWFhpkdw6vQ5eQ60y/AHj58uOrWratdu3YpJSVFKSkp+uOPP3TnnXdq7NixOnDggEqWLOl0bYW3ycrM1JqVy5Rx8YIqVYs1O45X+WzqSNW4vZ6qxNU1O4rXO3furCQpKDjE5CSAay5fuqTt237XXfXuto/5+Pjorrvu1q9bNpmYzHswx3kj9eghjXqug8a98LjmTxiu0yePmR0JtxDTVyZef/11zZs3TxUr/t9v46Ojo/Xuu+/qoYce0p9//qkRI0booYceMjGlexzYu1sDX+imy5cuyT8gQImD3lGZyApmx/Ia67//Vgf3/KGX351mdhSvl5WVpanj3lG12FqKqkA7GfKHU2mnlJmZma3VJjw8XHv3/mlSKu/CHLtf6egqavvsywqPKKMzp1KVPP9DzRzaRz3eni5rQCGz4+EWYHoxceTIEV25ciXb+JUrV3T06FFJUkREhM6cOXPN12dkZCgjI8Np7FJGhgparbkfNpdFlInUW5Pm6Py5s/r5++806Z3BGvjuFAqKXHDqxDHNmzZGzw8ZI7+Cnv9eyO8mjUrS/r279c57M82OAgC3lEq17rT/uUS5iioTXVVjez+mbT+tUu2mLU1Mls/R5+Qy09ucmjZtqmeffVabNv3fcuemTZvUs2dP3XPPPZKk3377TeXLl7/m65OSkhQSEuJ0mzFxVJ5kv1kF/PxUsnRZVahcVZ2eel6RFSpp6YJPzY7lFQ7s2akzp0/p7cRu6h3fSL3jG2n375u0evFc9Y5vpKzMTLMjeo1Jo5P0y9pkJY2dpqLFS5gdB3BZkdAi8vX1VUpKitN4SkqKihYtalIq78Ic5z3/wCCFlyqj1GOHzY6CW4TpKxPTp09Xly5ddPvtt8vPz0/S1VWJZs2aafr06ZKkoKAgjRw58pqv79+/vxITE53Gth3NuOZzPV1Wlk2XL18yO4ZXiIm7Xa+One009tH4YSpROlL3xj8uHy/bHcwMNptNk8e8pbXJK5Q0bppKRpQ2OxKQI34FC6pqter6+ae1uqdZc0lXW/Z+/nmtOnZ63OR03oE5znuXLl5Q6rHDim3Q3OwouEWYXkyULFlSy5cv144dO/THH39IkmJiYhQTE2N/TtOmTa/7eqvVKus/WpoKnkp3T9hc9Mn0CapV924VLV5SFy6c148rlmr7rxv0yvDxZkfzCv4BgYr4R7tYQWuAAgsHZxuHMRNHDdfqb5dowPAxCigUqNSUk5KkwKAgWa3+JqfzDufPn9ehgwfs948cOqRdO3coOCREJUqWMjGZ9+iS0FUDXu2n6tVrqEZsTX00e5YuXLigdu3jzY7mNZhj91o2Z7Iq31ZPoUVL6MypFK2aO1M+Pj6qcfc9ZkfL1yz0ObnM9GLib1WqVFGVKlXMjpFn0tNOaeI7g5WWelKFCgWpXIVovTJ8vGrefueNXwx4gK8X/k+S9Erv7k7jffoP0b0t25oRyevs3LZVvXt0s9+fMHqEJOmB1m312uBhZsXyKg+0aKlTqamaOGGcTp48oZgqVTVxyjSF04KTa5hj9zqTckLzxw/ThbPpKhQconKVa6jb0AkKDA41OxpuERabzWYzM0C3bt3+9fEPPvggx8fcuN/zVybyu5TztGO5U/nwQLMjeL3gAI/5XYrXCg7wMzsCcFPmbfnL7AherfPtZcyOcF2b9l974x9PUDuysNkRnJj+f9NTp0453b98+bK2bt2qtLQ0+wXYAAAAQF6x0OXkMtOLiQULFmQby8rKUs+ePZ0+ewIAAACAZzF9a9hr8fHxUWJiokaPHm12FAAAAADXYfrKxPXs2bPnmh9mBwAAALgTXU6uM72Y+OdnRNhsNh05ckSLFy9WQkKCSakAAAAA3IjpxYTjJ19LV1ucihUrppEjR95wpycAAAAA5jG9mFi8eLFsNpsCA69uhblv3z4tXLhQkZGRKlDA9HgAAAC41dDn5DLTL8Bu166dZs+eLUlKS0vTXXfdpZEjR6pdu3aaNGmSyekAAAAAXI/pxcTGjRvVsGFDSdLcuXNVokQJ7d+/Xx9++KHGjRtncjoAAAAA12N6H9H58+dVuPDVT/JbtmyZ4uPj5ePjo7vuukv79+83OR0AAABuNRb6nFxm+spEdHS0Fi5cqIMHD+qbb77RfffdJ0k6fvy4goODTU4HAAAA4HpMLyYGDhyo//73v4qKitKdd96pevXqSbq6SlG7dm2T0wEAAAC4HtPbnB5++GE1aNBAR44cUVxcnH28WbNmat++vYnJAAAAcCuy0OXkMtOLCUkqWbKkSpYs6TR2xx13mJQGAAAAgCtMb3MCAAAAkD95xMoEAAAA4CnocnIdKxMAAAAADKGYAAAAAGAIbU4AAACAI/qcXMbKBAAAAABDKCYAAAAAGEKbEwAAAODAQp+Ty1iZAAAAAGAIxQQAAAAAQ2hzAgAAABxY6HJyGSsTAAAAAAyhmAAAAABgCG1OAAAAgAO6nFzHygQAAAAAQygmAAAAABhCmxMAAADgiD4nl7EyAQAAAMAQigkAAAAAhtDmBAAAADiw0OfkMlYmAAAAABhCMQEAAADAENqcAAAAAAcWupxcxsoEAAAAAEMoJgAAAAAYQpsTAAAA4IAuJ9exMgEAAADAEIoJAAAAAIbQ5gQAAAA4os/JZaxMAAAAADCEYgIAAACAIbQ5AQAAAA4s9Dm5jJUJAAAAwMskJSWpbt26Kly4sIoXL6527dpp586duX4eigkAAADAy6xevVq9evXSTz/9pOXLl+vy5cu67777dO7cuVw9D21OAAAAgAOLF3Q5LV261On+zJkzVbx4cW3YsEGNGjXKtfNQTAAAAAD5REZGhjIyMpzGrFarrFbrv77u9OnTkqSwsLBczeOVxcT0jYfMjuD17i5X2OwIXi28UEGzIwA3LTjAz+wIwE2pWjTY7AhANklJSRoyZIjT2KBBgzR48ODrviYrK0t9+vRR/fr1VaNGjVzN45XFBAAAAGCUJ3c59e/fX4mJiU5jN1qV6NWrl7Zu3aoffvgh1/NQTAAAAAD5hCstTY6ef/55LVq0SMnJySpTpkyu56GYAAAAALyMzWbTf/7zHy1YsECrVq1S+fLl3XIeigkAAADAkSf3ObmoV69e+vjjj/XFF1+ocOHCOnr0qCQpJCREAQEBuXYePmcCAAAA8DKTJk3S6dOn1aRJE5UqVcp+++yzz3L1PKxMAAAAAF7GZrPlyXkoJgAAAAAHFm/oc8ojtDkBAAAAMIRiAgAAAIAhtDkBAAAADix0ObmMlQkAAAAAhlBMAAAAADCENicAAADAAV1OrmNlAgAAAIAhFBMAAAAADKHNCQAAAHBEn5PLWJkAAAAAYAjFBAAAAABDaHMCAAAAHFjoc3IZKxMAAAAADKGYAAAAAGAIbU4AAACAAwtdTi5jZQIAAACAIRQTAAAAAAyhmAAAAABgCNdMAAAAAA64ZMJ1rEwAAAAAMIRiAgAAAIAhtDkBAAAADtga1nWsTAAAAAAwhGICAAAAgCG0OQEAAABO6HNyFSsTAAAAAAyhmAAAAABgCG1OAAAAgAN2c3IdKxMAAAAADKGYAAAAAGAIbU4AAACAA7qcXMfKBAAAAABDKCYAAAAAGEKbEwAAAOCA3Zxcx8oEAAAAAEMoJgAAAAAYQpsTAAAA4MDCfk4uY2UCAAAAgCEUEwAAAAAMoc0JAAAAcESXk8tYmQAAAABgCCsTJhl6X0WFBxbMNr76z1R9vuWYCYm8z6q5s5Q8/0OnsfBSZdVr5ExzAnmZ5V/N1fJF83Ty2BFJUpnICorv/JRq3VHf5GTe4/PZ07Um+Tv9tX+fClqtqlojTl179lGZclFmR/Mqn348R7NmTNfJkydUOaaKXnl1gGJr1jQ7lldhjt2H78UwG8WESUas2icfhyW0UsFW9W4QqU2HzpgXygsVKxOlLq++Y7/v4+NrYhrvEla0uDo99bxKli4r2WxKXr5Y7w7+r5ImfqSyURXNjucVftu8Qa3aP6rKVasrMzNTs6aM1+uJPTV59nz5BwSYHc8rLF3ytd4dkaTXBw1RbGyc5syepZ7PPqUvFi1VeHi42fG8AnPsXnwvdg+6nFxHm5NJzl7KVHrG/91qlCysE2cvadfJ82ZH8yo+vr4KCg2z3woFh5gdyWvcXq+Rat9RX6VKl1OpMpF6tOtz8g8opN3bt5odzWu8MXKi7m3ZVpHlo1UhOkaJrw7ViWNHtHvnNrOjeY3Zs2Yo/uEOatf+IVWMjtbrg4bI399fC+fPMzua12CO3YvvxTCb6cVE9+7dtWrVKrNjmMrXIt1RNlhr96eZHcXrpB49pFHPddC4Fx7X/AnDdfokLWTukJWZqTUrlynj4gVVqhZrdhyvde7cWUlSEEVxrrh86ZK2b/tdd9W72z7m4+Oju+66W79u2WRiMu/BHOctvhfDDKa3OZ04cUIPPPCAihUrpo4dO+rxxx9XXFyc2bHyVFxEYQX4+eqnA6fNjuJVSkdXUdtnX1Z4RBmdOZWq5PkfaubQPurx9nRZAwqZHc8rHNi7WwNf6KbLly7JPyBAiYPeUZnICmbH8kpZWVmaOu4dVYutpagK0WbH8Qqn0k4pMzMzW6tNeHi49u7906RU3oU5zht8L859FvqcXGb6ysQXX3yhI0eOaMCAAVq3bp1uu+02Va9eXcOHD9e+fftu+PqMjAylp6c73TIvX3J/8FxULzJU246d1emLV8yO4lUq1bpT1e5qrBLlKio6rq4eezlJF8+d07afVpkdzWtElInUW5Pm6I1xM9S89UOa9M5g/bWfHxDcYdKoJO3fu1v9Br9tdhQAHobvxTCT6cWEJBUpUkTPPPOMVq1apf379+vJJ5/U7NmzFR1949++JSUlKSQkxOm2Yd7UPEidO8ICCqhK8UCtocXJ7fwDgxReqoxSjx02O4rXKODnp5Kly6pC5arq9NTziqxQSUsXfGp2LK8zaXSSflmbrKSx01S0eAmz43iNIqFF5Ovrq5SUFKfxlJQUFS1a1KRU3oU5zht8L4aZPKKY+Nvly5e1fv16/fzzz9q3b59KlLjx/zT79++v06dPO91uf+iZPEibO+6KDNWZjExtPXrW7Che79LFC0o9dlhBoWFmR/FaWVk2Xc5nK4OezGazadLoJK1NXqHhY6aqZERpsyN5Fb+CBVW1WnX9/NNa+1hWVpZ+/nmtasbVNjGZ92COzcH34ptn8eD/PI3p10xI0sqVK/Xxxx9r3rx5ysrKUnx8vBYtWqR77rnnhq+1Wq2yWq1OY75+2T+/wRNZdLXF6ecDacqymZ3G+yybM1mVb6un0KIldOZUilbNnSkfHx/VuPvG7yvc2CfTJ6hW3btVtHhJXbhwXj+uWKrtv27QK8PHmx3Na0wcNVyrv12iAcPHKKBQoFJTTkqSAoOCZLX6m5zOO3RJ6KoBr/ZT9eo1VCO2pj6aPUsXLlxQu/bxZkfzGsyxe/G9GGYzvZgoXbq0UlNT9cADD2jq1Kl68MEHsxUH3iqmeKDCCvlp7X4uvHaHMyknNH/8MF04m65CwSEqV7mGug2doMDgULOjeYX0tFOa+M5gpaWeVKFCQSpXIVqvDB+vmrffaXY0r/H1wv9Jkl7p3d1pvE//Ibq3ZVszInmdB1q01KnUVE2cME4nT55QTJWqmjhlmsJpwck1zLF78b0YZrPYbDZTfyf+/vvv65FHHlFoaGiuHbPXgu25dixc293lCpsdwatVLRpsdgSvFxzgZ3YEr1cmjA/WQ/627VC62RG82m2Rnvv/uhNnPXdTnGJBpq8FODE9zdNPP212BAAAAAAGeNQF2AAAAADyD9NXJgAAAABP4nl7JnkuViYAAAAAGEIxAQAAAMAQ2pwAAAAABxb6nFzGygQAAAAAQygmAAAAABhCmxMAAADgwMJ+Ti5jZQIAAACAIRQTAAAAAAyhzQkAAABwwG5OrmNlAgAAAIAhFBMAAAAADKGYAAAAAGAIxQQAAAAAQygmAAAAABjCbk4AAACAA3Zzch0rEwAAAAAMoZgAAAAAYAhtTgAAAIADi+hzchUrEwAAAAAMoZgAAAAAYAhtTgAAAIADdnNyHSsTAAAAAAyhmAAAAABgCG1OAAAAgAO6nFzHygQAAAAAQygmAAAAABhCmxMAAADgiD4nl7EyAQAAAMAQigkAAAAAhtDmBAAAADiw0OfkMlYmAAAAABhCMQEAAADAENqcAAAAAAcWupxcxsoEAAAAAEMoJgAAAAAYQpsTAAAA4IAuJ9exMgEAAADAEIoJAAAAAIbQ5gQAAAA4os/JZaxMAAAAADCEYgIAAACAIbQ5AQAAAA4s9Dm5jJUJAAAAAIZQTAAAAABe6r333lNUVJT8/f1155136pdffsnV41NMAAAAAA4sFs+95cRnn32mxMREDRo0SBs3blRcXJzuv/9+HT9+PNfmimICAAAA8EKjRo3S008/ra5du6patWqaPHmyChUqpA8++CDXzkExAQAAAOQTGRkZSk9Pd7plZGRke96lS5e0YcMGNW/e3D7m4+Oj5s2ba+3atbmWxyt3c3qvfVWzI+RIRkaGkpKS1L9/f1mtVrPjeB3m1/2YY/dift2POXav/Dq/t0UGmx3BZfl1jj2Vvwf/hDz4zSQNGTLEaWzQoEEaPHiw09jJkyeVmZmpEiVKOI2XKFFCO3bsyLU8FpvNZsu1o8GQ9PR0hYSE6PTp0woOzj/fuPIL5tf9mGP3Yn7djzl2L+bX/ZjjW0dGRka2lQir1ZqtiDx8+LBKly6tNWvWqF69evbxl19+WatXr9bPP/+cK3k8uO4CAAAA4OhahcO1FC1aVL6+vjp27JjT+LFjx1SyZMlcy8M1EwAAAICXKViwoG6//XZ999139rGsrCx99913TisVN4uVCQAAAMALJSYmKiEhQXXq1NEdd9yhMWPG6Ny5c+ratWuunYNiwgNYrVYNGjSIC6bchPl1P+bYvZhf92OO3Yv5dT/mGNfy6KOP6sSJExo4cKCOHj2qWrVqaenSpdkuyr4ZXIANAAAAwBCumQAAAABgCMUEAAAAAEMoJgAAAAAYQjFxk5588km1a9dOktSkSRP16dPH1DzeLLfneubMmQoNDb3pXLcS3u9XOc5DXsmL9+ut/HcKADCG3Zxu0tixY8U17HnDca7nz58vPz+/mzreo48+qpYtW+ZGtFtGbv8d5Ff8uwcA4CqKiZsUEhJidoRbhuNch4WF3fTxAgICFBAQcNPHuZXk9t9BfsW/e+9w6dIlFSxY0OwYAJCv0eZ0k/7Z7pCVlaWXX35ZYWFhKlmypAYPHuz0/AMHDqht27YKCgpScHCwOnTo4PQx54MHD1atWrU0ZcoUlS1bVoUKFVKHDh10+vRpp+NMmzZNVatWlb+/v6pUqaKJEye688v0CP/WYhMVFaU333xTTzzxhIKCghQZGakvv/xSJ06csM93zZo1tX79evtr/tk28vfcz549W1FRUQoJCVHHjh115syZPPoKPd+N/g6GDx+ubt26qXDhwipXrpymTp1qTlA3c5yHqKgojRkzxunxWrVqOf3bt1gsmjZtmtq3b69ChQqpUqVK+vLLL51e8+WXX6pSpUry9/dX06ZNNWvWLFksFqWlpV0zw4kTJ1SnTh21b99eGRkZysjIUO/evVW8eHH5+/urQYMGWrdundNrVq9erTvuuENWq1WlSpXSK6+8oitXrlz361y8eLFCQkI0Z84cl+fGkzVp0kTPP/+8+vTpo6JFi8pqtcpiseibb75R7dq1FRAQoHvuuUfHjx/XkiVLVLVqVQUHB+uxxx7T+fPnzY7vEebOnavY2FgFBAQoPDxczZs317lz567ZIteuXTs9+eST9vtHjhxRq1atFBAQoPLly+vjjz/O9u9n1KhRio2NVWBgoMqWLavnnntOZ8+ezZsvLh+aOnWqIiIilJWV5TTetm1bdevWzaRUuNVQTOSyWbNmKTAwUD///LNGjBihoUOHavny5ZKuFhpt27ZVamqqVq9ereXLl+vPP//Uo48+6nSM3bt36/PPP9dXX32lpUuXatOmTXruuefsj8+ZM0cDBw7UsGHDtH37dg0fPlwDBgzQrFmz8vRr9TSjR49W/fr1tWnTJrVq1UpdunTRE088occff1wbN25UxYoV9cQTT/xre8qePXu0cOFCLVq0SIsWLdLq1av11ltv5eFXkb+NHDlSderUsb9ne/bsqZ07d5odyyMMGTJEHTp00K+//qqWLVuqc+fOSk1NlSTt3btXDz/8sNq1a6ctW7bo2Wef1WuvvXbdYx08eFANGzZUjRo1NHfuXFmtVr388suaN2+eZs2apY0bNyo6Olr333+//RyHDh1Sy5YtVbduXW3ZskWTJk3S9OnT9eabb17zHB9//LE6deqkOXPmqHPnzrk/ISaZNWuWChYsqB9//FGTJ0+WdPUXCRMmTNCaNWt08OBBdejQQWPGjNHHH3+sxYsXa9myZRo/frzJyc135MgRderUSd26ddP27du1atUqxcfHu9zy98QTT+jw4cNatWqV5s2bp6lTp+r48eNOz/Hx8dG4ceP0+++/a9asWVqxYoVefvlld3w5XuGRRx5RSkqKVq5caR9LTU3V0qVLverfLTycDTclISHB1rZtW5vNZrM1btzY1qBBA6fH69ata+vXr5/NZrPZli1bZvP19bUdOHDA/vjvv/9uk2T75ZdfbDabzTZo0CCbr6+v7a+//rI/Z8mSJTYfHx/bkSNHbDabzVaxYkXbxx9/7HSeN954w1avXr1c//o8yT/n+oUXXrA/FhkZaXv88cft948cOWKTZBswYIB9bO3atTZJ9nmcMWOGLSQkxP74oEGDbIUKFbKlp6fbx/r27Wu788473fMF5UM5+TvIysqyFS9e3DZp0qQ8Tul+jvMQGRlpGz16tNPjcXFxtkGDBtnvS7K9/vrr9vtnz561SbItWbLEZrPZbP369bPVqFHD6RivvfaaTZLt1KlTNpvt/96vO3bssJUtW9bWu3dvW1ZWlv14fn5+tjlz5thff+nSJVtERIRtxIgRNpvNZnv11VdtMTEx9tfYbDbbe++9ZwsKCrJlZmbabLb/+zudMGGCLSQkxLZq1Srjk+SBGjdubKtdu7b9/sqVK22SbN9++619LCkpySbJtmfPHvvYs88+a7v//vvzNKsn2rBhg02Sbd++fdke++f3A5vNZmvbtq0tISHBZrPZbNu3b7dJsq1bt87++K5du2ySsv37cfS///3PFh4enhvxvVbbtm1t3bp1s9+fMmWKLSIiwv7vGnA3ViZyWc2aNZ3ulypVyv6bl+3bt6ts2bIqW7as/fFq1aopNDRU27dvt4+VK1dOpUuXtt+vV6+esrKytHPnTp07d0579uzRU089paCgIPvtzTff1J49e9z81Xk2x7n/+2PiY2Njs4398zdhjqKiolS4cGH7fce/P9yY49+BxWJRyZIlmb//z3FuAgMDFRwcbJ+bnTt3qm7duk7Pv+OOO7Id48KFC2rYsKHi4+M1duxYWSwWSVdX1C5fvqz69evbn+vn56c77rjD/r1l+/btqlevnv01klS/fn2dPXtWf/31l31s7ty5evHFF7V8+XI1btw4F75yz3L77bdnG/vn945ChQqpQoUKTmO8j6W4uDg1a9ZMsbGxeuSRR/T+++/r1KlTLr12586dKlCggG677Tb7WHR0tIoUKeL0vG+//VbNmjVT6dKlVbhwYXXp0kUpKSm0mf2Lzp07a968ecrIyJB0tXuhY8eO8vHhRzzkDd5pueyfu9tYLJZsvYw34+/e0ffff1+bN2+237Zu3aqffvop186THznO/d8/MF1r7N/+Ptz99+ftbsX58/Hxydbmcfny5WzPy425sVqtat68uRYtWqRDhw7lPKwLateurWLFiumDDz7wyh2rAgMDs4398/vErfg+doWvr6+WL1+uJUuWqFq1aho/frxiYmK0d+9el/8d/Jt9+/apdevWqlmzpubNm6cNGzbovffek3T1Ynlc24MPPiibzabFixfr4MGD+v7772lxQp6imMhDVatW1cGDB3Xw4EH72LZt25SWlqZq1arZxw4cOKDDhw/b7//000/y8fFRTEyMSpQooYiICP3555+Kjo52upUvXz5Pvx4AUrFixXTkyBH7/fT0dO3duzdHx4iJiXHaHEBStounpauFy+zZs3X77beradOm9u8TFStWtF8H8LfLly9r3bp19u8tVatW1dq1a51+4Pvxxx9VuHBhlSlTxj5WsWJFrVy5Ul988YX+85//5OjrgPezWCyqX7++hgwZok2bNqlgwYJasGBBtn8HmZmZ2rp1q/1+TEyMrly5ok2bNtnHdu/e7bSysWHDBmVlZWnkyJG66667VLlyZaf/F+La/P39FR8frzlz5uiTTz5RTEyM0woQ4G4UE3moefPmio2NVefOnbVx40b98ssveuKJJ9S4cWPVqVPH/jx/f38lJCRoy5Yt+v7779W7d2916NBBJUuWlHT1Qs6kpCSNGzdOf/zxh3777TfNmDFDo0aNMutLA25Z99xzj2bPnq3vv/9ev/32mxISEuTr65ujYzz77LPasWOH+vXrpz/++EOff/65Zs6cKUlObUnS1d8Oz5kzR3Fxcbrnnnt09OhRBQYGqmfPnurbt6+WLl2qbdu26emnn9b58+f11FNPSZKee+45HTx4UP/5z3+0Y8cOffHFFxo0aJASExOztUNUrlxZK1eu1Lx58/gQO9j9/PPPGj58uNavX68DBw5o/vz5OnHihKpWrap77rlHixcv1uLFi7Vjxw717NnTaSeyKlWqqHnz5nrmmWf0yy+/aNOmTXrmmWcUEBBgf49HR0fr8uXLGj9+vP7880/Nnj3bfpE8/l3nzp21ePFiffDBB6xKIM9RTOQhi8WiL774QkWKFFGjRo3UvHlzVahQQZ999pnT86KjoxUfH6+WLVvqvvvuU82aNZ22fu3evbumTZumGTNmKDY2Vo0bN9bMmTNZmQBM0L9/fzVu3FitW7dWq1at1K5dO1WsWDFHxyhfvrzmzp2r+fPnq2bNmpo0aZJ9Nyer1Zrt+QUKFNAnn3yi6tWr27cyfeutt/TQQw+pS5cuuu2227R7925988039p700qVL6+uvv9Yvv/yiuLg49ejRQ0899ZRef/31a2aKiYnRihUr9Mknn+ill17K4azAGwUHBys5OVktW7ZU5cqV9frrr2vkyJFq0aKFunXrpoSEBPsvyCpUqKCmTZs6vf7DDz9UiRIl1KhRI7Vv315PP/20ChcuLH9/f0lXr8kYNWqU3n77bdWoUUNz5sxRUlKSGV9qvnPPPfcoLCxMO3fu1GOPPWZ2HNxiLDZvbIrNQ506dZKvr68++uijXDne4MGDtXDhQm3evDlXjudNcnuukXP8HVyVF/MwbNgwTZ482aktEvAmf/31l8qWLWu/6BpA/sTKhEFXrlzRtm3btHbtWlWvXt3sOF6NuTYffwdXuXMeJk6cqHXr1tnbO9555x0lJCTk6jkAM61YsUJffvml9u7dqzVr1qhjx46KiopSo0aNzI4G4CZQTBi0detW1alTR9WrV1ePHj3MjuPVmGvz8XdwlTvnYdeuXWrbtq2qVaumN954Qy+99JLTp2gD+d3ly5f16quvqnr16mrfvr2KFSumVatWZds9C0D+QpsTAAAAAENYmQAAAABgCMUEAAAAAEMoJgAAAAAYQjEBAAAAwBCKCQAAAACGUEwAQA49+eSTateunf1+kyZN1KdPnzzPsWrVKlksFqWlpbntHP/8Wo3Ii5wAAHNQTADwCk8++aQsFossFosKFiyo6OhoDR06VFeuXHH7uefPn6833njDpefm9Q/WUVFRGjNmTJ6cCwBw6ylgdgAAyC0PPPCAZsyYoYyMDH399dfq1auX/Pz81L9//2zPvXTpkgoWLJgr5w0LC8uV4wAAkN+wMgHAa1itVpUsWVKRkZHq2bOnmjdvri+//FLS/7XrDBs2TBEREYqJiZEkHTx4UB06dFBoaKjCwsLUtm1b7du3z37MzMxMJSYmKjQ0VOHh4Xr55Zf1z8/6/GebU0ZGhvr166eyZcvKarUqOjpa06dP1759+9S0aVNJUpEiRWSxWPTkk09KkrKyspSUlKTy5csrICBAcXFxmjt3rtN5vv76a1WuXFkBAQFq2rSpU04jMjMz9dRTT9nPGRMTo7Fjx17zuUOGDFGxYsUUHBysHj166NKlS/bHXMnuaP/+/XrwwQdVpEgRBQYGqnr16vr6669v6msBAJiDlQkAXisgIEApKSn2+999952Cg4O1fPlySdLly5d1//33q169evr+++9VoEABvfnmm3rggQf066+/qmDBgho5cqRmzpypDz74QFWrVtXIkSO1YMEC3XPPPdc97xNPPKG1a9dq3LhxiouL0969e3Xy5EmVLVtW8+bN00MPPaSdO3cqODhYAQEBkqSkpCR99NFHmjx5sipVqqTk5GQ9/vjjKlasmBo3bqyDBw8qPj5evXr10jPPPKP169frpZdeuqn5ycrKUpkyZfS///1P4eHhWrNmjZ555hmVKlVKHTp0cJo3f39/rVq1Svv27VPXrl0VHh6uYcOGuZT9n3r16qVLly4pOTlZgYGB2rZtm4KCgm7qawEAmMQGAF4gISHB1rZtW5vNZrNlZWXZli9fbrNarbb//ve/9sdLlChhy8jIsL9m9uzZtpiYGFtWVpZ9LCMjwxYQEGD75ptvbDabzVaqVCnbiBEj7I9fvnzZVqZMGfu5bDabrXHjxrYXXnjBZrPZbDt37rRJsi1fvvyaOVeuXGmTZDt16pR97OLFi7ZChQrZ1qxZ4/Tcp556ytapUyebzWaz9e/f31atWjWnx/v165ftWP8UGRlpGz169HUf/6devXrZHnroIfv9hIQEW1hYmO3cuXP2sUmTJtmCgoJsmZmZLmX/59ccGxtrGzx4sMuZAACei5UJAF5j0aJFCgoK0uXLl5WVlaXHHntMgwcPtj8eGxvrdJ3Eli1btHv3bhUuXNjpOBcvXtSePXt0+vRpHTlyRHfeeaf9sQIFCqhOnTrZWp3+tnnzZvn6+l7zN/LXs3v3bp0/f1733nuv0/ilS5dUu3ZtSdL27dudckhSvXr1XD7H9bz33nv64IMPdODA/2vvXkJSW8MwAL9tNgmZDaILFVpBRgZl2SBqIERCjYou4CBC6EJmdzKogUQEGZSjIIMmOSgqCBxkUJNuBEoXahJdEKHLKKLJCkOyfUYJVrs868A5++z9PrAGrvX5+30T4eVfS6/g9/sRCARQUFAQVqNWqxETExP2uYIg4Pr6GoIgfNn7W93d3Whvb8fGxgZ0Oh3q6uqQn5//j2chIqJ/H8MEEf02ysrKYLfbER0djdTUVHz/Hv4VJ5VKw14LgoCioiLMz8+/WysxMVFUD6+3Lf0dgiAAAFwuF9LS0sKuSSQSUX1EYnFxEWazGTabDSUlJZDJZJiYmIDH44l4DTG9t7S0oKKiAi6XCxsbG7BarbDZbOjq6hI/DBER/ScYJojotyGVSpGVlRVxvUajwdLSEpKSkhAXF/dhTUpKCjweD7RaLQDg+fkZh4eH0Gg0H9bn5eXh5eUF29vb0Ol0766/7owEg8HQudzcXEgkElxdXf10R0OlUoUeJn/ldru/HvITe3t7KC0thclkCp3zer3v6k5OTuD3+0NBye12IzY2FnK5HPHx8V/2/hG5XA6j0Qij0YihoSHMzs4yTBAR/Q/x15yI6I/V0NCAhIQEVFdXY3d3Fz6fD1tbW+ju7sbNzQ0AoKenB+Pj43A6nTg7O4PJZPr0PyIyMjJgMBjQ1NQEp9MZWnN5eRkAkJ6ejqioKKyuruLu7g6CIEAmk8FsNqOvrw8OhwNerxdHR0eYmpqCw+EAABiNRlxeXmJgYADn5+dYWFjA3NxcRHPe3t7i+Pg47Hh4eIBSqcTBwQHW19dxcXEBi8WC/f39d+8PBAJobm7G6ekp1tbWMDw8jM7OTnz79i2i3t/q7e3F+vo6fD4fjo6OsLm5CZVKFdEsRET0a2GYIKI/VkxMDHZ2dqBQKFBbWwuVSoXm5mY8PT2Fdir6+/vR2NgIg8EQuhWopqbm03Xtdjvq6+thMpmQk5OD1tZWPD4+AgDS0tIwMjKCwcFBJCcno7OzEwAwOjoKi8UCq9UKlUqFyspKuFwuZGZmAgAUCgVWVlbgdDqhVqsxMzODsbGxiOacnJxEYWFh2OFyudDW1oba2lro9XoUFxfj/v4+bJfiVXl5OZRKJbRaLfR6PaqqqsKeRfmq97eCwSA6OjpCtdnZ2Zieno5oFiIi+rVE/fjZU4RERERERESf4M4EERERERGJwjBBRERERESiMEwQEREREZEoDBNERERERCQKwwQREREREYnCMEFERERERKIwTBARERERkSgME0REREREJArDBBERERERicIwQUREREREojBMEBERERGRKH8BIQWlkeaPc1IAAAAASUVORK5CYII="},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"resnet50_model.avgpool = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n#resnet18_model.avgpool = nn.Sequential()\nresnet50_model.fc = nn.Sequential()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class YoloV1(nn.Module):\n    def __init__(self, in_channels=3, out_conv_channels=2048, **kwargs):\n        super(YoloV1, self).__init__()\n        self.in_channels = in_channels\n        self.model_channels = out_conv_channels\n        self.darknet = resnet50_model\n        self.fcs = self._create_fcs(**kwargs)\n        \n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x, start_dim=1))\n    \n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        \"\"\"\n        В изначальной статье используется nn.Linear(1024 * S * S, 4096), но не 496. \n        Также у последнего слоя будет изменена размерность до (S, S, 13) где C+B*5 = 13\n        \"\"\"\n        S, B, C = split_size, num_boxes, num_classes\n        return nn.Sequential(\n            nn.Flatten(), \n            nn.Linear(self.model_channels * S * S, 496), \n            nn.Dropout(0.0), \n            nn.LeakyReLU(0.1), \n            nn.Linear(496, S * S * (C + B * 5))\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.749761Z","iopub.execute_input":"2024-12-24T17:57:25.750021Z","iopub.status.idle":"2024-12-24T17:57:25.756457Z","shell.execute_reply.started":"2024-12-24T17:57:25.749997Z","shell.execute_reply":"2024-12-24T17:57:25.755627Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"Этот код на Python вычисляет пересечение над объединением (Intersection over Union, IoU) для пар предсказанных и истинных ограничивающих рамок. IoU — это метрика, которая часто используется в задачах компьютерного зрения для оценки качества предсказанных ограничивающих рамок. Вот как работает этот код:\n\n1. Параметры функции:\n\n   • boxes_preds — тензор с предсказанными ограничивающими рамками размером (BATCH_SIZE, 4).\n\n   • boxes_labels — тензор с истинными ограничивающими рамками размером (BATCH_SIZE, 4).\n\n   • box_format — формат представления рамок: 'midpoint' (центр и размеры) или 'corners' (координаты углов).\n\n2. Преобразование формата рамок:\n\n   • Если формат 'midpoint', рамки преобразуются из формата (x, y, w, h) в (x1, y1, x2, y2).\n\n   • Если формат 'corners', рамки уже в нужном формате (x1, y1, x2, y2).\n\n3. Вычисление координат пересечения:\n\n   • Используется функция torch.max для вычисления верхних левых углов пересечения.\n\n   • Используется функция torch.min для вычисления нижних правых углов пересечения.\n\n4. Вычисление площади пересечения:\n\n   • Площадь пересечения вычисляется как произведение высоты и ширины пересекающейся области. Для случая, когда рамки не пересекаются, используется .clamp(0), чтобы избежать отрицательных значений.\n\n5. Вычисление площадей отдельных рамок:\n\n   • Площади предсказанных и истинных рамок вычисляются на основе их координат.\n\n6. Вычисление IoU:\n\n   • IoU вычисляется как отношение площади пересечения к объединенной площади двух рамок (площадь первой рамки + площадь второй рамки - площадь пересечения). Небольшая константа 1e-6 добавляется к знаменателю для предотвращения деления на ноль.\n\nЭтот код полезен для оценки качества алгоритмов обнаружения объектов, так как позволяет количественно оценить, насколько хорошо предсказанные ограничивающие рамки соответствуют истинным объектам на изображении.\n","metadata":{}},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n    if box_format == 'midpoint':\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n        \n    if box_format == 'corners':\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4] \n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n    \n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n    \n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    \n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n    \n    return intersection / (box1_area + box2_area - intersection + 1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.758449Z","iopub.execute_input":"2024-12-24T17:57:25.758825Z","iopub.status.idle":"2024-12-24T17:57:25.770705Z","shell.execute_reply.started":"2024-12-24T17:57:25.758800Z","shell.execute_reply":"2024-12-24T17:57:25.770045Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    \"\"\"\n    Выполняет подавление немаксимумов (Non Max Suppression) для заданных ограничивающих рамок.\n    Параметры:\n        bboxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        threshold (float): порог для удаления предсказанных рамок (независимо от IoU)\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n    Возвращает:\n        list: ограничивающие рамки после выполнения NMS с заданным порогом IoU\n    \"\"\"\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.771578Z","iopub.execute_input":"2024-12-24T17:57:25.771800Z","iopub.status.idle":"2024-12-24T17:57:25.787506Z","shell.execute_reply.started":"2024-12-24T17:57:25.771778Z","shell.execute_reply":"2024-12-24T17:57:25.786775Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=7\n):\n    \"\"\"\n    Вычисляет среднюю точность (mean average precision).\n    Параметры:\n        pred_boxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): аналогично pred_boxes, но для всех правильных\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n        num_classes (int): количество классов\n    Возвращает:\n        float: значение mAP для всех классов при заданном пороге IoU\n    \"\"\"\n\n    # список для хранения всех AP для соответствующих классов\n    average_precisions = []\n\n    # используется для численной стабильности позже\n    epsilon = 1e-6\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Проходим через все предсказания и цели,\n        # и добавляем только те, которые принадлежат\n        # текущему классу c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # находим количество рамок для каждого обучающего примера\n        # Counter здесь находит, сколько истинных рамок мы получаем\n        # для каждого обучающего примера. Например, если у img 0 их 3,\n        # а у img 1 их 5, то мы получим словарь с:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # Затем мы проходим через каждый ключ и значение в этом словаре\n        # и преобразуем в следующее (относительно того же примера):\n        # amount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # сортируем по вероятностям рамок, которые находятся в индексе 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # Если ничего не существует для этого класса, то можно безопасно пропустить\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Берем только те истинные рамки, у которых такой же индекс\n            # обучения, как у предсказания\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # засчитываем истинное предсказание только один раз\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # истинно положительное и добавляем эту рамку в просмотренные\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # если IOU ниже порога, то предсказание является ложноположительным\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz для численного интегрирования\n        average_precisions.append(torch.trapz(precisions, recalls))\n    return sum(average_precisions) / len(average_precisions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.788672Z","iopub.execute_input":"2024-12-24T17:57:25.788946Z","iopub.status.idle":"2024-12-24T17:57:25.803141Z","shell.execute_reply.started":"2024-12-24T17:57:25.788916Z","shell.execute_reply":"2024-12-24T17:57:25.802431Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\n\ndef convert_cellboxes(predictions, S=7, C=7):\n    \"\"\"\n    Преобразует ограничивающие рамки, полученные от Yolo с\n    размером разбиения изображения S, в соотношения для всего изображения,\n    а не относительно ячеек. Пытались сделать это векторизованно,\n    но это привело к довольно сложному для чтения коду...\n    Использовать как черный ящик? Или реализовать более интуитивно понятный метод,\n    используя 2 цикла for, которые перебирают range(S) и преобразуют их\n    по одному, что приведет к более медленной, но более читаемой реализации.\n    \"\"\"\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, S, S, C + 10)\n    bboxes1 = predictions[..., C + 1:C + 5]\n    bboxes2 = predictions[..., C + 6:C + 10]\n    scores = torch.cat(\n        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n    \ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.804103Z","iopub.execute_input":"2024-12-24T17:57:25.804489Z","iopub.status.idle":"2024-12-24T17:57:25.820367Z","shell.execute_reply.started":"2024-12-24T17:57:25.804463Z","shell.execute_reply":"2024-12-24T17:57:25.819648Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=7, transform=None):\n        self.annotations = df\n        self.files_dir = files_dir\n        self.transform = transform\n        self.S = S\n        self.B = B\n        self.C = C\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n        boxes = []\n        names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\n        class_dictionary = dict((i, names[i]) for i in range(7))\n        with open(label_path,'r') as file:\n            klass, centerx, centery, boxwidth, boxheight = map(float, file.readline()[:-1].split())\n            boxes.append([klass, centerx, centery, boxwidth, boxheight])\n                \n        boxes = torch.tensor(boxes)\n        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        image = image.convert(\"RGB\")\n\n        if self.transform:\n            # image = self.transform(image)\n            image, boxes = self.transform(image, boxes)\n\n        # Convert To Cells\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box.tolist()\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n\n            \"\"\"\n            Вычисление ширины и высоты ячейки ограничивающей рамки\n            относительно ячейки выполняется следующим образом, на примере\n            ширины:\n            \n            width_pixels = (width*self.image_width)\n            cell_pixels = (self.image_width)\n            \n            Затем, чтобы найти ширину относительно ячейки, достаточно:\n            width_pixels/cell_pixels, что при упрощении приводит к\n            формулам ниже.\n            \"\"\"\n            width_cell, height_cell = (\n                width * self.S,\n                height * self.S,\n            )\n\n            # If no object already found for specific cell i,j\n            # Note: This means we restrict to ONE object\n            # per cell!\n#             print(i, j)\n            if label_matrix[i, j, self.C] == 0:\n                # Set that there exists an object\n                label_matrix[i, j, self.C] = 1\n\n                # Box coordinates\n                box_coordinates = torch.tensor(\n                    [x_cell, y_cell, width_cell, height_cell]\n                )\n\n                label_matrix[i, j, 4:8] = box_coordinates\n\n                # Set one hot encoding for class_label\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:26.998188Z","iopub.execute_input":"2024-12-24T17:57:26.998526Z","iopub.status.idle":"2024-12-24T17:57:27.015278Z","shell.execute_reply.started":"2024-12-24T17:57:26.998491Z","shell.execute_reply":"2024-12-24T17:57:27.014430Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    \"\"\"\n    Calculate the loss for yolo (v1) model\n    \"\"\"\n\n    def __init__(self, S=7, B=2, C=7):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss(reduction=\"sum\")\n\n        \"\"\"\n        S is split size of image (in paper 7),\n        B is number of boxes (in paper 2),\n        C is number of classes (in paper 20, in dataset 3),\n        \"\"\"\n        self.S = S\n        self.B = B\n        self.C = C\n\n        # These are from Yolo paper, signifying how much we should\n        # pay loss for no object (noobj) and the box coordinates (coord)\n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n\n    def forward(self, predictions, target):\n        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes with target bbox\n        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        # Note that bestbox will be indices of 0, 1 for which bbox was best\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n\n        # ======================== #\n        #   FOR BOX COORDINATES    #\n        # ======================== #\n\n        # Set boxes with no object in them to 0. We only take out one of the two \n        # predictions, which is the one with highest Iou calculated previously.\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., self.C + 6:self.C + 10]\n                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n            )\n        )\n\n        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # ==================== #\n        #   FOR OBJECT LOSS    #\n        # ==================== #\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n        )\n\n        # ======================= #\n        # ЛОСС ОТСУТСТВИЯ КЛАССА  #\n        # ======================= #\n\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n        )\n\n        # ================== #\n        #   КЛАССОВЫЙ ЛОСС   #\n        # ================== #\n\n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.016536Z","iopub.execute_input":"2024-12-24T17:57:27.016858Z","iopub.status.idle":"2024-12-24T17:57:27.032885Z","shell.execute_reply.started":"2024-12-24T17:57:27.016823Z","shell.execute_reply":"2024-12-24T17:57:27.032051Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"LEARNING_RATE = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\nWEIGHT_DECAY = 0.1\nEPOCHS = 20\nNUM_WORKERS = 2\nPIN_MEMORY = True\nLOAD_MODEL = False\nLOAD_MODEL_FILE = \"model.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.033854Z","iopub.execute_input":"2024-12-24T17:57:27.034100Z","iopub.status.idle":"2024-12-24T17:57:27.048520Z","shell.execute_reply.started":"2024-12-24T17:57:27.034077Z","shell.execute_reply":"2024-12-24T17:57:27.047755Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n    \n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss = loss.item())\n        \n    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.049546Z","iopub.execute_input":"2024-12-24T17:57:27.049885Z","iopub.status.idle":"2024-12-24T17:57:27.064443Z","shell.execute_reply.started":"2024-12-24T17:57:27.049850Z","shell.execute_reply":"2024-12-24T17:57:27.063636Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.065334Z","iopub.execute_input":"2024-12-24T17:57:27.065627Z","iopub.status.idle":"2024-12-24T17:57:27.076324Z","shell.execute_reply.started":"2024-12-24T17:57:27.065603Z","shell.execute_reply":"2024-12-24T17:57:27.075431Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def main():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    train_dataset = BTSDataset(\n        transform=transform,\n        files_dir=files_dir,\n        df = train\n    )\n\n    test_dataset = BTSDataset(\n        transform=transform, \n        files_dir=files_dir,\n        df = test\n    )\n\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    for epoch in range(EPOCHS):\n        train_fn(train_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            train_loader, model, iou_threshold=0.9, threshold=0.9\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.2, box_format=\"midpoint\"\n        )\n        print(f\"Train mAP: {mean_avg_prec}\")\n        \n        scheduler.step(mean_avg_prec)\n    \n    checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.077397Z","iopub.execute_input":"2024-12-24T17:57:27.077802Z","iopub.status.idle":"2024-12-24T17:57:27.090552Z","shell.execute_reply.started":"2024-12-24T17:57:27.077764Z","shell.execute_reply":"2024-12-24T17:57:27.089751Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.091624Z","iopub.execute_input":"2024-12-24T17:57:27.091973Z","iopub.status.idle":"2024-12-24T17:57:28.448646Z","shell.execute_reply.started":"2024-12-24T17:57:27.091937Z","shell.execute_reply":"2024-12-24T17:57:28.447278Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n  0%|          | 0/21 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[47], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     32\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     33\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     34\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m get_bboxes(\n\u001b[1;32m     42\u001b[0m         train_loader, model, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     mean_avg_prec \u001b[38;5;241m=\u001b[39m mean_average_precision(\n\u001b[1;32m     46\u001b[0m         pred_boxes, target_boxes, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     )\n","Cell \u001b[0;32mIn[45], line 7\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[1;32m      6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 7\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(out, y)\n\u001b[1;32m      9\u001b[0m     mean_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mYoloV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdarknet(x)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1000 and 100352x496)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x1000 and 100352x496)","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"LOAD_MODEL = True\nEPOCHS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.449730Z","iopub.status.idle":"2024-12-24T17:57:28.450178Z","shell.execute_reply.started":"2024-12-24T17:57:28.449949Z","shell.execute_reply":"2024-12-24T17:57:28.449972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predictions():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    test_dataset = BTSDataset(\n        transform=transform, \n        df=test,\n        files_dir=files_dir\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n        \n    for epoch in range(EPOCHS):\n        model.eval()\n        train_fn(test_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            test_loader, model, iou_threshold=0.9, threshold=0.9\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.9, box_format=\"midpoint\"\n        )\n        print(f\"Test mAP: {mean_avg_prec}\")\n\n\npredictions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.451137Z","iopub.status.idle":"2024-12-24T17:57:28.451590Z","shell.execute_reply.started":"2024-12-24T17:57:28.451342Z","shell.execute_reply":"2024-12-24T17:57:28.451364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n\noptimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n)\n\nloss_fn = YoloLoss()\n\nload_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\ntest_dataset = BTSDataset(\n        transform=transform, \n        df=test,\n        files_dir=files_dir\n    )\n\ntest_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.453416Z","iopub.status.idle":"2024-12-24T17:57:28.453857Z","shell.execute_reply.started":"2024-12-24T17:57:28.453633Z","shell.execute_reply":"2024-12-24T17:57:28.453655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bboxes_images(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_images = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        all_images.append(x)\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes, torch.cat(all_images, dim=0)\n\n\npred_boxes, target_boxes, images = get_bboxes_images(\n            test_loader, model, iou_threshold=0.9, threshold=0.9\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.454893Z","iopub.status.idle":"2024-12-24T17:57:28.455357Z","shell.execute_reply.started":"2024-12-24T17:57:28.455116Z","shell.execute_reply":"2024-12-24T17:57:28.455141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [{'num_imag': pred_box[0], 'class': pred_box[1], 'conf': pred_box[2], 'box': list(np.array(pred_box[3:])*448)} \\\n         for pred_box in pred_boxes]\npreds[:10:1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.457007Z","iopub.status.idle":"2024-12-24T17:57:28.457453Z","shell.execute_reply.started":"2024-12-24T17:57:28.457211Z","shell.execute_reply":"2024-12-24T17:57:28.457234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.459055Z","iopub.status.idle":"2024-12-24T17:57:28.459514Z","shell.execute_reply.started":"2024-12-24T17:57:28.459260Z","shell.execute_reply":"2024-12-24T17:57:28.459283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncolor_classes = {\n    0:'red',\n    1:\"blue\",\n    2:\"green\",\n    3:'purple',\n    4:'orange',\n    5:'yellow',\n    6:'black'\n}\n\nfig, ax = plt.subplots(5, 5, figsize = (10, 10))\nfor idx, image in enumerate(images):\n    try:\n        ax[idx//5, idx%5].imshow(image.permute((1, 2, 0)).detach().numpy())\n    except IndexError:\n        pass\nfor box in preds:\n    # Create a Rectangle patch\n    box_rect = box.get('box')\n    center_x = box_rect[0]\n    center_y = box_rect[1]\n    width = box_rect[2]\n    height = box_rect[3]\n    rect = patches.Rectangle((center_x, center_y), \n                             width, height, \n                             linewidth=1, \n                             edgecolor=color_classes.get(int(box.get(\"class\"))), \n                             facecolor='none')\n    idx_box = box.get(\"num_imag\")\n    # Add the patch to the Axes\n    try:\n        ax[idx_box//5, idx_box%5].add_patch(rect)\n    except IndexError:\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.461431Z","iopub.status.idle":"2024-12-24T17:57:28.462316Z","shell.execute_reply.started":"2024-12-24T17:57:28.462074Z","shell.execute_reply":"2024-12-24T17:57:28.462099Z"}},"outputs":[],"execution_count":null}]}