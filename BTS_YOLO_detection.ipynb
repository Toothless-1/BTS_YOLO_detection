{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10286471,"sourceType":"datasetVersion","datasetId":6365696}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport os\nimport PIL\nimport skimage\nfrom skimage import io\nimport numpy as np\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.transforms.functional as FT\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchinfo import summary\n\nseed = 42\nimport cv2\nimport xml.etree.ElementTree as ET\n\ntorch.manual_seed(seed)\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.062974Z","iopub.execute_input":"2024-11-16T16:23:42.063341Z","iopub.status.idle":"2024-11-16T16:23:42.071498Z","shell.execute_reply.started":"2024-11-16T16:23:42.063308Z","shell.execute_reply":"2024-11-16T16:23:42.070572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import models\nresnet18_model = models.mobilenetv3(pretrained=True)\nresnet18_model.avgpool = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n#resnet18_model.avgpool = nn.Sequential()\nresnet18_model.fc = nn.Sequential()\n\nfor param in resnet18_model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.094495Z","iopub.execute_input":"2024-11-16T16:23:42.094814Z","iopub.status.idle":"2024-11-16T16:23:42.548406Z","shell.execute_reply.started":"2024-11-16T16:23:42.094782Z","shell.execute_reply":"2024-11-16T16:23:42.5474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(resnet18_model, input_size=(16, 3, 448, 448))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.552453Z","iopub.execute_input":"2024-11-16T16:23:42.552818Z","iopub.status.idle":"2024-11-16T16:23:42.718309Z","shell.execute_reply.started":"2024-11-16T16:23:42.55278Z","shell.execute_reply":"2024-11-16T16:23:42.717339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class YoloV1(nn.Module):\n    def __init__(self, in_channels=3, out_conv_channels=512, **kwargs):\n        super(YoloV1, self).__init__()\n        self.in_channels = in_channels\n        self.model_channels = out_conv_channels\n        self.darknet = resnet18_model\n        self.fcs = self._create_fcs(**kwargs)\n        \n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x, start_dim=1))\n    \n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        \"\"\"\n        В изначальной статье используется nn.Linear(1024 * S * S, 4096), но не 496. \n        Также у последнего слоя будет изменена размерность до (S, S, 13) где C+B*5 = 13\n        \"\"\"\n        S, B, C = split_size, num_boxes, num_classes\n        return nn.Sequential(\n            nn.Flatten(), \n            nn.Linear(self.model_channels * S * S, 496), \n            nn.Dropout(0.0), \n            nn.LeakyReLU(0.1), \n            nn.Linear(496, S * S * (C + B * 5))\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.720017Z","iopub.execute_input":"2024-11-16T16:23:42.72041Z","iopub.status.idle":"2024-11-16T16:23:42.729051Z","shell.execute_reply.started":"2024-11-16T16:23:42.720367Z","shell.execute_reply":"2024-11-16T16:23:42.728095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# architecture_config = [\n#     #Tuple: (kernel_size, number of filters, strides, padding)\n#     (7, 64, 2, 3),\n#     #\"M\" = Max Pool Layer\n#     \"M\",\n#     (3, 192, 1, 1),\n#     \"M\",\n#     (1, 128, 1, 0),\n#     (3, 256, 1, 1),\n#     (1, 256, 1, 0),\n#     (3, 512, 1, 1),\n#     \"M\",\n#     #List: [(tuple), (tuple), how many times to repeat]\n#     [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n#     (1, 512, 1, 0),\n#     (3, 1024, 1, 1),\n#     \"M\",\n#     [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n#     (3, 1024, 1, 1),\n#     (3, 1024, 2, 1),\n#     (3, 1024, 1, 1),\n#     (3, 1024, 1, 1),\n#     #Doesnt include fc layers\n# ]\n\n\n# class CNNBlock(nn.Module):\n#     def __init__(self, in_channels, out_channels, **kwargs):\n#         super(CNNBlock, self).__init__()\n#         self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n#         self.batchnorm = nn.BatchNorm2d(out_channels)\n#         self.leakyrelu = nn.LeakyReLU(0.1)\n        \n#     def forward(self, x):\n#         return self.leakyrelu(self.batchnorm(self.conv(x)))\n    \n# class YoloV1(nn.Module):\n#     def __init__(self, in_channels=3, **kwargs):\n#         super(YoloV1, self).__init__()\n#         self.architecture = architecture_config\n#         self.in_channels = in_channels\n#         self.darknet = self._create_conv_layers(self.architecture)\n#         self.fcs = self._create_fcs(**kwargs)\n        \n#     def forward(self, x):\n#         x = self.darknet(x)\n#         return self.fcs(torch.flatten(x, start_dim=1))\n    \n#     def _create_conv_layers(self, architecture):\n#         layers = []\n#         in_channels = self.in_channels\n        \n#         for x in architecture:\n#             if type(x) == tuple:\n#                 layers += [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n#                 in_channels = x[1]\n#             elif type(x) == str:\n#                 layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n#             elif type(x) == list:\n#                 conv1 = x[0] #Tuple\n#                 conv2 = x[1] #Tuple\n#                 repeats = x[2] #Int\n                \n#                 for _ in range(repeats):\n#                     layers += [CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n#                     layers += [CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n#                     in_channels = conv2[1]\n                    \n#         return nn.Sequential(*layers)\n    \n#     def _create_fcs(self, split_size, num_boxes, num_classes):\n#         S, B, C = split_size, num_boxes, num_classes\n#         return nn.Sequential(nn.Flatten(), nn.Linear(1024 * S * S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S * S * (C + B * 5)))#Original paper uses nn.Linear(1024 * S * S, 4096) not 496. Also the last layer will be reshaped to (S, S, 13) where C+B*5 = 13","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.731812Z","iopub.execute_input":"2024-11-16T16:23:42.732138Z","iopub.status.idle":"2024-11-16T16:23:42.740151Z","shell.execute_reply.started":"2024-11-16T16:23:42.732106Z","shell.execute_reply":"2024-11-16T16:23:42.739323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Этот код на Python вычисляет пересечение над объединением (Intersection over Union, IoU) для пар предсказанных и истинных ограничивающих рамок. IoU — это метрика, которая часто используется в задачах компьютерного зрения для оценки качества предсказанных ограничивающих рамок. Вот как работает этот код:\r\n\r\n1. Параметры функции:\r\n\r\n   • boxes_preds — тензор с предсказанными ограничивающими рамками размером (BATCH_SIZE, 4).\r\n\r\n   • boxes_labels — тензор с истинными ограничивающими рамками размером (BATCH_SIZE, 4).\r\n\r\n   • box_format — формат представления рамок: 'midpoint' (центр и размеры) или 'corners' (координаты углов).\r\n\r\n2. Преобразование формата рамок:\r\n\r\n   • Если формат 'midpoint', рамки преобразуются из формата (x, y, w, h) в (x1, y1, x2, y2).\r\n\r\n   • Если формат 'corners', рамки уже в нужном формате (x1, y1, x2, y2).\r\n\r\n3. Вычисление координат пересечения:\r\n\r\n   • Используется функция torch.max для вычисления верхних левых углов пересечения.\r\n\r\n   • Используется функция torch.min для вычисления нижних правых углов пересечения.\r\n\r\n4. Вычисление площади пересечения:\r\n\r\n   • Площадь пересечения вычисляется как произведение высоты и ширины пересекающейся области. Для случая, когда рамки не пересекаются, используется .clamp(0), чтобы избежать отрицательных значений.\r\n\r\n5. Вычисление площадей отдельных рамок:\r\n\r\n   • Площади предсказанных и истинных рамок вычисляются на основе их координат.\r\n\r\n6. Вычисление IoU:\r\n\r\n   • IoU вычисляется как отношение площади пересечения к объединенной площади двух рамок (площадь первой рамки + площадь второй рамки - площадь пересечения). Небольшая константа 1e-6 добавляется к знаменателю для предотвращения деления на ноль.\r\n\r\nЭтот код полезен для оценки качества алгоритмов обнаружения объектов, так как позволяет количественно оценить, насколько хорошо предсказанные ограничивающие рамки соответствуют истинным объектам на изображении.","metadata":{}},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n    if box_format == 'midpoint':\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n        \n    if box_format == 'corners':\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4] \n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n    \n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n    \n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    \n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n    \n    return intersection / (box1_area + box2_area - intersection + 1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.741322Z","iopub.execute_input":"2024-11-16T16:23:42.741956Z","iopub.status.idle":"2024-11-16T16:23:42.755064Z","shell.execute_reply.started":"2024-11-16T16:23:42.741914Z","shell.execute_reply":"2024-11-16T16:23:42.754212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    \"\"\"\n    Выполняет подавление немаксимумов (Non Max Suppression) для заданных ограничивающих рамок.\n    Параметры:\n        bboxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        threshold (float): порог для удаления предсказанных рамок (независимо от IoU)\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n    Возвращает:\n        list: ограничивающие рамки после выполнения NMS с заданным порогом IoU\n    \"\"\"\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.756031Z","iopub.execute_input":"2024-11-16T16:23:42.756314Z","iopub.status.idle":"2024-11-16T16:23:42.77105Z","shell.execute_reply.started":"2024-11-16T16:23:42.756269Z","shell.execute_reply":"2024-11-16T16:23:42.770131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n):\n    \"\"\"\n    Вычисляет среднюю точность (mean average precision).\n    Параметры:\n        pred_boxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): аналогично pred_boxes, но для всех правильных\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n        num_classes (int): количество классов\n    Возвращает:\n        float: значение mAP для всех классов при заданном пороге IoU\n    \"\"\"\n\n    # список для хранения всех AP для соответствующих классов\n    average_precisions = []\n\n    # используется для численной стабильности позже\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Проходим через все предсказания и цели,\n        # и добавляем только те, которые принадлежат\n        # текущему классу c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # находим количество рамок для каждого обучающего примера\n        # Counter здесь находит, сколько истинных рамок мы получаем\n        # для каждого обучающего примера. Например, если у img 0 их 3,\n        # а у img 1 их 5, то мы получим словарь с:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # Затем мы проходим через каждый ключ и значение в этом словаре\n        # и преобразуем в следующее (относительно того же примера):\n        # amount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # сортируем по вероятностям рамок, которые находятся в индексе 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # Если ничего не существует для этого класса, то можно безопасно пропустить\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Берем только те истинные рамки, у которых такой же индекс\n            # обучения, как у предсказания\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # засчитываем истинное предсказание только один раз\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # истинно положительное и добавляем эту рамку в просмотренные\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # если IOU ниже порога, то предсказание является ложноположительным\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz для численного интегрирования\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / len(average_precisions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.772232Z","iopub.execute_input":"2024-11-16T16:23:42.772543Z","iopub.status.idle":"2024-11-16T16:23:42.789319Z","shell.execute_reply.started":"2024-11-16T16:23:42.772513Z","shell.execute_reply":"2024-11-16T16:23:42.788473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\n\ndef convert_cellboxes(predictions, S=7, C=3):\n    \"\"\"\n    Преобразует ограничивающие рамки, полученные от Yolo с\n    размером разбиения изображения S, в соотношения для всего изображения,\n    а не относительно ячеек. Пытались сделать это векторизованно,\n    но это привело к довольно сложному для чтения коду...\n    Использовать как черный ящик? Или реализовать более интуитивно понятный метод,\n    используя 2 цикла for, которые перебирают range(S) и преобразуют их\n    по одному, что приведет к более медленной, но более читаемой реализации.\n    \"\"\"\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n    bboxes1 = predictions[..., C + 1:C + 5]\n    bboxes2 = predictions[..., C + 6:C + 10]\n    scores = torch.cat(\n        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n    \ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.790879Z","iopub.execute_input":"2024-11-16T16:23:42.791222Z","iopub.status.idle":"2024-11-16T16:23:42.811779Z","shell.execute_reply.started":"2024-11-16T16:23:42.791182Z","shell.execute_reply":"2024-11-16T16:23:42.811069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files_dir = '/kaggle/input/fruit-images-for-object-detection/test_zip/test'\ntest_dir = '/kaggle/input/fruit-images-for-object-detection/test_zip/test'\n\nimages = [image for image in sorted(os.listdir(files_dir))\n                        if image[-4:]=='.jpg']\nannots = []\nfor image in images:\n    annot = image[:-4] + '.xml'\n    annots.append(annot)\n    \nimages = pd.Series(images, name='images')\nannots = pd.Series(annots, name='annots')\ndf = pd.concat([images, annots], axis=1)\ndf = pd.DataFrame(df)\n\ntest_images = [image for image in sorted(os.listdir(test_dir))\n                        if image[-4:]=='.jpg']\n\ntest_annots = []\nfor image in test_images:\n    annot = image[:-4] + '.xml'\n    test_annots.append(annot)\n\ntest_images = pd.Series(test_images, name='test_images')\ntest_annots = pd.Series(test_annots, name='test_annots')\ntest_df = pd.concat([test_images, test_annots], axis=1)\ntest_df = pd.DataFrame(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.812854Z","iopub.execute_input":"2024-11-16T16:23:42.813586Z","iopub.status.idle":"2024-11-16T16:23:42.847749Z","shell.execute_reply.started":"2024-11-16T16:23:42.813544Z","shell.execute_reply":"2024-11-16T16:23:42.847041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FruitImagesDataset(torch.utils.data.Dataset):\n    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=3, transform=None):\n        self.annotations = df\n        self.files_dir = files_dir\n        self.transform = transform\n        self.S = S\n        self.B = B\n        self.C = C\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n        boxes = []\n        tree = ET.parse(label_path)\n        root = tree.getroot()\n        \n        class_dictionary = {'apple':0, 'banana':1, 'orange':2}\n    \n        if(int(root.find('size').find('height').text) == 0):\n            filename = root.find('filename').text\n            img = Image.open(self.files_dir + '/' + filename)\n            img_width, img_height = img.size\n            \n            for member in root.findall('object'):\n            \n                klass = member.find('name').text\n                klass = class_dictionary[klass]\n            \n                # bounding box\n                xmin = int(member.find('bndbox').find('xmin').text)\n                xmax = int(member.find('bndbox').find('xmax').text)\n            \n                ymin = int(member.find('bndbox').find('ymin').text)\n                ymax = int(member.find('bndbox').find('ymax').text)\n                \n                centerx = ((xmax + xmin) / 2) / img_width\n                centery = ((ymax + ymin) / 2) / img_height\n                boxwidth = (xmax - xmin) / img_width\n                boxheight = (ymax - ymin) / img_height\n            \n                \n                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n            \n        elif(int(root.find('size').find('height').text) != 0):\n            \n            for member in root.findall('object'):\n            \n                klass = member.find('name').text\n                klass = class_dictionary[klass]\n            \n                                # bounding box\n                xmin = int(member.find('bndbox').find('xmin').text)\n                xmax = int(member.find('bndbox').find('xmax').text)\n                img_width = int(root.find('size').find('width').text)\n            \n                ymin = int(member.find('bndbox').find('ymin').text)\n                ymax = int(member.find('bndbox').find('ymax').text)\n                img_height = int(root.find('size').find('height').text)\n                \n                centerx = ((xmax + xmin) / 2) / img_width\n                centery = ((ymax + ymin) / 2) / img_height\n                boxwidth = (xmax - xmin) / img_width\n                boxheight = (ymax - ymin) / img_height\n            \n            \n                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n\n                \n        boxes = torch.tensor(boxes)\n        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        image = image.convert(\"RGB\")\n\n        if self.transform:\n            # image = self.transform(image)\n            image, boxes = self.transform(image, boxes)\n\n        # Convert To Cells\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box.tolist()\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n\n            \"\"\"\n            Вычисление ширины и высоты ячейки ограничивающей рамки\n            относительно ячейки выполняется следующим образом, на примере\n            ширины:\n            \n            width_pixels = (width*self.image_width)\n            cell_pixels = (self.image_width)\n            \n            Затем, чтобы найти ширину относительно ячейки, достаточно:\n            width_pixels/cell_pixels, что при упрощении приводит к\n            формулам ниже.\n            \"\"\"\n            width_cell, height_cell = (\n                width * self.S,\n                height * self.S,\n            )\n\n            # If no object already found for specific cell i,j\n            # Note: This means we restrict to ONE object\n            # per cell!\n#             print(i, j)\n            if label_matrix[i, j, self.C] == 0:\n                # Set that there exists an object\n                label_matrix[i, j, self.C] = 1\n\n                # Box coordinates\n                box_coordinates = torch.tensor(\n                    [x_cell, y_cell, width_cell, height_cell]\n                )\n\n                label_matrix[i, j, 4:8] = box_coordinates\n\n                # Set one hot encoding for class_label\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.850678Z","iopub.execute_input":"2024-11-16T16:23:42.850961Z","iopub.status.idle":"2024-11-16T16:23:42.873423Z","shell.execute_reply.started":"2024-11-16T16:23:42.850931Z","shell.execute_reply":"2024-11-16T16:23:42.872481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    \"\"\"\n    Calculate the loss for yolo (v1) model\n    \"\"\"\n\n    def __init__(self, S=7, B=2, C=3):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss(reduction=\"sum\")\n\n        \"\"\"\n        S is split size of image (in paper 7),\n        B is number of boxes (in paper 2),\n        C is number of classes (in paper 20, in dataset 3),\n        \"\"\"\n        self.S = S\n        self.B = B\n        self.C = C\n\n        # These are from Yolo paper, signifying how much we should\n        # pay loss for no object (noobj) and the box coordinates (coord)\n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n\n    def forward(self, predictions, target):\n        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes with target bbox\n        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        # Note that bestbox will be indices of 0, 1 for which bbox was best\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n\n        # ======================== #\n        #   FOR BOX COORDINATES    #\n        # ======================== #\n\n        # Set boxes with no object in them to 0. We only take out one of the two \n        # predictions, which is the one with highest Iou calculated previously.\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., self.C + 6:self.C + 10]\n                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n            )\n        )\n\n        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # ==================== #\n        #   FOR OBJECT LOSS    #\n        # ==================== #\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n        )\n\n        # ======================= #\n        # ЛОСС ОТСУТСТВИЯ КЛАССА  #\n        # ======================= #\n\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n        )\n\n        # ================== #\n        #   КЛАССОВЫЙ ЛОСС   #\n        # ================== #\n\n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.874605Z","iopub.execute_input":"2024-11-16T16:23:42.874963Z","iopub.status.idle":"2024-11-16T16:23:42.894361Z","shell.execute_reply.started":"2024-11-16T16:23:42.874933Z","shell.execute_reply":"2024-11-16T16:23:42.893586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE = 2e-5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\nWEIGHT_DECAY = 0.1\nEPOCHS = 20\nNUM_WORKERS = 2\nPIN_MEMORY = True\nLOAD_MODEL = False\nLOAD_MODEL_FILE = \"model.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.895495Z","iopub.execute_input":"2024-11-16T16:23:42.895847Z","iopub.status.idle":"2024-11-16T16:23:42.907772Z","shell.execute_reply.started":"2024-11-16T16:23:42.895817Z","shell.execute_reply":"2024-11-16T16:23:42.906895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n    \n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss = loss.item())\n        \n    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.909226Z","iopub.execute_input":"2024-11-16T16:23:42.90963Z","iopub.status.idle":"2024-11-16T16:23:42.917522Z","shell.execute_reply.started":"2024-11-16T16:23:42.909588Z","shell.execute_reply":"2024-11-16T16:23:42.916707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.918688Z","iopub.execute_input":"2024-11-16T16:23:42.918952Z","iopub.status.idle":"2024-11-16T16:23:42.928533Z","shell.execute_reply.started":"2024-11-16T16:23:42.918924Z","shell.execute_reply":"2024-11-16T16:23:42.927588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    train_dataset = FruitImagesDataset(\n        transform=transform,\n        files_dir=files_dir\n    )\n\n    test_dataset = FruitImagesDataset(\n        transform=transform, \n        files_dir=test_dir\n    )\n\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    for epoch in range(EPOCHS):\n        train_fn(train_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            train_loader, model, iou_threshold=0.5, threshold=0.4\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n        )\n        print(f\"Train mAP: {mean_avg_prec}\")\n        \n        scheduler.step(mean_avg_prec)\n    \n    checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.929707Z","iopub.execute_input":"2024-11-16T16:23:42.930455Z","iopub.status.idle":"2024-11-16T16:23:42.940199Z","shell.execute_reply.started":"2024-11-16T16:23:42.930414Z","shell.execute_reply":"2024-11-16T16:23:42.939355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:23:42.941258Z","iopub.execute_input":"2024-11-16T16:23:42.9416Z","iopub.status.idle":"2024-11-16T16:24:50.337399Z","shell.execute_reply.started":"2024-11-16T16:23:42.94156Z","shell.execute_reply":"2024-11-16T16:24:50.336345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LOAD_MODEL = True\nEPOCHS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:24:50.339416Z","iopub.execute_input":"2024-11-16T16:24:50.340229Z","iopub.status.idle":"2024-11-16T16:24:50.345008Z","shell.execute_reply.started":"2024-11-16T16:24:50.340169Z","shell.execute_reply":"2024-11-16T16:24:50.343949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predictions():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    test_dataset = FruitImagesDataset(\n        transform=transform, \n        df=test_df,\n        files_dir=test_dir\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n        \n    for epoch in range(EPOCHS):\n        model.eval()\n        train_fn(test_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            test_loader, model, iou_threshold=0.5, threshold=0.4\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n        )\n        print(f\"Test mAP: {mean_avg_prec}\")\n\n\npredictions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:24:50.34623Z","iopub.execute_input":"2024-11-16T16:24:50.346544Z","iopub.status.idle":"2024-11-16T16:24:53.929195Z","shell.execute_reply.started":"2024-11-16T16:24:50.346513Z","shell.execute_reply":"2024-11-16T16:24:53.928282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n\noptimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n)\n\nloss_fn = YoloLoss()\n\nload_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\ntest_dataset = FruitImagesDataset(\n        transform=transform, \n        df=test_df,\n        files_dir=test_dir\n    )\n\ntest_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:24:53.930536Z","iopub.execute_input":"2024-11-16T16:24:53.930948Z","iopub.status.idle":"2024-11-16T16:24:54.380869Z","shell.execute_reply.started":"2024-11-16T16:24:53.930905Z","shell.execute_reply":"2024-11-16T16:24:54.379252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bboxes_images(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_images = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        all_images.append(x)\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes, torch.cat(all_images, dim=0)\n\n\npred_boxes, target_boxes, images = get_bboxes_images(\n            test_loader, model, iou_threshold=0.5, threshold=0.5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:25:38.177726Z","iopub.execute_input":"2024-11-16T16:25:38.178119Z","iopub.status.idle":"2024-11-16T16:25:39.678849Z","shell.execute_reply.started":"2024-11-16T16:25:38.178082Z","shell.execute_reply":"2024-11-16T16:25:39.677999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [{'num_imag': pred_box[0], 'class': pred_box[1], 'conf': pred_box[2], 'box': pred_box[3:]} for pred_box in pred_boxes]\npreds[:10:1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:25:39.680837Z","iopub.execute_input":"2024-11-16T16:25:39.68123Z","iopub.status.idle":"2024-11-16T16:25:39.690607Z","shell.execute_reply.started":"2024-11-16T16:25:39.68119Z","shell.execute_reply":"2024-11-16T16:25:39.689695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncolor_classes = {\n    0:'r',\n    1:\"b\",\n    2:\"g\"\n}\n\nfig, ax = plt.subplots(10, 6, figsize = (12, 20))\nfor idx, image in enumerate(images):\n    ax[idx//6, idx%6].imshow(image.permute((1, 2, 0)).detach().numpy())\nfor box in preds:\n    # Create a Rectangle patch\n    box_rect = box.get('box')\n    center_x = box_rect[0] - box_rect[2]/2\n    center_y = box_rect[1] - box_rect[3]/2\n    width = box_rect[2]\n    height = box_rect[3]\n    rect = patches.Rectangle((center_x * 448, center_y * 448), \n                             width * 448, height * 448, \n                             linewidth=1, \n                             edgecolor=color_classes.get(int(box.get(\"class\"))), \n                             facecolor='none')\n    idx_box = box.get(\"num_imag\")\n    # Add the patch to the Axes\n    ax[idx_box//6, idx_box%6].add_patch(rect)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T16:25:39.777618Z","iopub.execute_input":"2024-11-16T16:25:39.777966Z","iopub.status.idle":"2024-11-16T16:25:49.23124Z","shell.execute_reply.started":"2024-11-16T16:25:39.777931Z","shell.execute_reply":"2024-11-16T16:25:49.230083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}