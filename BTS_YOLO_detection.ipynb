{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10287488,"sourceType":"datasetVersion","datasetId":6366491}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport os\nimport PIL\nimport skimage\nfrom skimage import io\nimport numpy as np\nfrom PIL import Image\nimport shutil \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms.functional as FT\nfrom torchvision.transforms.v2 import functional as F\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchinfo import summary\nimport copy\nimport datetime\nimport random\nimport traceback\nfrom IPython.display import display, clear_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nseed = 42\nimport cv2\nimport xml.etree.ElementTree as ET\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:14:57.558484Z","iopub.execute_input":"2024-12-26T16:14:57.558800Z","iopub.status.idle":"2024-12-26T16:15:03.345988Z","shell.execute_reply.started":"2024-12-26T16:14:57.558769Z","shell.execute_reply":"2024-12-26T16:15:03.345067Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torchvision import models\nresnet50_model = models.resnet50(pretrained=True)\nresnet50_model.avgpool = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n#resnet18_model.avgpool = nn.Sequential()\nresnet50_model.fc = nn.Sequential()\nfor param in resnet50_model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:15:03.347332Z","iopub.execute_input":"2024-12-26T16:15:03.347703Z","iopub.status.idle":"2024-12-26T16:15:04.487925Z","shell.execute_reply.started":"2024-12-26T16:15:03.347676Z","shell.execute_reply":"2024-12-26T16:15:04.487179Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 206MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"summary(resnet50_model, input_size=(16, 3, 448, 448))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:15:04.489589Z","iopub.execute_input":"2024-12-26T16:15:04.489923Z","iopub.status.idle":"2024-12-26T16:15:05.433040Z","shell.execute_reply.started":"2024-12-26T16:15:04.489885Z","shell.execute_reply":"2024-12-26T16:15:05.432191Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet                                   [16, 100352]              --\n├─Conv2d: 1-1                            [16, 64, 224, 224]        9,408\n├─BatchNorm2d: 1-2                       [16, 64, 224, 224]        128\n├─ReLU: 1-3                              [16, 64, 224, 224]        --\n├─MaxPool2d: 1-4                         [16, 64, 112, 112]        --\n├─Sequential: 1-5                        [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-1                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-1                  [16, 64, 112, 112]        4,096\n│    │    └─BatchNorm2d: 3-2             [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-3                    [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-4                  [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-5             [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-6                    [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-7                  [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-8             [16, 256, 112, 112]       512\n│    │    └─Sequential: 3-9              [16, 256, 112, 112]       16,896\n│    │    └─ReLU: 3-10                   [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-2                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-11                 [16, 64, 112, 112]        16,384\n│    │    └─BatchNorm2d: 3-12            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-13                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-14                 [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-15            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-16                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-17                 [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-18            [16, 256, 112, 112]       512\n│    │    └─ReLU: 3-19                   [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-3                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-20                 [16, 64, 112, 112]        16,384\n│    │    └─BatchNorm2d: 3-21            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-22                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-23                 [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-24            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-25                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-26                 [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-27            [16, 256, 112, 112]       512\n│    │    └─ReLU: 3-28                   [16, 256, 112, 112]       --\n├─Sequential: 1-6                        [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-4                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-29                 [16, 128, 112, 112]       32,768\n│    │    └─BatchNorm2d: 3-30            [16, 128, 112, 112]       256\n│    │    └─ReLU: 3-31                   [16, 128, 112, 112]       --\n│    │    └─Conv2d: 3-32                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-33            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-34                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-35                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-36            [16, 512, 56, 56]         1,024\n│    │    └─Sequential: 3-37             [16, 512, 56, 56]         132,096\n│    │    └─ReLU: 3-38                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-5                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-39                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-40            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-41                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-42                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-43            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-44                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-45                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-46            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-47                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-6                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-48                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-49            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-50                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-51                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-52            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-53                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-54                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-55            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-56                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-7                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-57                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-58            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-59                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-60                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-61            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-62                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-63                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-64            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-65                   [16, 512, 56, 56]         --\n├─Sequential: 1-7                        [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-8                   [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-66                 [16, 256, 56, 56]         131,072\n│    │    └─BatchNorm2d: 3-67            [16, 256, 56, 56]         512\n│    │    └─ReLU: 3-68                   [16, 256, 56, 56]         --\n│    │    └─Conv2d: 3-69                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-70            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-71                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-72                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-73            [16, 1024, 28, 28]        2,048\n│    │    └─Sequential: 3-74             [16, 1024, 28, 28]        526,336\n│    │    └─ReLU: 3-75                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-9                   [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-76                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-77            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-78                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-79                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-80            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-81                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-82                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-83            [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-84                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-10                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-85                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-86            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-87                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-88                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-89            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-90                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-91                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-92            [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-93                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-11                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-94                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-95            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-96                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-97                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-98            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-99                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-100                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-101           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-102                  [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-12                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-103                [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-104           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-105                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-106                [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-107           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-108                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-109                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-110           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-111                  [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-13                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-112                [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-113           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-114                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-115                [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-116           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-117                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-118                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-119           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-120                  [16, 1024, 28, 28]        --\n├─Sequential: 1-8                        [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-14                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-121                [16, 512, 28, 28]         524,288\n│    │    └─BatchNorm2d: 3-122           [16, 512, 28, 28]         1,024\n│    │    └─ReLU: 3-123                  [16, 512, 28, 28]         --\n│    │    └─Conv2d: 3-124                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-125           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-126                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-127                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-128           [16, 2048, 14, 14]        4,096\n│    │    └─Sequential: 3-129            [16, 2048, 14, 14]        2,101,248\n│    │    └─ReLU: 3-130                  [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-15                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-131                [16, 512, 14, 14]         1,048,576\n│    │    └─BatchNorm2d: 3-132           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-133                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-134                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-135           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-136                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-137                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-138           [16, 2048, 14, 14]        4,096\n│    │    └─ReLU: 3-139                  [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-16                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-140                [16, 512, 14, 14]         1,048,576\n│    │    └─BatchNorm2d: 3-141           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-142                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-143                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-144           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-145                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-146                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-147           [16, 2048, 14, 14]        4,096\n│    │    └─ReLU: 3-148                  [16, 2048, 14, 14]        --\n├─AvgPool2d: 1-9                         [16, 2048, 7, 7]          --\n├─Sequential: 1-10                       [16, 100352]              --\n==========================================================================================\nTotal params: 23,508,032\nTrainable params: 23,508,032\nNon-trainable params: 0\nTotal mult-adds (G): 261.58\n==========================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 11380.72\nParams size (MB): 94.03\nEstimated Total Size (MB): 11513.29\n=========================================================================================="},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:15:05.434722Z","iopub.execute_input":"2024-12-26T16:15:05.435452Z","iopub.status.idle":"2024-12-26T16:15:05.439101Z","shell.execute_reply.started":"2024-12-26T16:15:05.435411Z","shell.execute_reply":"2024-12-26T16:15:05.438234Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"temp1 = ['/'+image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))\n         and os.path.getsize(files_dir+'/'+image[:-4]+'.txt') != 0]\ntemp2 = ['/'+annot for annot in sorted(os.listdir(files_dir))\n                        if (annot[-4:]=='.txt') and os.path.getsize(files_dir+'/'+annot) != 0]\n\nimages = pd.Series(temp1, name='images')\nimage_id = pd.Series(list(range(len(temp1))), name='id')\ntrain_img_df = pd.DataFrame(pd.concat([images, image_id], axis=1))\nimages = []\nimage_id = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for j in range(len(file.readlines())):\n            images.append(temp1[i])\n            image_id.append(i)\n        file.close()\nbboxes = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for line in file.readlines():\n            bboxes.append(list(map(float, line.split())))\n        file.close()\nimages = pd.Series(images, name='images')\nbboxes = pd.Series(bboxes, name='bboxes')\nimage_id = pd.Series(image_id, name='image_id')\nind = pd.Series(list(range(len(images))), name='id')\ndf = pd.concat([images, ind,image_id,bboxes], axis=1)\ntrain_df = pd.DataFrame(df)\narea = []\nfor i in range(train_df.shape[0]):\n    img_path = files_dir + train_df.iloc[i,0]\n    img = read_image(img_path)\n    area.append(train_df.iloc[i,3][3]*train_df.iloc[i,3][4])\ntrain_df = pd.concat([train_df, pd.Series(area, name='area')],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:15:28.714764Z","iopub.execute_input":"2024-12-26T16:15:28.715480Z","iopub.status.idle":"2024-12-26T16:15:32.260477Z","shell.execute_reply.started":"2024-12-26T16:15:28.715445Z","shell.execute_reply":"2024-12-26T16:15:32.259555Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:15:32.261823Z","iopub.execute_input":"2024-12-26T16:15:32.262099Z","iopub.status.idle":"2024-12-26T16:15:32.284102Z","shell.execute_reply.started":"2024-12-26T16:15:32.262072Z","shell.execute_reply":"2024-12-26T16:15:32.283330Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"            images   id  image_id  \\\n0      /jhope0.png    0         0   \n1      /jhope1.png    1         1   \n2     /jhope10.png    2         2   \n3    /jhope100.png    3         3   \n4    /jhope101.png    4         4   \n..             ...  ...       ...   \n804      /v291.png  804       803   \n805      /v292.png  805       804   \n806      /v293.png  806       805   \n807      /v294.png  807       806   \n808      /v295.png  808       807   \n\n                                            bboxes      area  \n0    [0.0, 0.497253, 0.289855, 0.467033, 0.543478]  0.253822  \n1    [0.0, 0.477778, 0.391111, 0.848889, 0.782222]  0.664020  \n2             [0.0, 0.4, 0.393333, 0.408889, 0.44]  0.179911  \n3    [0.0, 0.555184, 0.446429, 0.622074, 0.892857]  0.555423  \n4     [0.0, 0.52901, 0.319767, 0.361775, 0.639535]  0.231368  \n..                                             ...       ...  \n804  [5.0, 0.553763, 0.279006, 0.218638, 0.292818]  0.064021  \n805  [5.0, 0.426667, 0.255556, 0.382222, 0.342222]  0.130805  \n806  [5.0, 0.591667, 0.494048, 0.456667, 0.714286]  0.326191  \n807   [5.0, 0.528796, 0.301136, 0.743455, 0.42803]  0.318221  \n808  [5.0, 0.507895, 0.390566, 0.952632, 0.615094]  0.585958  \n\n[809 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>images</th>\n      <th>id</th>\n      <th>image_id</th>\n      <th>bboxes</th>\n      <th>area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/jhope0.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0.0, 0.497253, 0.289855, 0.467033, 0.543478]</td>\n      <td>0.253822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/jhope1.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0.0, 0.477778, 0.391111, 0.848889, 0.782222]</td>\n      <td>0.664020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/jhope10.png</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0.0, 0.4, 0.393333, 0.408889, 0.44]</td>\n      <td>0.179911</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/jhope100.png</td>\n      <td>3</td>\n      <td>3</td>\n      <td>[0.0, 0.555184, 0.446429, 0.622074, 0.892857]</td>\n      <td>0.555423</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/jhope101.png</td>\n      <td>4</td>\n      <td>4</td>\n      <td>[0.0, 0.52901, 0.319767, 0.361775, 0.639535]</td>\n      <td>0.231368</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>804</th>\n      <td>/v291.png</td>\n      <td>804</td>\n      <td>803</td>\n      <td>[5.0, 0.553763, 0.279006, 0.218638, 0.292818]</td>\n      <td>0.064021</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>/v292.png</td>\n      <td>805</td>\n      <td>804</td>\n      <td>[5.0, 0.426667, 0.255556, 0.382222, 0.342222]</td>\n      <td>0.130805</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>/v293.png</td>\n      <td>806</td>\n      <td>805</td>\n      <td>[5.0, 0.591667, 0.494048, 0.456667, 0.714286]</td>\n      <td>0.326191</td>\n    </tr>\n    <tr>\n      <th>807</th>\n      <td>/v294.png</td>\n      <td>807</td>\n      <td>806</td>\n      <td>[5.0, 0.528796, 0.301136, 0.743455, 0.42803]</td>\n      <td>0.318221</td>\n    </tr>\n    <tr>\n      <th>808</th>\n      <td>/v295.png</td>\n      <td>808</td>\n      <td>807</td>\n      <td>[5.0, 0.507895, 0.390566, 0.952632, 0.615094]</td>\n      <td>0.585958</td>\n    </tr>\n  </tbody>\n</table>\n<p>809 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\nclass_names = dict((i, names[i]) for i in range(7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:17:03.774812Z","iopub.execute_input":"2024-12-26T16:17:03.775539Z","iopub.status.idle":"2024-12-26T16:17:03.780002Z","shell.execute_reply.started":"2024-12-26T16:17:03.775504Z","shell.execute_reply":"2024-12-26T16:17:03.779143Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:31:44.783126Z","iopub.execute_input":"2024-12-26T16:31:44.783502Z","iopub.status.idle":"2024-12-26T16:31:44.787708Z","shell.execute_reply.started":"2024-12-26T16:31:44.783472Z","shell.execute_reply":"2024-12-26T16:31:44.786718Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train, test = train_test_split(train_df, test_size = 0.2, shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:32:05.438383Z","iopub.execute_input":"2024-12-26T16:32:05.438720Z","iopub.status.idle":"2024-12-26T16:32:05.446116Z","shell.execute_reply.started":"2024-12-26T16:32:05.438691Z","shell.execute_reply":"2024-12-26T16:32:05.445255Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"np.unique(test.iloc[:,0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:32:29.376678Z","iopub.execute_input":"2024-12-26T16:32:29.377355Z","iopub.status.idle":"2024-12-26T16:32:29.386102Z","shell.execute_reply.started":"2024-12-26T16:32:29.377307Z","shell.execute_reply":"2024-12-26T16:32:29.385070Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array(['/jhope10.png', '/jhope107.png', '/jhope113.png', '/jhope176.png',\n       '/jhope184.png', '/jhope187.png', '/jhope188.png', '/jhope19.png',\n       '/jhope198.png', '/jhope211.png', '/jhope22.png', '/jhope279.png',\n       '/jhope280.png', '/jhope281.png', '/jhope283.png', '/jhope290.png',\n       '/jhope297.png', '/jhope298.png', '/jhope3.png', '/jhope32.png',\n       '/jhope37.png', '/jhope4.png', '/jhope5.png', '/jhope50.png',\n       '/jhope54.png', '/jhope62.png', '/jhope63.png', '/jhope75.png',\n       '/jhope78.png', '/jimin0.png', '/jimin10.png', '/jimin12.png',\n       '/jimin177.png', '/jimin183.png', '/jimin208.png', '/jimin214.png',\n       '/jimin215.png', '/jimin23.png', '/jimin27.png', '/jimin270.png',\n       '/jimin271.png', '/jimin273.png', '/jimin274.png', '/jimin275.png',\n       '/jimin277.png', '/jimin297.png', '/jimin304.png', '/jimin310.png',\n       '/jimin316.png', '/jimin33.png', '/jimin36.png', '/jimin378.png',\n       '/jimin382.png', '/jimin389.png', '/jimin39.png', '/jimin54.png',\n       '/jimin63.png', '/jimin73.png', '/jimin90.png', '/jimin94.png',\n       '/jin10.png', '/jin103.png', '/jin105.png', '/jin114.png',\n       '/jin15.png', '/jin16.png', '/jin17.png', '/jin18.png',\n       '/jin181.png', '/jin188.png', '/jin202.png', '/jin204.png',\n       '/jin205.png', '/jin209.png', '/jin211.png', '/jin214.png',\n       '/jin217.png', '/jin25.png', '/jin283.png', '/jin285.png',\n       '/jin288.png', '/jin303.png', '/jin305.png', '/jin307.png',\n       '/jin31.png', '/jin318.png', '/jin32.png', '/jin36.png',\n       '/jin379.png', '/jin40.png', '/jin41.png', '/jin42.png',\n       '/jin45.png', '/jin48.png', '/jin56.png', '/jin78.png',\n       '/jin83.png', '/jungkook107.png', '/jungkook11.png',\n       '/jungkook176.png', '/jungkook178.png', '/jungkook181.png',\n       '/jungkook19.png', '/jungkook191.png', '/jungkook206.png',\n       '/jungkook212.png', '/jungkook213.png', '/jungkook217.png',\n       '/jungkook27.png', '/jungkook28.png', '/jungkook282.png',\n       '/jungkook288.png', '/rm10.png', '/rm100.png', '/rm103.png',\n       '/rm112.png', '/rm114.png', '/rm193.png', '/rm197.png',\n       '/rm211.png', '/rm216.png', '/rm22.png', '/rm27.png', '/rm277.png',\n       '/rm278.png', '/rm29.png', '/rm301.png', '/suga112.png',\n       '/suga119.png', '/suga14.png', '/suga181.png', '/suga20.png',\n       '/suga211.png', '/suga25.png', '/suga26.png', '/suga27.png',\n       '/suga276.png', '/suga277.png', '/suga309.png', '/suga316.png',\n       '/suga371.png', '/suga378.png', '/suga381.png', '/v1.png',\n       '/v108.png', '/v111.png', '/v115.png', '/v116.png', '/v13.png',\n       '/v184.png', '/v186.png', '/v189.png', '/v190.png', '/v191.png',\n       '/v195.png', '/v202.png', '/v205.png', '/v271.png', '/v279.png',\n       '/v285.png', '/v291.png', '/v292.png'], dtype=object)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"img_transforms  = transforms.Compose([\n    transforms.Resize((448, 448)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[-0.0932, -0.0971, -0.1260], std=[0.5091, 0.4912, 0.4931])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:33:52.787035Z","iopub.execute_input":"2024-12-26T16:33:52.787390Z","iopub.status.idle":"2024-12-26T16:33:52.792050Z","shell.execute_reply.started":"2024-12-26T16:33:52.787358Z","shell.execute_reply":"2024-12-26T16:33:52.791146Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!mkdir train test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:02.092812Z","iopub.execute_input":"2024-12-26T16:34:02.093115Z","iopub.status.idle":"2024-12-26T16:34:03.120779Z","shell.execute_reply.started":"2024-12-26T16:34:02.093081Z","shell.execute_reply":"2024-12-26T16:34:03.119378Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!mkdir train/jhope train/jin train/jimin train/jungkook train/suga train/rm train/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:03.507550Z","iopub.execute_input":"2024-12-26T16:34:03.507912Z","iopub.status.idle":"2024-12-26T16:34:04.525884Z","shell.execute_reply.started":"2024-12-26T16:34:03.507878Z","shell.execute_reply":"2024-12-26T16:34:04.524638Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"!mkdir test/jhope test/jin test/jimin test/jungkook test/suga test/rm test/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:05.558985Z","iopub.execute_input":"2024-12-26T16:34:05.559454Z","iopub.status.idle":"2024-12-26T16:34:06.565781Z","shell.execute_reply.started":"2024-12-26T16:34:05.559405Z","shell.execute_reply":"2024-12-26T16:34:06.564801Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"for class_name in class_names:\n    for i in range(train.shape[0]):\n        temp = train.iloc[i,0].split('.')[0][1:]\n        flags = list(map(str.isalpha, list(temp)))\n        temp_str = temp[:flags.index(False)]\n        if temp_str == class_names.get(class_name):\n            shutil.copy(files_dir+train.iloc[i,0], \\\n                        '/kaggle/working/train'+'/'+class_names.get(class_name)+train.iloc[i,0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:09.052894Z","iopub.execute_input":"2024-12-26T16:34:09.053780Z","iopub.status.idle":"2024-12-26T16:34:10.376551Z","shell.execute_reply.started":"2024-12-26T16:34:09.053740Z","shell.execute_reply":"2024-12-26T16:34:10.375641Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"!ls train/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:12.281027Z","iopub.execute_input":"2024-12-26T16:34:12.281395Z","iopub.status.idle":"2024-12-26T16:34:13.288524Z","shell.execute_reply.started":"2024-12-26T16:34:12.281362Z","shell.execute_reply":"2024-12-26T16:34:13.287579Z"}},"outputs":[{"name":"stdout","text":"v0.png\t  v11.png   v15.png   v179.png\tv196.png  v208.png  v27.png   v286.png\nv10.png   v110.png  v16.png   v18.png\tv197.png  v209.png  v270.png  v287.png\nv101.png  v112.png  v169.png  v180.png\tv198.png  v21.png   v272.png  v288.png\nv102.png  v113.png  v170.png  v183.png\tv199.png  v212.png  v273.png  v289.png\nv103.png  v114.png  v172.png  v185.png\tv2.png\t  v213.png  v274.png  v293.png\nv104.png  v117.png  v173.png  v187.png\tv20.png   v218.png  v275.png  v294.png\nv105.png  v118.png  v174.png  v19.png\tv200.png  v22.png   v278.png  v295.png\nv106.png  v119.png  v175.png  v192.png\tv201.png  v23.png   v280.png\nv107.png  v12.png   v176.png  v193.png\tv204.png  v25.png   v281.png\nv109.png  v14.png   v177.png  v194.png\tv206.png  v26.png   v283.png\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"for class_name in class_names:\n    for i in range(test.shape[0]):\n        temp = test.iloc[i,0].split('.')[0][1:]\n        flags = list(map(str.isalpha, list(temp)))\n        temp_str = temp[:flags.index(False)]\n        if temp_str == class_names.get(class_name):\n            shutil.copy(files_dir+test.iloc[i,0], \\\n                        '/kaggle/working/test'+'/'+class_names.get(class_name)+test.iloc[i,0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:13.955621Z","iopub.execute_input":"2024-12-26T16:34:13.956586Z","iopub.status.idle":"2024-12-26T16:34:14.293537Z","shell.execute_reply.started":"2024-12-26T16:34:13.956540Z","shell.execute_reply":"2024-12-26T16:34:14.292783Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"!ls test/suga","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:34:16.109685Z","iopub.execute_input":"2024-12-26T16:34:16.110318Z","iopub.status.idle":"2024-12-26T16:34:17.132073Z","shell.execute_reply.started":"2024-12-26T16:34:16.110280Z","shell.execute_reply":"2024-12-26T16:34:17.131214Z"}},"outputs":[{"name":"stdout","text":"suga112.png  suga181.png  suga25.png  suga276.png  suga316.png\tsuga381.png\nsuga119.png  suga20.png   suga26.png  suga277.png  suga371.png\nsuga14.png   suga211.png  suga27.png  suga309.png  suga378.png\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=16,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    step_size=10,\n                    plot=False):\n    \"\"\"\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer, step_size)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n\n    best_val_loss = float('inf')\n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n\n# Dynamic plot\n    if plot:\n        plot_epoch_data = []\n        plot_train_loss = []\n        plot_val_loss = []\n\n        fig, ax = plt.subplots()\n        line_train, = ax.plot([], [], 'r-')\n        line_val, = ax.plot([], [], 'b-')\n        ax.legend(['train', 'val'])\n        ax.set_xlim(0, epoch_n)\n\n        def add_point(epoch_i, train_loss, val_loss):\n            max_loss = max(ax.viewLim.y1 / 1.1, train_loss, val_loss)\n            ax.set_ylim(0, max_loss * 1.1)\n            \n            plot_epoch_data.append(epoch_i)\n            plot_train_loss.append(train_loss)\n            plot_val_loss.append(val_loss)\n            line_train.set_data(plot_epoch_data, plot_train_loss)\n            line_val.set_data(plot_epoch_data, plot_val_loss)\n            clear_output(wait=True)\n            display(fig)\n\n\n    for epoch_i in range(epoch_n):\n        try:\n            epoch_start = datetime.datetime.now()\n            \n\n            print('Эпоха {}'.format(epoch_i))\n\n            model.train()\n            mean_train_loss = 0\n            train_batches_n = 0\n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                mean_train_loss += float(loss)\n                train_batches_n += 1\n\n            mean_train_loss /= train_batches_n\n\n            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n            print('Среднее значение функции потерь на обучении', mean_train_loss)\n\n\n\n            model.eval()\n            mean_val_loss = 0\n            val_batches_n = 0\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n\n                    mean_val_loss += float(loss)\n                    val_batches_n += 1\n\n            mean_val_loss /= val_batches_n\n\n            if plot:\n                add_point(epoch_i, mean_train_loss, mean_val_loss)\n            else:\n                pass\n            \n            print('Среднее значение функции потерь на валидации', mean_val_loss)\n\n            if mean_val_loss < best_val_loss:\n                best_epoch_i = epoch_i\n                best_val_loss = mean_val_loss\n                best_model = copy.deepcopy(model)\n                print('Новая лучшая модель! На эпохе {}'.format(epoch_i))\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(mean_val_loss)\n\n            print()\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n        finally:\n            if plot:\n                plt.close(fig)\n\n    return best_val_loss, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:37:10.230382Z","iopub.execute_input":"2024-12-26T16:37:10.230718Z","iopub.status.idle":"2024-12-26T16:37:10.252481Z","shell.execute_reply.started":"2024-12-26T16:37:10.230689Z","shell.execute_reply":"2024-12-26T16:37:10.251423Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:35:27.950350Z","iopub.execute_input":"2024-12-26T16:35:27.950687Z","iopub.status.idle":"2024-12-26T16:35:27.955243Z","shell.execute_reply.started":"2024-12-26T16:35:27.950655Z","shell.execute_reply":"2024-12-26T16:35:27.954410Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_data = ImageFolder(root='/kaggle/working/train', transform=img_transforms)\nvalidation_data = ImageFolder(root='/kaggle/working/test', transform=img_transforms)\ntrain_dataloader = DataLoader(dataset=train_data, batch_size=10, shuffle=True, num_workers=2)\nvalidation_dataloader = DataLoader(dataset=validation_data, batch_size=4, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:47:12.246270Z","iopub.execute_input":"2024-12-26T16:47:12.247315Z","iopub.status.idle":"2024-12-26T16:47:12.259661Z","shell.execute_reply.started":"2024-12-26T16:47:12.247257Z","shell.execute_reply":"2024-12-26T16:47:12.258801Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"best_loss, best_model = train_eval_loop(model=resnet50_model, \n                train_dataset=train_data, \n                val_dataset=validation_data, \n                criterion=nn.CrossEntropyLoss(),\n                lr=1e-3, \n                epoch_n=50, \n                batch_size=5,\n                device=device, \n                early_stopping_patience=5, \n                l2_reg_alpha=0,\n                max_batches_per_epoch_train=10000,\n                max_batches_per_epoch_val=1000,\n                data_loader_ctor=DataLoader,\n                optimizer_ctor=torch.optim.Adam,\n                lr_scheduler_ctor=torch.optim.lr_scheduler.StepLR,\n                step_size = 4,\n                shuffle_train=True,\n                dataloader_workers_n=2,\n                plot=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:47:24.181943Z","iopub.execute_input":"2024-12-26T16:47:24.182281Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Среднее значение функции потерь на валидации 7.302190115957549\nНовая лучшая модель! На эпохе 0\n\nЭпоха 1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Эпоха: 130 итераций, 39.85 сек\nСреднее значение функции потерь на обучении 6.23033841573275\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"predicted_labels = []\nactual_labels = []\n\nresnet50_model.eval()  # Set the model to evaluation mode\nwith torch.inference_mode():  # Ensure no gradients are computed\n    for images, labels in validation_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = resnet50_model(images)\n        _, predicted = torch.max(outputs, 1)\n        predicted_labels.extend(predicted.cpu().numpy())\n        actual_labels.extend(labels.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\nclassification_report, confusion_matrix\n\n# Вычисление тестовых метрик\naccuracy = accuracy_score(actual_labels, predicted_labels)\nprecision = precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\nrecall = recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\nf1 = f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\n\n# Принт метрик\nprint(f\"Model Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Model Precision: {precision * 100:.2f}%\")\nprint(f\"Model Recall: {recall * 100:.2f}%\")\nprint(f\"Model F1 Score: {f1 * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(actual_labels, predicted_labels)\nclass_names = validation_dataloader.dataset.classes\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class YoloV1(nn.Module):\n    def __init__(self, in_channels=3, out_conv_channels=2048, **kwargs):\n        super(YoloV1, self).__init__()\n        self.in_channels = in_channels\n        self.model_channels = out_conv_channels\n        self.darknet = resnet50_model\n        self.fcs = self._create_fcs(**kwargs)\n        \n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x, start_dim=1))\n    \n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        \"\"\"\n        В изначальной статье используется nn.Linear(1024 * S * S, 4096), но не 496. \n        Также у последнего слоя будет изменена размерность до (S, S, 13) где C+B*5 = 13\n        \"\"\"\n        S, B, C = split_size, num_boxes, num_classes\n        return nn.Sequential(\n            nn.Flatten(), \n            nn.Linear(self.model_channels * S * S, 496), \n            nn.Dropout(0.0), \n            nn.LeakyReLU(0.1), \n            nn.Linear(496, S * S * (C + B * 5))\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:16.550220Z","iopub.execute_input":"2024-12-26T16:01:16.550867Z","iopub.status.idle":"2024-12-26T16:01:16.557628Z","shell.execute_reply.started":"2024-12-26T16:01:16.550834Z","shell.execute_reply":"2024-12-26T16:01:16.556605Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"Этот код на Python вычисляет пересечение над объединением (Intersection over Union, IoU) для пар предсказанных и истинных ограничивающих рамок. IoU — это метрика, которая часто используется в задачах компьютерного зрения для оценки качества предсказанных ограничивающих рамок. Вот как работает этот код:\n\n1. Параметры функции:\n\n   • boxes_preds — тензор с предсказанными ограничивающими рамками размером (BATCH_SIZE, 4).\n\n   • boxes_labels — тензор с истинными ограничивающими рамками размером (BATCH_SIZE, 4).\n\n   • box_format — формат представления рамок: 'midpoint' (центр и размеры) или 'corners' (координаты углов).\n\n2. Преобразование формата рамок:\n\n   • Если формат 'midpoint', рамки преобразуются из формата (x, y, w, h) в (x1, y1, x2, y2).\n\n   • Если формат 'corners', рамки уже в нужном формате (x1, y1, x2, y2).\n\n3. Вычисление координат пересечения:\n\n   • Используется функция torch.max для вычисления верхних левых углов пересечения.\n\n   • Используется функция torch.min для вычисления нижних правых углов пересечения.\n\n4. Вычисление площади пересечения:\n\n   • Площадь пересечения вычисляется как произведение высоты и ширины пересекающейся области. Для случая, когда рамки не пересекаются, используется .clamp(0), чтобы избежать отрицательных значений.\n\n5. Вычисление площадей отдельных рамок:\n\n   • Площади предсказанных и истинных рамок вычисляются на основе их координат.\n\n6. Вычисление IoU:\n\n   • IoU вычисляется как отношение площади пересечения к объединенной площади двух рамок (площадь первой рамки + площадь второй рамки - площадь пересечения). Небольшая константа 1e-6 добавляется к знаменателю для предотвращения деления на ноль.\n\nЭтот код полезен для оценки качества алгоритмов обнаружения объектов, так как позволяет количественно оценить, насколько хорошо предсказанные ограничивающие рамки соответствуют истинным объектам на изображении.\n","metadata":{}},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n    if box_format == 'midpoint':\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n        \n    if box_format == 'corners':\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4] \n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n    \n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n    \n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    \n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n    \n    return intersection / (box1_area + box2_area - intersection + 1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:21.169426Z","iopub.execute_input":"2024-12-26T16:01:21.169787Z","iopub.status.idle":"2024-12-26T16:01:21.179183Z","shell.execute_reply.started":"2024-12-26T16:01:21.169756Z","shell.execute_reply":"2024-12-26T16:01:21.178174Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    \"\"\"\n    Выполняет подавление немаксимумов (Non Max Suppression) для заданных ограничивающих рамок.\n    Параметры:\n        bboxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        threshold (float): порог для удаления предсказанных рамок (независимо от IoU)\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n    Возвращает:\n        list: ограничивающие рамки после выполнения NMS с заданным порогом IoU\n    \"\"\"\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:23.576543Z","iopub.execute_input":"2024-12-26T16:01:23.576915Z","iopub.status.idle":"2024-12-26T16:01:23.583724Z","shell.execute_reply.started":"2024-12-26T16:01:23.576884Z","shell.execute_reply":"2024-12-26T16:01:23.582851Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=7\n):\n    \"\"\"\n    Вычисляет среднюю точность (mean average precision).\n    Параметры:\n        pred_boxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): аналогично pred_boxes, но для всех правильных\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n        num_classes (int): количество классов\n    Возвращает:\n        float: значение mAP для всех классов при заданном пороге IoU\n    \"\"\"\n\n    # список для хранения всех AP для соответствующих классов\n    average_precisions = []\n\n    # используется для численной стабильности позже\n    epsilon = 1e-6\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Проходим через все предсказания и цели,\n        # и добавляем только те, которые принадлежат\n        # текущему классу c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # находим количество рамок для каждого обучающего примера\n        # Counter здесь находит, сколько истинных рамок мы получаем\n        # для каждого обучающего примера. Например, если у img 0 их 3,\n        # а у img 1 их 5, то мы получим словарь с:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # Затем мы проходим через каждый ключ и значение в этом словаре\n        # и преобразуем в следующее (относительно того же примера):\n        # amount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # сортируем по вероятностям рамок, которые находятся в индексе 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # Если ничего не существует для этого класса, то можно безопасно пропустить\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Берем только те истинные рамки, у которых такой же индекс\n            # обучения, как у предсказания\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # засчитываем истинное предсказание только один раз\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # истинно положительное и добавляем эту рамку в просмотренные\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # если IOU ниже порога, то предсказание является ложноположительным\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz для численного интегрирования\n        average_precisions.append(torch.trapz(precisions, recalls))\n    return sum(average_precisions) / len(average_precisions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:25.805783Z","iopub.execute_input":"2024-12-26T16:01:25.806110Z","iopub.status.idle":"2024-12-26T16:01:25.817883Z","shell.execute_reply.started":"2024-12-26T16:01:25.806081Z","shell.execute_reply":"2024-12-26T16:01:25.817002Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\n\ndef convert_cellboxes(predictions, S=7, C=7):\n    \"\"\"\n    Преобразует ограничивающие рамки, полученные от Yolo с\n    размером разбиения изображения S, в соотношения для всего изображения,\n    а не относительно ячеек. Пытались сделать это векторизованно,\n    но это привело к довольно сложному для чтения коду...\n    Использовать как черный ящик? Или реализовать более интуитивно понятный метод,\n    используя 2 цикла for, которые перебирают range(S) и преобразуют их\n    по одному, что приведет к более медленной, но более читаемой реализации.\n    \"\"\"\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, S, S, C + 10)\n    bboxes1 = predictions[..., C + 1:C + 5]\n    bboxes2 = predictions[..., C + 6:C + 10]\n    scores = torch.cat(\n        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n    \ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:29.306077Z","iopub.execute_input":"2024-12-26T16:01:29.306437Z","iopub.status.idle":"2024-12-26T16:01:29.321312Z","shell.execute_reply.started":"2024-12-26T16:01:29.306402Z","shell.execute_reply":"2024-12-26T16:01:29.320477Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=7, transform=None):\n        self.annotations = df\n        self.files_dir = files_dir\n        self.transform = transform\n        self.S = S\n        self.B = B\n        self.C = C\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = self.files_dir + self.annotations.iloc[index, 0][:-4]+'.txt'\n        boxes = []\n        names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\n        class_dictionary = dict((i, names[i]) for i in range(7))\n        with open(label_path,'r') as file:\n            klass, centerx, centery, boxwidth, boxheight = map(float, file.readline()[:-1].split())\n            boxes.append([klass, centerx, centery, boxwidth, boxheight])\n                \n        boxes = torch.tensor(boxes)\n        img_path = self.files_dir+self.annotations.iloc[index, 0]\n        image = Image.open(img_path)\n        image = image.convert(\"RGB\")\n\n        if self.transform:\n            # image = self.transform(image)\n            image, boxes = self.transform(image, boxes)\n\n        # Convert To Cells\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box.tolist()\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n\n            \"\"\"\n            Вычисление ширины и высоты ячейки ограничивающей рамки\n            относительно ячейки выполняется следующим образом, на примере\n            ширины:\n            \n            width_pixels = (width*self.image_width)\n            cell_pixels = (self.image_width)\n            \n            Затем, чтобы найти ширину относительно ячейки, достаточно:\n            width_pixels/cell_pixels, что при упрощении приводит к\n            формулам ниже.\n            \"\"\"\n            width_cell, height_cell = (\n                width * self.S,\n                height * self.S,\n            )\n\n            # If no object already found for specific cell i,j\n            # Note: This means we restrict to ONE object\n            # per cell!\n#             print(i, j)\n            if label_matrix[i, j, self.C] == 0:\n                # Set that there exists an object\n                label_matrix[i, j, self.C] = 1\n\n                # Box coordinates\n                box_coordinates = torch.tensor(\n                    [x_cell, y_cell, width_cell, height_cell]\n                )\n\n                label_matrix[i, j, 4:8] = box_coordinates\n\n                # Set one hot encoding for class_label\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:02:41.403257Z","iopub.execute_input":"2024-12-26T16:02:41.403970Z","iopub.status.idle":"2024-12-26T16:02:41.415045Z","shell.execute_reply.started":"2024-12-26T16:02:41.403936Z","shell.execute_reply":"2024-12-26T16:02:41.414204Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    \"\"\"\n    Calculate the loss for yolo (v1) model\n    \"\"\"\n\n    def __init__(self, S=7, B=2, C=7):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss(reduction=\"sum\")\n\n        \"\"\"\n        S is split size of image (in paper 7),\n        B is number of boxes (in paper 2),\n        C is number of classes (in paper 20, in dataset 3),\n        \"\"\"\n        self.S = S\n        self.B = B\n        self.C = C\n\n        # These are from Yolo paper, signifying how much we should\n        # pay loss for no object (noobj) and the box coordinates (coord)\n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n\n    def forward(self, predictions, target):\n        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes with target bbox\n        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        # Note that bestbox will be indices of 0, 1 for which bbox was best\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n\n        # ======================== #\n        #   FOR BOX COORDINATES    #\n        # ======================== #\n\n        # Set boxes with no object in them to 0. We only take out one of the two \n        # predictions, which is the one with highest Iou calculated previously.\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., self.C + 6:self.C + 10]\n                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n            )\n        )\n\n        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # ==================== #\n        #   FOR OBJECT LOSS    #\n        # ==================== #\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n        )\n\n        # ======================= #\n        # ЛОСС ОТСУТСТВИЯ КЛАССА  #\n        # ======================= #\n\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n        )\n\n        # ================== #\n        #   КЛАССОВЫЙ ЛОСС   #\n        # ================== #\n\n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:36.891525Z","iopub.execute_input":"2024-12-26T16:01:36.892306Z","iopub.status.idle":"2024-12-26T16:01:36.905634Z","shell.execute_reply.started":"2024-12-26T16:01:36.892270Z","shell.execute_reply":"2024-12-26T16:01:36.904713Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"LEARNING_RATE = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 10 # 64 in original paper but resource exhausted error otherwise.\nWEIGHT_DECAY = 0.1\nEPOCHS = 20\nNUM_WORKERS = 2\nPIN_MEMORY = True\nLOAD_MODEL = False\nLOAD_MODEL_FILE = \"model.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:41.537818Z","iopub.execute_input":"2024-12-26T16:01:41.538488Z","iopub.status.idle":"2024-12-26T16:01:41.542882Z","shell.execute_reply.started":"2024-12-26T16:01:41.538453Z","shell.execute_reply":"2024-12-26T16:01:41.542015Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n    \n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss = loss.item())\n        \n    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:43.136082Z","iopub.execute_input":"2024-12-26T16:01:43.136434Z","iopub.status.idle":"2024-12-26T16:01:43.142236Z","shell.execute_reply.started":"2024-12-26T16:01:43.136403Z","shell.execute_reply":"2024-12-26T16:01:43.141366Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:45.635374Z","iopub.execute_input":"2024-12-26T16:01:45.636244Z","iopub.status.idle":"2024-12-26T16:01:45.641195Z","shell.execute_reply.started":"2024-12-26T16:01:45.636209Z","shell.execute_reply":"2024-12-26T16:01:45.640316Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def main():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=3, verbose=True)\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    train_dataset = BTSDataset(\n        transform=transform,\n        files_dir=files_dir,\n        df = train\n    )\n\n    test_dataset = BTSDataset(\n        transform=transform, \n        files_dir=files_dir,\n        df = test\n    )\n\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    for epoch in range(EPOCHS):\n        train_fn(train_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            train_loader, model, iou_threshold=0.2, threshold=0.2\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.2, box_format=\"midpoint\"\n        )\n        print(f\"Train mAP: {mean_avg_prec}\")\n        \n        scheduler.step(mean_avg_prec)\n    \n    checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:01:48.283546Z","iopub.execute_input":"2024-12-26T16:01:48.284234Z","iopub.status.idle":"2024-12-26T16:01:48.291471Z","shell.execute_reply.started":"2024-12-26T16:01:48.284201Z","shell.execute_reply":"2024-12-26T16:01:48.290606Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T16:02:46.001378Z","iopub.execute_input":"2024-12-26T16:02:46.002207Z","iopub.status.idle":"2024-12-26T16:05:04.465641Z","shell.execute_reply.started":"2024-12-26T16:02:46.002173Z","shell.execute_reply":"2024-12-26T16:05:04.464223Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 65/65 [00:42<00:00,  1.53it/s, loss=917]    \n","output_type":"stream"},{"name":"stdout","text":"Mean loss was 2726.3417987530047\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Train mAP: 0.0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 65/65 [00:42<00:00,  1.53it/s, loss=1.64e+3]\n","output_type":"stream"},{"name":"stdout","text":"Mean loss was 29561.739325420673\nTrain mAP: 0.0\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 16/65 [00:10<00:31,  1.54it/s, loss=1.73e+3]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[37], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     32\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     33\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     34\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m get_bboxes(\n\u001b[1;32m     42\u001b[0m         train_loader, model, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     mean_avg_prec \u001b[38;5;241m=\u001b[39m mean_average_precision(\n\u001b[1;32m     46\u001b[0m         pred_boxes, target_boxes, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     )\n","Cell \u001b[0;32mIn[35], line 5\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      2\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[1;32m      6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      7\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[39], line 29\u001b[0m, in \u001b[0;36mBTSDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# image = self.transform(image)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     image, boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Convert To Cells\u001b[39;00m\n\u001b[1;32m     32\u001b[0m label_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB))\n","Cell \u001b[0;32mIn[36], line 7\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img, bboxes)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, bboxes):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m----> 7\u001b[0m         img, bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m, bboxes\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, bboxes\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:742\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    740\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 742\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:823\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    820\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in tobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m--> 823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"LOAD_MODEL = True\nEPOCHS = 1","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predictions():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    test_dataset = BTSDataset(\n        transform=transform, \n        df=test,\n        files_dir=files_dir\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n        \n    for epoch in range(EPOCHS):\n        model.eval()\n        train_fn(test_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            test_loader, model, iou_threshold=0.9, threshold=0.9\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.9, box_format=\"midpoint\"\n        )\n        print(f\"Test mAP: {mean_avg_prec}\")\n\n\npredictions()","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n\noptimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n)\n\nloss_fn = YoloLoss()\n\nload_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\ntest_dataset = BTSDataset(\n        transform=transform, \n        df=test,\n        files_dir=files_dir\n    )\n\ntest_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bboxes_images(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_images = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        all_images.append(x)\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes, torch.cat(all_images, dim=0)\n\n\npred_boxes, target_boxes, images = get_bboxes_images(\n            test_loader, model, iou_threshold=0.9, threshold=0.9\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [{'num_imag': pred_box[0], 'class': pred_box[1], 'conf': pred_box[2], 'box': list(np.array(pred_box[3:])*448)} \\\n         for pred_box in pred_boxes]\npreds[:10:1]","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncolor_classes = {\n    0:'red',\n    1:\"blue\",\n    2:\"green\",\n    3:'purple',\n    4:'orange',\n    5:'yellow',\n    6:'black'\n}\n\nfig, ax = plt.subplots(5, 5, figsize = (10, 10))\nfor idx, image in enumerate(images):\n    try:\n        ax[idx//5, idx%5].imshow(image.permute((1, 2, 0)).detach().numpy())\n    except IndexError:\n        pass\nfor box in preds:\n    # Create a Rectangle patch\n    box_rect = box.get('box')\n    center_x = box_rect[0]\n    center_y = box_rect[1]\n    width = box_rect[2]\n    height = box_rect[3]\n    rect = patches.Rectangle((center_x, center_y), \n                             width, height, \n                             linewidth=1, \n                             edgecolor=color_classes.get(int(box.get(\"class\"))), \n                             facecolor='none')\n    idx_box = box.get(\"num_imag\")\n    # Add the patch to the Axes\n    try:\n        ax[idx_box//5, idx_box%5].add_patch(rect)\n    except IndexError:\n        pass","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T15:44:35.415Z"}},"outputs":[],"execution_count":null}]}