{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10287488,"sourceType":"datasetVersion","datasetId":6366491}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport os\nimport PIL\nimport skimage\nfrom skimage import io\nimport numpy as np\nfrom PIL import Image\nimport shutil \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms.functional as FT\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchinfo import summary\nimport copy\nimport datetime\nimport random\nimport traceback\nfrom IPython.display import display, clear_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nseed = 42\nimport cv2\nimport xml.etree.ElementTree as ET\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:52:00.361819Z","iopub.execute_input":"2024-12-26T13:52:00.362142Z","iopub.status.idle":"2024-12-26T13:52:05.798851Z","shell.execute_reply.started":"2024-12-26T13:52:00.362111Z","shell.execute_reply":"2024-12-26T13:52:05.797935Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torchvision import models\nresnet50_model = models.resnet50(pretrained=True)\nresnet50_model.avgpool = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n#resnet18_model.avgpool = nn.Sequential()\nresnet50_model.fc = nn.Sequential()\nfor param in resnet50_model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:35:40.446756Z","iopub.execute_input":"2024-12-26T13:35:40.447429Z","iopub.status.idle":"2024-12-26T13:35:42.156227Z","shell.execute_reply.started":"2024-12-26T13:35:40.447378Z","shell.execute_reply":"2024-12-26T13:35:42.155106Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 127MB/s] \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"summary(resnet50_model, input_size=(16, 3, 448, 448))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:35:42.157389Z","iopub.execute_input":"2024-12-26T13:35:42.157698Z","iopub.status.idle":"2024-12-26T13:35:52.074409Z","shell.execute_reply.started":"2024-12-26T13:35:42.157669Z","shell.execute_reply":"2024-12-26T13:35:52.073233Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet                                   [16, 100352]              --\n├─Conv2d: 1-1                            [16, 64, 224, 224]        9,408\n├─BatchNorm2d: 1-2                       [16, 64, 224, 224]        128\n├─ReLU: 1-3                              [16, 64, 224, 224]        --\n├─MaxPool2d: 1-4                         [16, 64, 112, 112]        --\n├─Sequential: 1-5                        [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-1                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-1                  [16, 64, 112, 112]        4,096\n│    │    └─BatchNorm2d: 3-2             [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-3                    [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-4                  [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-5             [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-6                    [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-7                  [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-8             [16, 256, 112, 112]       512\n│    │    └─Sequential: 3-9              [16, 256, 112, 112]       16,896\n│    │    └─ReLU: 3-10                   [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-2                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-11                 [16, 64, 112, 112]        16,384\n│    │    └─BatchNorm2d: 3-12            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-13                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-14                 [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-15            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-16                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-17                 [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-18            [16, 256, 112, 112]       512\n│    │    └─ReLU: 3-19                   [16, 256, 112, 112]       --\n│    └─Bottleneck: 2-3                   [16, 256, 112, 112]       --\n│    │    └─Conv2d: 3-20                 [16, 64, 112, 112]        16,384\n│    │    └─BatchNorm2d: 3-21            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-22                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-23                 [16, 64, 112, 112]        36,864\n│    │    └─BatchNorm2d: 3-24            [16, 64, 112, 112]        128\n│    │    └─ReLU: 3-25                   [16, 64, 112, 112]        --\n│    │    └─Conv2d: 3-26                 [16, 256, 112, 112]       16,384\n│    │    └─BatchNorm2d: 3-27            [16, 256, 112, 112]       512\n│    │    └─ReLU: 3-28                   [16, 256, 112, 112]       --\n├─Sequential: 1-6                        [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-4                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-29                 [16, 128, 112, 112]       32,768\n│    │    └─BatchNorm2d: 3-30            [16, 128, 112, 112]       256\n│    │    └─ReLU: 3-31                   [16, 128, 112, 112]       --\n│    │    └─Conv2d: 3-32                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-33            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-34                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-35                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-36            [16, 512, 56, 56]         1,024\n│    │    └─Sequential: 3-37             [16, 512, 56, 56]         132,096\n│    │    └─ReLU: 3-38                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-5                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-39                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-40            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-41                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-42                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-43            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-44                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-45                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-46            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-47                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-6                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-48                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-49            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-50                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-51                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-52            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-53                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-54                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-55            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-56                   [16, 512, 56, 56]         --\n│    └─Bottleneck: 2-7                   [16, 512, 56, 56]         --\n│    │    └─Conv2d: 3-57                 [16, 128, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-58            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-59                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-60                 [16, 128, 56, 56]         147,456\n│    │    └─BatchNorm2d: 3-61            [16, 128, 56, 56]         256\n│    │    └─ReLU: 3-62                   [16, 128, 56, 56]         --\n│    │    └─Conv2d: 3-63                 [16, 512, 56, 56]         65,536\n│    │    └─BatchNorm2d: 3-64            [16, 512, 56, 56]         1,024\n│    │    └─ReLU: 3-65                   [16, 512, 56, 56]         --\n├─Sequential: 1-7                        [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-8                   [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-66                 [16, 256, 56, 56]         131,072\n│    │    └─BatchNorm2d: 3-67            [16, 256, 56, 56]         512\n│    │    └─ReLU: 3-68                   [16, 256, 56, 56]         --\n│    │    └─Conv2d: 3-69                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-70            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-71                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-72                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-73            [16, 1024, 28, 28]        2,048\n│    │    └─Sequential: 3-74             [16, 1024, 28, 28]        526,336\n│    │    └─ReLU: 3-75                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-9                   [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-76                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-77            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-78                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-79                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-80            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-81                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-82                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-83            [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-84                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-10                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-85                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-86            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-87                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-88                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-89            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-90                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-91                 [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-92            [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-93                   [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-11                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-94                 [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-95            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-96                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-97                 [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-98            [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-99                   [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-100                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-101           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-102                  [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-12                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-103                [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-104           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-105                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-106                [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-107           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-108                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-109                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-110           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-111                  [16, 1024, 28, 28]        --\n│    └─Bottleneck: 2-13                  [16, 1024, 28, 28]        --\n│    │    └─Conv2d: 3-112                [16, 256, 28, 28]         262,144\n│    │    └─BatchNorm2d: 3-113           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-114                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-115                [16, 256, 28, 28]         589,824\n│    │    └─BatchNorm2d: 3-116           [16, 256, 28, 28]         512\n│    │    └─ReLU: 3-117                  [16, 256, 28, 28]         --\n│    │    └─Conv2d: 3-118                [16, 1024, 28, 28]        262,144\n│    │    └─BatchNorm2d: 3-119           [16, 1024, 28, 28]        2,048\n│    │    └─ReLU: 3-120                  [16, 1024, 28, 28]        --\n├─Sequential: 1-8                        [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-14                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-121                [16, 512, 28, 28]         524,288\n│    │    └─BatchNorm2d: 3-122           [16, 512, 28, 28]         1,024\n│    │    └─ReLU: 3-123                  [16, 512, 28, 28]         --\n│    │    └─Conv2d: 3-124                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-125           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-126                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-127                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-128           [16, 2048, 14, 14]        4,096\n│    │    └─Sequential: 3-129            [16, 2048, 14, 14]        2,101,248\n│    │    └─ReLU: 3-130                  [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-15                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-131                [16, 512, 14, 14]         1,048,576\n│    │    └─BatchNorm2d: 3-132           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-133                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-134                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-135           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-136                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-137                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-138           [16, 2048, 14, 14]        4,096\n│    │    └─ReLU: 3-139                  [16, 2048, 14, 14]        --\n│    └─Bottleneck: 2-16                  [16, 2048, 14, 14]        --\n│    │    └─Conv2d: 3-140                [16, 512, 14, 14]         1,048,576\n│    │    └─BatchNorm2d: 3-141           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-142                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-143                [16, 512, 14, 14]         2,359,296\n│    │    └─BatchNorm2d: 3-144           [16, 512, 14, 14]         1,024\n│    │    └─ReLU: 3-145                  [16, 512, 14, 14]         --\n│    │    └─Conv2d: 3-146                [16, 2048, 14, 14]        1,048,576\n│    │    └─BatchNorm2d: 3-147           [16, 2048, 14, 14]        4,096\n│    │    └─ReLU: 3-148                  [16, 2048, 14, 14]        --\n├─AvgPool2d: 1-9                         [16, 2048, 7, 7]          --\n├─Sequential: 1-10                       [16, 100352]              --\n==========================================================================================\nTotal params: 23,508,032\nTrainable params: 23,508,032\nNon-trainable params: 0\nTotal mult-adds (G): 261.58\n==========================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 11380.72\nParams size (MB): 94.03\nEstimated Total Size (MB): 11513.29\n=========================================================================================="},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:35:52.076661Z","iopub.execute_input":"2024-12-26T13:35:52.077115Z","iopub.status.idle":"2024-12-26T13:35:52.082033Z","shell.execute_reply.started":"2024-12-26T13:35:52.077070Z","shell.execute_reply":"2024-12-26T13:35:52.080970Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"temp1 = ['/'+image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))\n         and os.path.getsize(files_dir+'/'+image[:-4]+'.txt') != 0]\ntemp2 = ['/'+annot for annot in sorted(os.listdir(files_dir))\n                        if (annot[-4:]=='.txt') and os.path.getsize(files_dir+'/'+annot) != 0]\n\nimages = pd.Series(temp1, name='images')\nimage_id = pd.Series(list(range(len(temp1))), name='id')\ntrain_img_df = pd.DataFrame(pd.concat([images, image_id], axis=1))\nimages = []\nimage_id = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for j in range(len(file.readlines())):\n            images.append(temp1[i])\n            image_id.append(i)\n        file.close()\nbboxes = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for line in file.readlines():\n            bboxes.append(list(map(float, line.split())))\n        file.close()\nimages = pd.Series(images, name='images')\nbboxes = pd.Series(bboxes, name='bboxes')\nimage_id = pd.Series(image_id, name='image_id')\nind = pd.Series(list(range(len(images))), name='id')\ndf = pd.concat([images, ind,image_id,bboxes], axis=1)\ntrain_df = pd.DataFrame(df)\narea = []\nfor i in range(train_df.shape[0]):\n    area.append(train_df.iloc[i,3][3]*train_df.iloc[i,3][4])\ntrain_df = pd.concat([train_df, pd.Series(area, name='area')],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:48:21.419534Z","iopub.execute_input":"2024-12-26T13:48:21.419950Z","iopub.status.idle":"2024-12-26T13:48:30.289884Z","shell.execute_reply.started":"2024-12-26T13:48:21.419916Z","shell.execute_reply":"2024-12-26T13:48:30.288747Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T13:48:30.291782Z","iopub.execute_input":"2024-12-26T13:48:30.292125Z","iopub.status.idle":"2024-12-26T13:48:30.317443Z","shell.execute_reply.started":"2024-12-26T13:48:30.292091Z","shell.execute_reply":"2024-12-26T13:48:30.316207Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"            images   id  image_id  \\\n0      /jhope0.png    0         0   \n1      /jhope1.png    1         1   \n2     /jhope10.png    2         2   \n3    /jhope100.png    3         3   \n4    /jhope101.png    4         4   \n..             ...  ...       ...   \n804      /v291.png  804       803   \n805      /v292.png  805       804   \n806      /v293.png  806       805   \n807      /v294.png  807       806   \n808      /v295.png  808       807   \n\n                                            bboxes      area  \n0    [0.0, 0.497253, 0.289855, 0.467033, 0.543478]  0.253822  \n1    [0.0, 0.477778, 0.391111, 0.848889, 0.782222]  0.664020  \n2             [0.0, 0.4, 0.393333, 0.408889, 0.44]  0.179911  \n3    [0.0, 0.555184, 0.446429, 0.622074, 0.892857]  0.555423  \n4     [0.0, 0.52901, 0.319767, 0.361775, 0.639535]  0.231368  \n..                                             ...       ...  \n804  [5.0, 0.553763, 0.279006, 0.218638, 0.292818]  0.064021  \n805  [5.0, 0.426667, 0.255556, 0.382222, 0.342222]  0.130805  \n806  [5.0, 0.591667, 0.494048, 0.456667, 0.714286]  0.326191  \n807   [5.0, 0.528796, 0.301136, 0.743455, 0.42803]  0.318221  \n808  [5.0, 0.507895, 0.390566, 0.952632, 0.615094]  0.585958  \n\n[809 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>images</th>\n      <th>id</th>\n      <th>image_id</th>\n      <th>bboxes</th>\n      <th>area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/jhope0.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0.0, 0.497253, 0.289855, 0.467033, 0.543478]</td>\n      <td>0.253822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/jhope1.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0.0, 0.477778, 0.391111, 0.848889, 0.782222]</td>\n      <td>0.664020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/jhope10.png</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0.0, 0.4, 0.393333, 0.408889, 0.44]</td>\n      <td>0.179911</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/jhope100.png</td>\n      <td>3</td>\n      <td>3</td>\n      <td>[0.0, 0.555184, 0.446429, 0.622074, 0.892857]</td>\n      <td>0.555423</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/jhope101.png</td>\n      <td>4</td>\n      <td>4</td>\n      <td>[0.0, 0.52901, 0.319767, 0.361775, 0.639535]</td>\n      <td>0.231368</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>804</th>\n      <td>/v291.png</td>\n      <td>804</td>\n      <td>803</td>\n      <td>[5.0, 0.553763, 0.279006, 0.218638, 0.292818]</td>\n      <td>0.064021</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>/v292.png</td>\n      <td>805</td>\n      <td>804</td>\n      <td>[5.0, 0.426667, 0.255556, 0.382222, 0.342222]</td>\n      <td>0.130805</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>/v293.png</td>\n      <td>806</td>\n      <td>805</td>\n      <td>[5.0, 0.591667, 0.494048, 0.456667, 0.714286]</td>\n      <td>0.326191</td>\n    </tr>\n    <tr>\n      <th>807</th>\n      <td>/v294.png</td>\n      <td>807</td>\n      <td>806</td>\n      <td>[5.0, 0.528796, 0.301136, 0.743455, 0.42803]</td>\n      <td>0.318221</td>\n    </tr>\n    <tr>\n      <th>808</th>\n      <td>/v295.png</td>\n      <td>808</td>\n      <td>807</td>\n      <td>[5.0, 0.507895, 0.390566, 0.952632, 0.615094]</td>\n      <td>0.585958</td>\n    </tr>\n  </tbody>\n</table>\n<p>809 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:35.241485Z","iopub.execute_input":"2024-12-24T20:23:35.242094Z","iopub.status.idle":"2024-12-24T20:23:35.526413Z","shell.execute_reply.started":"2024-12-24T20:23:35.242057Z","shell.execute_reply":"2024-12-24T20:23:35.525733Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train, test = train_test_split(df, test_size = 0.2, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:40.336377Z","iopub.execute_input":"2024-12-24T20:23:40.337073Z","iopub.status.idle":"2024-12-24T20:23:40.343304Z","shell.execute_reply.started":"2024-12-24T20:23:40.337035Z","shell.execute_reply":"2024-12-24T20:23:40.342325Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"img_transforms  = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[-0.0932, -0.0971, -0.1260], std=[0.5091, 0.4912, 0.4931])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:42.481071Z","iopub.execute_input":"2024-12-24T20:23:42.481521Z","iopub.status.idle":"2024-12-24T20:23:42.486597Z","shell.execute_reply.started":"2024-12-24T20:23:42.481473Z","shell.execute_reply":"2024-12-24T20:23:42.485512Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!mkdir train test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:44.764070Z","iopub.execute_input":"2024-12-24T20:23:44.764372Z","iopub.status.idle":"2024-12-24T20:23:45.782092Z","shell.execute_reply.started":"2024-12-24T20:23:44.764347Z","shell.execute_reply":"2024-12-24T20:23:45.780877Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!mkdir train/jhope train/jin train/jimin train/jungkook train/suga train/rm train/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:46.983760Z","iopub.execute_input":"2024-12-24T20:23:46.984142Z","iopub.status.idle":"2024-12-24T20:23:48.001248Z","shell.execute_reply.started":"2024-12-24T20:23:46.984111Z","shell.execute_reply":"2024-12-24T20:23:48.000281Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!mkdir test/jhope test/jin test/jimin test/jungkook test/suga test/rm test/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:49.318170Z","iopub.execute_input":"2024-12-24T20:23:49.318578Z","iopub.status.idle":"2024-12-24T20:23:50.325767Z","shell.execute_reply.started":"2024-12-24T20:23:49.318521Z","shell.execute_reply":"2024-12-24T20:23:50.324488Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\nclass_names = dict((i, names[i]) for i in range(7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:50.803230Z","iopub.execute_input":"2024-12-24T20:23:50.803647Z","iopub.status.idle":"2024-12-24T20:23:50.808900Z","shell.execute_reply.started":"2024-12-24T20:23:50.803612Z","shell.execute_reply":"2024-12-24T20:23:50.807974Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"for class_name in class_names:\n    for i in range(train.shape[0]):\n        temp = train.iloc[i,:][0].split('.')[0]\n        flags = list(map(str.isalpha, list(temp)))\n        temp_str = temp[:flags.index(False)]\n        if temp_str == class_names.get(class_name):\n            shutil.copy(os.path.join(files_dir, train.iloc[i,:][0]), \\\n                        os.path.join('/kaggle/working/train'+'/'+class_names.get(class_name), train.iloc[i,:][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:53.293028Z","iopub.execute_input":"2024-12-24T20:23:53.293647Z","iopub.status.idle":"2024-12-24T20:23:55.835215Z","shell.execute_reply.started":"2024-12-24T20:23:53.293608Z","shell.execute_reply":"2024-12-24T20:23:55.834258Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/588640762.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  temp = train.iloc[i,:][0].split('.')[0]\n/tmp/ipykernel_30/588640762.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  shutil.copy(os.path.join(files_dir, train.iloc[i,:][0]), \\\n/tmp/ipykernel_30/588640762.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  os.path.join('/kaggle/working/train'+'/'+class_names.get(class_name), train.iloc[i,:][0]))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!ls train/v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:56.095762Z","iopub.execute_input":"2024-12-24T20:23:56.096120Z","iopub.status.idle":"2024-12-24T20:23:57.111436Z","shell.execute_reply.started":"2024-12-24T20:23:56.096090Z","shell.execute_reply":"2024-12-24T20:23:57.110612Z"}},"outputs":[{"name":"stdout","text":"v0.png\t  v110.png  v16.png   v192.png\tv200.png  v218.png  v278.png  v293.png\nv10.png   v114.png  v169.png  v193.png\tv201.png  v22.png   v280.png  v295.png\nv101.png  v115.png  v175.png  v196.png\tv206.png  v23.png   v281.png\nv103.png  v118.png  v177.png  v197.png\tv208.png  v27.png   v283.png\nv104.png  v119.png  v179.png  v198.png\tv209.png  v270.png  v286.png\nv105.png  v12.png   v180.png  v199.png\tv21.png   v273.png  v287.png\nv109.png  v14.png   v185.png  v2.png\tv212.png  v274.png  v288.png\nv11.png   v15.png   v187.png  v20.png\tv213.png  v275.png  v289.png\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"for class_name in class_names:\n    for i in range(test.shape[0]):\n        temp = test.iloc[i,:][0].split('.')[0]\n        flags = list(map(str.isalpha, list(temp)))\n        temp_str = temp[:flags.index(False)]\n        if temp_str == class_names.get(class_name):\n            shutil.copy(os.path.join(files_dir, test.iloc[i,:][0]), \\\n                        os.path.join('/kaggle/working/test'+'/'+class_names.get(class_name), test.iloc[i,:][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:23:58.258742Z","iopub.execute_input":"2024-12-24T20:23:58.259092Z","iopub.status.idle":"2024-12-24T20:24:00.051608Z","shell.execute_reply.started":"2024-12-24T20:23:58.259065Z","shell.execute_reply":"2024-12-24T20:24:00.050623Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/598401277.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  temp = test.iloc[i,:][0].split('.')[0]\n/tmp/ipykernel_30/598401277.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  shutil.copy(os.path.join(files_dir, test.iloc[i,:][0]), \\\n/tmp/ipykernel_30/598401277.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  os.path.join('/kaggle/working/test'+'/'+class_names.get(class_name), test.iloc[i,:][0]))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!ls test/suga","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:00.738475Z","iopub.execute_input":"2024-12-24T20:24:00.739336Z","iopub.status.idle":"2024-12-24T20:24:01.745593Z","shell.execute_reply.started":"2024-12-24T20:24:00.739301Z","shell.execute_reply":"2024-12-24T20:24:01.744696Z"}},"outputs":[{"name":"stdout","text":"suga0.png    suga16.png   suga19.png   suga26.png   suga28.png\t suga376.png\nsuga101.png  suga17.png   suga20.png   suga268.png  suga290.png  suga378.png\nsuga113.png  suga175.png  suga201.png  suga270.png  suga309.png  suga38.png\nsuga114.png  suga178.png  suga204.png  suga274.png  suga312.png  suga381.png\nsuga119.png  suga183.png  suga214.png  suga276.png  suga314.png\nsuga13.png   suga185.png  suga217.png  suga277.png  suga316.png\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"train_data = ImageFolder(root='/kaggle/working/train', transform=img_transforms)\nvalidation_data = ImageFolder(root='/kaggle/working/test', transform=img_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:03.830420Z","iopub.execute_input":"2024-12-24T20:24:03.830777Z","iopub.status.idle":"2024-12-24T20:24:03.839592Z","shell.execute_reply.started":"2024-12-24T20:24:03.830748Z","shell.execute_reply":"2024-12-24T20:24:03.838890Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_data, batch_size=16, shuffle=True, num_workers=2)\nvalidation_dataloader = DataLoader(dataset=validation_data, batch_size=16, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:06.302970Z","iopub.execute_input":"2024-12-24T20:24:06.303613Z","iopub.status.idle":"2024-12-24T20:24:06.308252Z","shell.execute_reply.started":"2024-12-24T20:24:06.303568Z","shell.execute_reply":"2024-12-24T20:24:06.307316Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=16,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0, \n                    plot=False):\n    \"\"\"\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n\n    best_val_loss = float('inf')\n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n\n# Dynamic plot\n    if plot:\n        plot_epoch_data = []\n        plot_train_loss = []\n        plot_val_loss = []\n\n        fig, ax = plt.subplots()\n        line_train, = ax.plot([], [], 'r-')\n        line_val, = ax.plot([], [], 'b-')\n        ax.legend(['train', 'val'])\n        ax.set_xlim(0, epoch_n)\n\n        def add_point(epoch_i, train_loss, val_loss):\n            max_loss = max(ax.viewLim.y1 / 1.1, train_loss, val_loss)\n            ax.set_ylim(0, max_loss * 1.1)\n            \n            plot_epoch_data.append(epoch_i)\n            plot_train_loss.append(train_loss)\n            plot_val_loss.append(val_loss)\n            line_train.set_data(plot_epoch_data, plot_train_loss)\n            line_val.set_data(plot_epoch_data, plot_val_loss)\n            clear_output(wait=True)\n            display(fig)\n\n\n    for epoch_i in range(epoch_n):\n        try:\n            epoch_start = datetime.datetime.now()\n            \n\n            print('Эпоха {}'.format(epoch_i))\n\n            model.train()\n            mean_train_loss = 0\n            train_batches_n = 0\n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                mean_train_loss += float(loss)\n                train_batches_n += 1\n\n            mean_train_loss /= train_batches_n\n\n            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n            print('Среднее значение функции потерь на обучении', mean_train_loss)\n\n\n\n            model.eval()\n            mean_val_loss = 0\n            val_batches_n = 0\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n\n                    mean_val_loss += float(loss)\n                    val_batches_n += 1\n\n            mean_val_loss /= val_batches_n\n\n            if plot:\n                add_point(epoch_i, mean_train_loss, mean_val_loss)\n            else:\n                pass\n            \n            print('Среднее значение функции потерь на валидации', mean_val_loss)\n\n            if mean_val_loss < best_val_loss:\n                best_epoch_i = epoch_i\n                best_val_loss = mean_val_loss\n                best_model = copy.deepcopy(model)\n                print('Новая лучшая модель! На эпохе {}'.format(epoch_i))\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(mean_val_loss)\n\n            print()\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n        finally:\n            if plot:\n                plt.close(fig)\n\n    return best_val_loss, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:09.017832Z","iopub.execute_input":"2024-12-24T20:24:09.018308Z","iopub.status.idle":"2024-12-24T20:24:09.041116Z","shell.execute_reply.started":"2024-12-24T20:24:09.018271Z","shell.execute_reply":"2024-12-24T20:24:09.040323Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:23.916017Z","iopub.execute_input":"2024-12-24T20:24:23.916364Z","iopub.status.idle":"2024-12-24T20:24:23.920784Z","shell.execute_reply.started":"2024-12-24T20:24:23.916332Z","shell.execute_reply":"2024-12-24T20:24:23.919766Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"best_loss, best_model = train_eval_loop(model=resnet50_model, \n                train_dataset=train_data, \n                val_dataset=validation_data, \n                criterion=nn.CrossEntropyLoss(),\n                lr=1e-3, \n                epoch_n=100, \n                batch_size=16,\n                device=device, \n                early_stopping_patience=20, \n                l2_reg_alpha=0.999,\n                max_batches_per_epoch_train=10000,\n                max_batches_per_epoch_val=1000,\n                data_loader_ctor=DataLoader,\n                optimizer_ctor=torch.optim.Adam,\n                lr_scheduler_ctor=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                shuffle_train=True,\n                dataloader_workers_n=2,\n                plot=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:25.538188Z","iopub.execute_input":"2024-12-24T20:24:25.539022Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Среднее значение функции потерь на валидации 7.865225496746245\n\nЭпоха 26\nЭпоха: 31 итераций, 3.52 сек\nСреднее значение функции потерь на обучении 0.05608216528930972\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"predicted_labels = []\nactual_labels = []\n\nresnet50_model.eval()  # Set the model to evaluation mode\nwith torch.inference_mode():  # Ensure no gradients are computed\n    for images, labels in validation_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = resnet50_model(images)\n        _, predicted = torch.max(outputs, 1)\n        predicted_labels.extend(predicted.cpu().numpy())\n        actual_labels.extend(labels.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\nclassification_report, confusion_matrix\n\n# Вычисление тестовых метрик\naccuracy = accuracy_score(actual_labels, predicted_labels)\nprecision = precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\nrecall = recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\nf1 = f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0)\n\n# Принт метрик\nprint(f\"Model Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Model Precision: {precision * 100:.2f}%\")\nprint(f\"Model Recall: {recall * 100:.2f}%\")\nprint(f\"Model F1 Score: {f1 * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(actual_labels, predicted_labels)\nclass_names = validation_dataloader.dataset.classes\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class YoloV1(nn.Module):\n    def __init__(self, in_channels=3, out_conv_channels=2048, **kwargs):\n        super(YoloV1, self).__init__()\n        self.in_channels = in_channels\n        self.model_channels = out_conv_channels\n        self.darknet = resnet50_model\n        self.fcs = self._create_fcs(**kwargs)\n        \n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x, start_dim=1))\n    \n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        \"\"\"\n        В изначальной статье используется nn.Linear(1024 * S * S, 4096), но не 496. \n        Также у последнего слоя будет изменена размерность до (S, S, 13) где C+B*5 = 13\n        \"\"\"\n        S, B, C = split_size, num_boxes, num_classes\n        return nn.Sequential(\n            nn.Flatten(), \n            nn.Linear(self.model_channels * S * S, 496), \n            nn.Dropout(0.0), \n            nn.LeakyReLU(0.1), \n            nn.Linear(496, S * S * (C + B * 5))\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.749761Z","iopub.execute_input":"2024-12-24T17:57:25.750021Z","iopub.status.idle":"2024-12-24T17:57:25.756457Z","shell.execute_reply.started":"2024-12-24T17:57:25.749997Z","shell.execute_reply":"2024-12-24T17:57:25.755627Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"Этот код на Python вычисляет пересечение над объединением (Intersection over Union, IoU) для пар предсказанных и истинных ограничивающих рамок. IoU — это метрика, которая часто используется в задачах компьютерного зрения для оценки качества предсказанных ограничивающих рамок. Вот как работает этот код:\n\n1. Параметры функции:\n\n   • boxes_preds — тензор с предсказанными ограничивающими рамками размером (BATCH_SIZE, 4).\n\n   • boxes_labels — тензор с истинными ограничивающими рамками размером (BATCH_SIZE, 4).\n\n   • box_format — формат представления рамок: 'midpoint' (центр и размеры) или 'corners' (координаты углов).\n\n2. Преобразование формата рамок:\n\n   • Если формат 'midpoint', рамки преобразуются из формата (x, y, w, h) в (x1, y1, x2, y2).\n\n   • Если формат 'corners', рамки уже в нужном формате (x1, y1, x2, y2).\n\n3. Вычисление координат пересечения:\n\n   • Используется функция torch.max для вычисления верхних левых углов пересечения.\n\n   • Используется функция torch.min для вычисления нижних правых углов пересечения.\n\n4. Вычисление площади пересечения:\n\n   • Площадь пересечения вычисляется как произведение высоты и ширины пересекающейся области. Для случая, когда рамки не пересекаются, используется .clamp(0), чтобы избежать отрицательных значений.\n\n5. Вычисление площадей отдельных рамок:\n\n   • Площади предсказанных и истинных рамок вычисляются на основе их координат.\n\n6. Вычисление IoU:\n\n   • IoU вычисляется как отношение площади пересечения к объединенной площади двух рамок (площадь первой рамки + площадь второй рамки - площадь пересечения). Небольшая константа 1e-6 добавляется к знаменателю для предотвращения деления на ноль.\n\nЭтот код полезен для оценки качества алгоритмов обнаружения объектов, так как позволяет количественно оценить, насколько хорошо предсказанные ограничивающие рамки соответствуют истинным объектам на изображении.\n","metadata":{}},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n    if box_format == 'midpoint':\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n        \n    if box_format == 'corners':\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4] \n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n    \n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n    \n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    \n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n    \n    return intersection / (box1_area + box2_area - intersection + 1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.758449Z","iopub.execute_input":"2024-12-24T17:57:25.758825Z","iopub.status.idle":"2024-12-24T17:57:25.770705Z","shell.execute_reply.started":"2024-12-24T17:57:25.758800Z","shell.execute_reply":"2024-12-24T17:57:25.770045Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    \"\"\"\n    Выполняет подавление немаксимумов (Non Max Suppression) для заданных ограничивающих рамок.\n    Параметры:\n        bboxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        threshold (float): порог для удаления предсказанных рамок (независимо от IoU)\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n    Возвращает:\n        list: ограничивающие рамки после выполнения NMS с заданным порогом IoU\n    \"\"\"\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.771578Z","iopub.execute_input":"2024-12-24T17:57:25.771800Z","iopub.status.idle":"2024-12-24T17:57:25.787506Z","shell.execute_reply.started":"2024-12-24T17:57:25.771778Z","shell.execute_reply":"2024-12-24T17:57:25.786775Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=7\n):\n    \"\"\"\n    Вычисляет среднюю точность (mean average precision).\n    Параметры:\n        pred_boxes (list): список списков, содержащих все ограничивающие рамки, \n        каждая из которых представлена как [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): аналогично pred_boxes, но для всех правильных\n        iou_threshold (float): порог, при котором предсказанная рамка считается корректной\n        box_format (str): \"midpoint\" или \"corners\", используемый для указания формата рамок\n        num_classes (int): количество классов\n    Возвращает:\n        float: значение mAP для всех классов при заданном пороге IoU\n    \"\"\"\n\n    # список для хранения всех AP для соответствующих классов\n    average_precisions = []\n\n    # используется для численной стабильности позже\n    epsilon = 1e-6\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Проходим через все предсказания и цели,\n        # и добавляем только те, которые принадлежат\n        # текущему классу c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # находим количество рамок для каждого обучающего примера\n        # Counter здесь находит, сколько истинных рамок мы получаем\n        # для каждого обучающего примера. Например, если у img 0 их 3,\n        # а у img 1 их 5, то мы получим словарь с:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # Затем мы проходим через каждый ключ и значение в этом словаре\n        # и преобразуем в следующее (относительно того же примера):\n        # amount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # сортируем по вероятностям рамок, которые находятся в индексе 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # Если ничего не существует для этого класса, то можно безопасно пропустить\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Берем только те истинные рамки, у которых такой же индекс\n            # обучения, как у предсказания\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # засчитываем истинное предсказание только один раз\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # истинно положительное и добавляем эту рамку в просмотренные\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # если IOU ниже порога, то предсказание является ложноположительным\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz для численного интегрирования\n        average_precisions.append(torch.trapz(precisions, recalls))\n    return sum(average_precisions) / len(average_precisions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.788672Z","iopub.execute_input":"2024-12-24T17:57:25.788946Z","iopub.status.idle":"2024-12-24T17:57:25.803141Z","shell.execute_reply.started":"2024-12-24T17:57:25.788916Z","shell.execute_reply":"2024-12-24T17:57:25.802431Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\n\ndef convert_cellboxes(predictions, S=7, C=7):\n    \"\"\"\n    Преобразует ограничивающие рамки, полученные от Yolo с\n    размером разбиения изображения S, в соотношения для всего изображения,\n    а не относительно ячеек. Пытались сделать это векторизованно,\n    но это привело к довольно сложному для чтения коду...\n    Использовать как черный ящик? Или реализовать более интуитивно понятный метод,\n    используя 2 цикла for, которые перебирают range(S) и преобразуют их\n    по одному, что приведет к более медленной, но более читаемой реализации.\n    \"\"\"\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, S, S, C + 10)\n    bboxes1 = predictions[..., C + 1:C + 5]\n    bboxes2 = predictions[..., C + 6:C + 10]\n    scores = torch.cat(\n        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 / S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n    \ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:25.804103Z","iopub.execute_input":"2024-12-24T17:57:25.804489Z","iopub.status.idle":"2024-12-24T17:57:25.820367Z","shell.execute_reply.started":"2024-12-24T17:57:25.804463Z","shell.execute_reply":"2024-12-24T17:57:25.819648Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=7, transform=None):\n        self.annotations = df\n        self.files_dir = files_dir\n        self.transform = transform\n        self.S = S\n        self.B = B\n        self.C = C\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n        boxes = []\n        names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\n        class_dictionary = dict((i, names[i]) for i in range(7))\n        with open(label_path,'r') as file:\n            klass, centerx, centery, boxwidth, boxheight = map(float, file.readline()[:-1].split())\n            boxes.append([klass, centerx, centery, boxwidth, boxheight])\n                \n        boxes = torch.tensor(boxes)\n        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        image = image.convert(\"RGB\")\n\n        if self.transform:\n            # image = self.transform(image)\n            image, boxes = self.transform(image, boxes)\n\n        # Convert To Cells\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box.tolist()\n            class_label = int(class_label)\n\n            # i,j represents the cell row and cell column\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n\n            \"\"\"\n            Вычисление ширины и высоты ячейки ограничивающей рамки\n            относительно ячейки выполняется следующим образом, на примере\n            ширины:\n            \n            width_pixels = (width*self.image_width)\n            cell_pixels = (self.image_width)\n            \n            Затем, чтобы найти ширину относительно ячейки, достаточно:\n            width_pixels/cell_pixels, что при упрощении приводит к\n            формулам ниже.\n            \"\"\"\n            width_cell, height_cell = (\n                width * self.S,\n                height * self.S,\n            )\n\n            # If no object already found for specific cell i,j\n            # Note: This means we restrict to ONE object\n            # per cell!\n#             print(i, j)\n            if label_matrix[i, j, self.C] == 0:\n                # Set that there exists an object\n                label_matrix[i, j, self.C] = 1\n\n                # Box coordinates\n                box_coordinates = torch.tensor(\n                    [x_cell, y_cell, width_cell, height_cell]\n                )\n\n                label_matrix[i, j, 4:8] = box_coordinates\n\n                # Set one hot encoding for class_label\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:26.998188Z","iopub.execute_input":"2024-12-24T17:57:26.998526Z","iopub.status.idle":"2024-12-24T17:57:27.015278Z","shell.execute_reply.started":"2024-12-24T17:57:26.998491Z","shell.execute_reply":"2024-12-24T17:57:27.014430Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    \"\"\"\n    Calculate the loss for yolo (v1) model\n    \"\"\"\n\n    def __init__(self, S=7, B=2, C=7):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss(reduction=\"sum\")\n\n        \"\"\"\n        S is split size of image (in paper 7),\n        B is number of boxes (in paper 2),\n        C is number of classes (in paper 20, in dataset 3),\n        \"\"\"\n        self.S = S\n        self.B = B\n        self.C = C\n\n        # These are from Yolo paper, signifying how much we should\n        # pay loss for no object (noobj) and the box coordinates (coord)\n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n\n    def forward(self, predictions, target):\n        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n\n        # Calculate IoU for the two predicted bounding boxes with target bbox\n        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n\n        # Take the box with highest IoU out of the two prediction\n        # Note that bestbox will be indices of 0, 1 for which bbox was best\n        iou_maxes, bestbox = torch.max(ious, dim=0)\n        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n\n        # ======================== #\n        #   FOR BOX COORDINATES    #\n        # ======================== #\n\n        # Set boxes with no object in them to 0. We only take out one of the two \n        # predictions, which is the one with highest Iou calculated previously.\n        box_predictions = exists_box * (\n            (\n                bestbox * predictions[..., self.C + 6:self.C + 10]\n                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n            )\n        )\n\n        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n\n        # Take sqrt of width, height of boxes to ensure that\n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n            torch.abs(box_predictions[..., 2:4] + 1e-6)\n        )\n        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2),\n            torch.flatten(box_targets, end_dim=-2),\n        )\n\n        # ==================== #\n        #   FOR OBJECT LOSS    #\n        # ==================== #\n\n        # pred_box is the confidence score for the bbox with highest IoU\n        pred_box = (\n            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n        )\n\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n        )\n\n        # ======================= #\n        # ЛОСС ОТСУТСТВИЯ КЛАССА  #\n        # ======================= #\n\n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n        )\n\n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n        )\n\n        # ================== #\n        #   КЛАССОВЫЙ ЛОСС   #\n        # ================== #\n\n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n        )\n\n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.016536Z","iopub.execute_input":"2024-12-24T17:57:27.016858Z","iopub.status.idle":"2024-12-24T17:57:27.032885Z","shell.execute_reply.started":"2024-12-24T17:57:27.016823Z","shell.execute_reply":"2024-12-24T17:57:27.032051Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"LEARNING_RATE = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\nWEIGHT_DECAY = 0.1\nEPOCHS = 20\nNUM_WORKERS = 2\nPIN_MEMORY = True\nLOAD_MODEL = False\nLOAD_MODEL_FILE = \"model.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.033854Z","iopub.execute_input":"2024-12-24T17:57:27.034100Z","iopub.status.idle":"2024-12-24T17:57:27.048520Z","shell.execute_reply.started":"2024-12-24T17:57:27.034077Z","shell.execute_reply":"2024-12-24T17:57:27.047755Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n    \n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss = loss.item())\n        \n    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.049546Z","iopub.execute_input":"2024-12-24T17:57:27.049885Z","iopub.status.idle":"2024-12-24T17:57:27.064443Z","shell.execute_reply.started":"2024-12-24T17:57:27.049850Z","shell.execute_reply":"2024-12-24T17:57:27.063636Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.065334Z","iopub.execute_input":"2024-12-24T17:57:27.065627Z","iopub.status.idle":"2024-12-24T17:57:27.076324Z","shell.execute_reply.started":"2024-12-24T17:57:27.065603Z","shell.execute_reply":"2024-12-24T17:57:27.075431Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def main():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    train_dataset = BTSDataset(\n        transform=transform,\n        files_dir=files_dir,\n        df = train\n    )\n\n    test_dataset = BTSDataset(\n        transform=transform, \n        files_dir=files_dir,\n        df = test\n    )\n\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    for epoch in range(EPOCHS):\n        train_fn(train_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            train_loader, model, iou_threshold=0.9, threshold=0.9\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.2, box_format=\"midpoint\"\n        )\n        print(f\"Train mAP: {mean_avg_prec}\")\n        \n        scheduler.step(mean_avg_prec)\n    \n    checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.077397Z","iopub.execute_input":"2024-12-24T17:57:27.077802Z","iopub.status.idle":"2024-12-24T17:57:27.090552Z","shell.execute_reply.started":"2024-12-24T17:57:27.077764Z","shell.execute_reply":"2024-12-24T17:57:27.089751Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:27.091624Z","iopub.execute_input":"2024-12-24T17:57:27.091973Z","iopub.status.idle":"2024-12-24T17:57:28.448646Z","shell.execute_reply.started":"2024-12-24T17:57:27.091937Z","shell.execute_reply":"2024-12-24T17:57:28.447278Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n  0%|          | 0/21 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[47], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     32\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     33\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     34\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m get_bboxes(\n\u001b[1;32m     42\u001b[0m         train_loader, model, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     mean_avg_prec \u001b[38;5;241m=\u001b[39m mean_average_precision(\n\u001b[1;32m     46\u001b[0m         pred_boxes, target_boxes, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     )\n","Cell \u001b[0;32mIn[45], line 7\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[1;32m      6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 7\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(out, y)\n\u001b[1;32m      9\u001b[0m     mean_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mYoloV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdarknet(x)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1000 and 100352x496)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x1000 and 100352x496)","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"LOAD_MODEL = True\nEPOCHS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.449730Z","iopub.status.idle":"2024-12-24T17:57:28.450178Z","shell.execute_reply.started":"2024-12-24T17:57:28.449949Z","shell.execute_reply":"2024-12-24T17:57:28.449972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predictions():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    test_dataset = BTSDataset(\n        transform=transform, \n        df=test,\n        files_dir=files_dir\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n    )\n        \n    for epoch in range(EPOCHS):\n        model.eval()\n        train_fn(test_loader, model, optimizer, loss_fn)\n        \n        pred_boxes, target_boxes = get_bboxes(\n            test_loader, model, iou_threshold=0.9, threshold=0.9\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.9, box_format=\"midpoint\"\n        )\n        print(f\"Test mAP: {mean_avg_prec}\")\n\n\npredictions()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.451137Z","iopub.status.idle":"2024-12-24T17:57:28.451590Z","shell.execute_reply.started":"2024-12-24T17:57:28.451342Z","shell.execute_reply":"2024-12-24T17:57:28.451364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YoloV1(split_size=7, num_boxes=2, num_classes=7).to(DEVICE)\n\noptimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n)\n\nloss_fn = YoloLoss()\n\nload_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\ntest_dataset = BTSDataset(\n        transform=transform, \n        df=test,\n        files_dir=files_dir\n    )\n\ntest_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        drop_last=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.453416Z","iopub.status.idle":"2024-12-24T17:57:28.453857Z","shell.execute_reply.started":"2024-12-24T17:57:28.453633Z","shell.execute_reply":"2024-12-24T17:57:28.453655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bboxes_images(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_images = []\n    for batch_idx, (x, labels) in enumerate(loader):\n        all_images.append(x)\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes, torch.cat(all_images, dim=0)\n\n\npred_boxes, target_boxes, images = get_bboxes_images(\n            test_loader, model, iou_threshold=0.9, threshold=0.9\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.454893Z","iopub.status.idle":"2024-12-24T17:57:28.455357Z","shell.execute_reply.started":"2024-12-24T17:57:28.455116Z","shell.execute_reply":"2024-12-24T17:57:28.455141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [{'num_imag': pred_box[0], 'class': pred_box[1], 'conf': pred_box[2], 'box': list(np.array(pred_box[3:])*448)} \\\n         for pred_box in pred_boxes]\npreds[:10:1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.457007Z","iopub.status.idle":"2024-12-24T17:57:28.457453Z","shell.execute_reply.started":"2024-12-24T17:57:28.457211Z","shell.execute_reply":"2024-12-24T17:57:28.457234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.459055Z","iopub.status.idle":"2024-12-24T17:57:28.459514Z","shell.execute_reply.started":"2024-12-24T17:57:28.459260Z","shell.execute_reply":"2024-12-24T17:57:28.459283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncolor_classes = {\n    0:'red',\n    1:\"blue\",\n    2:\"green\",\n    3:'purple',\n    4:'orange',\n    5:'yellow',\n    6:'black'\n}\n\nfig, ax = plt.subplots(5, 5, figsize = (10, 10))\nfor idx, image in enumerate(images):\n    try:\n        ax[idx//5, idx%5].imshow(image.permute((1, 2, 0)).detach().numpy())\n    except IndexError:\n        pass\nfor box in preds:\n    # Create a Rectangle patch\n    box_rect = box.get('box')\n    center_x = box_rect[0]\n    center_y = box_rect[1]\n    width = box_rect[2]\n    height = box_rect[3]\n    rect = patches.Rectangle((center_x, center_y), \n                             width, height, \n                             linewidth=1, \n                             edgecolor=color_classes.get(int(box.get(\"class\"))), \n                             facecolor='none')\n    idx_box = box.get(\"num_imag\")\n    # Add the patch to the Axes\n    try:\n        ax[idx_box//5, idx_box%5].add_patch(rect)\n    except IndexError:\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:57:28.461431Z","iopub.status.idle":"2024-12-24T17:57:28.462316Z","shell.execute_reply.started":"2024-12-24T17:57:28.462074Z","shell.execute_reply":"2024-12-24T17:57:28.462099Z"}},"outputs":[],"execution_count":null}]}