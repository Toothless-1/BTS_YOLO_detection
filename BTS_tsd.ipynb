{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3600745,"sourceType":"datasetVersion","datasetId":2159203},{"sourceId":7422059,"sourceType":"datasetVersion","datasetId":4313880},{"sourceId":10287488,"sourceType":"datasetVersion","datasetId":6366491}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs all of the libraries present in the 'offline-pytorch-2-1-2' dataset\n!pip install \\\n   --requirement /kaggle/input/offline-pytorch-2-1-2/requirements.txt \\\n   --no-index \\\n   --find-links file:///kaggle/input/offline-pytorch-2-1-2/wheels  \\\n--q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:19:50.874744Z","iopub.execute_input":"2024-12-26T19:19:50.875331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import koilerplate\nprint(f\"koilerplate: {koilerplate.__version__}\")\n\nimport torch\nprint(f\"torch: {torch.__version__}\")\n\nimport torchvision\nprint(f\"torchvision: {torchvision.__version__}\")\n\nimport torchaudio\nprint(f\"torchaudio: {torchaudio.__version__}\")\n\nimport torchdata\nprint(f\"torchdata: {torchdata.__version__}\")\n\nimport torchtext\nprint(f\"torchtext: {torchtext.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pycocotools --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport pandas as pd # data processing\nimport numpy as np\n# Data Visulization libraries \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom torchvision.io import read_image\nfrom torchvision.tv_tensors import BoundingBoxes, Image\nfrom torchvision.transforms.v2 import functional as F","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp1 = ['/'+image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))\n         and os.path.getsize(files_dir+'/'+image[:-4]+'.txt') != 0]\ntemp2 = ['/'+annot for annot in sorted(os.listdir(files_dir))\n                        if (annot[-4:]=='.txt') and os.path.getsize(files_dir+'/'+annot) != 0]\n\nimages = pd.Series(temp1, name='images')\nimage_id = pd.Series(list(range(len(temp1))), name='id')\ntrain_img_df = pd.DataFrame(pd.concat([images, image_id], axis=1))\nimages = []\nimage_id = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for j in range(len(file.readlines())):\n            images.append(temp1[i])\n            image_id.append(i)\n        file.close()\nbboxes = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for line in file.readlines():\n            bboxes.append(list(map(float, line.split())))\n        file.close()\nimages = pd.Series(images, name='images')\nbboxes = pd.Series(bboxes, name='bboxes')\nimage_id = pd.Series(image_id, name='image_id')\nind = pd.Series(list(range(len(images))), name='id')\ndf = pd.concat([images, ind,image_id,bboxes], axis=1)\ntrain_df = pd.DataFrame(df)\narea = []\nfor i in range(train_df.shape[0]):\n    img_path = files_dir + train_df.iloc[i,0]\n    img = read_image(img_path)\n    area.append(train_df.iloc[i,3][3]*train_df.iloc[i,3][4])\ntrain_df = pd.concat([train_df, pd.Series(area, name='area')],axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape, train_img_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, valid, train_img, valid_img = train_df.iloc[:int(809*0.8), :], train_df.iloc[int(809*0.8):, :], train_img_df.iloc[:int(809*0.8), :], train_img_df.iloc[int(809*0.8):, :]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 0 - Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\nclass BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, root, images_dataset, boxes_dataset, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.dataset = images_dataset\n        self.boxes_dataset = boxes_dataset\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = self.root + self.dataset.query('id == @idx').images[idx]\n        boxes_query = self.boxes_dataset.query('image_id == @idx')\n        boxes_list = list(boxes_query.bboxes.iloc[i][1:] for i in range(boxes_query.bboxes.shape[0]))\n        labels_list = list(boxes_query.bboxes.iloc[i][0] + 1 for i in range(boxes_query.bboxes.shape[0]))\n        img = read_image(img_path)\n        boxes = torch.tensor(boxes_list)\n        for box in boxes:\n            box[0], box[1], box[2], box[3] = \\\n            box[0]*F.get_size(img)[1], box[1]*F.get_size(img)[0], \\\n            box[2]*F.get_size(img)[1], box[3]*F.get_size(img)[0]\n        boxes = torchvision.ops.box_convert(boxes, 'cxcywh', 'xyxy')\n        labels = torch.tensor(labels_list, dtype=torch.int64)\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        \n        # Wrap sample and targets into torchvision tv_tensors:\n        img = Image(img)\n\n        target = {}\n        target[\"boxes\"] = BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        return img, target\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 - Modifying the model to add a different backbone","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import v2 as T\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.utils as utils\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom engine import train_one_epoch, evaluate\n\ndef my_collate(batch):\n    data = [item[0] for item in batch]\n    target = [item[1] for item in batch]\n    return [data, target]\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n# our dataset has two classes only - background and person\nnum_classes = 7+1\n# use our dataset and defined transformations\nimages_dataset = train_img\nboxes_dataset = train\ndataset = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=images_dataset, \n                     boxes_dataset=boxes_dataset, \n                     transforms=get_transform(train=True))\ndataset_test = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=valid_img, \n                     boxes_dataset=valid, \n                     transforms=get_transform(train=False))\n\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=True,\n    collate_fn=my_collate\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=my_collate\n)\n\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(\n    params,\n    lr=1e-3,\n    weight_decay = 0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=5,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    #evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.ops import nms\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084534_mp4-40_jpg.rf.796cb1b5e172d63df1c6116c2380cf19.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    x = x.to(device)\n    \n    predictions = model([x, ])\n    print(predictions)\n    pred = predictions[0]\n    print(pred)\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.1).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\nprint(pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084534_mp4-40_jpg.rf.796cb1b5e172d63df1c6116c2380cf19.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    pred['boxes'][..., [0, 2]] -= pred['boxes'][..., [2, 0]].diff(axis=1)/2\n    pred['boxes'][..., [1, 3]] -= pred['boxes'][..., [3, 1]].diff(axis=1)/2\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n#image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084920_mp4-131_jpg.rf.ff5859b06e7f5a4e5e3a57aa3b5b436d.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/BDD810C5-1B2A-4A40-91B9-B872748EABFB_mov-34_jpg.rf.86f65ad3a1d0661c9d7b0f7a077cf4cd.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}