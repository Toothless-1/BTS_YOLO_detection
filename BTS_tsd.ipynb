{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7422059,"sourceType":"datasetVersion","datasetId":4313880},{"sourceId":9300827,"sourceType":"datasetVersion","datasetId":5631547},{"sourceId":3600745,"sourceType":"datasetVersion","datasetId":2159203}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs all of the libraries present in the 'offline-pytorch-2-1-2' dataset\n!pip install \\\n   --requirement /kaggle/input/offline-pytorch-2-1-2/requirements.txt \\\n   --no-index \\\n   --find-links file:///kaggle/input/offline-pytorch-2-1-2/wheels  \\\n--q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:00:54.297921Z","iopub.execute_input":"2024-12-26T19:00:54.298329Z","iopub.status.idle":"2024-12-26T19:02:42.602695Z","shell.execute_reply.started":"2024-12-26T19:00:54.298284Z","shell.execute_reply":"2024-12-26T19:02:42.594926Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:02:42.614101Z","iopub.execute_input":"2024-12-26T19:02:42.616404Z","iopub.status.idle":"2024-12-26T19:02:44.156213Z","shell.execute_reply.started":"2024-12-26T19:02:42.616157Z","shell.execute_reply":"2024-12-26T19:02:44.154651Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import koilerplate\nprint(f\"koilerplate: {koilerplate.__version__}\")\n\nimport torch\nprint(f\"torch: {torch.__version__}\")\n\nimport torchvision\nprint(f\"torchvision: {torchvision.__version__}\")\n\nimport torchaudio\nprint(f\"torchaudio: {torchaudio.__version__}\")\n\nimport torchdata\nprint(f\"torchdata: {torchdata.__version__}\")\n\nimport torchtext\nprint(f\"torchtext: {torchtext.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:02:44.157806Z","iopub.execute_input":"2024-12-26T19:02:44.158127Z","iopub.status.idle":"2024-12-26T19:02:53.151277Z","shell.execute_reply.started":"2024-12-26T19:02:44.158100Z","shell.execute_reply":"2024-12-26T19:02:53.149662Z"}},"outputs":[{"name":"stdout","text":"koilerplate: 0.1.3\ntorch: 2.1.2+cu118\ntorchvision: 0.16.2+cu118\ntorchaudio: 2.1.2+cu118\ntorchdata: 0.7.1+cpu\ntorchtext: 0.16.2+cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install pycocotools --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-26T19:02:53.155106Z","iopub.execute_input":"2024-12-26T19:02:53.155873Z","iopub.status.idle":"2024-12-26T19:03:07.278696Z","shell.execute_reply.started":"2024-12-26T19:02:53.155835Z","shell.execute_reply":"2024-12-26T19:03:07.277128Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport pandas as pd # data processing\nimport numpy as np\n# Data Visulization libraries \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom torchvision.io import read_image\nfrom torchvision.tv_tensors import BoundingBoxes, Image\nfrom torchvision.transforms.v2 import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-12-26T19:03:07.280445Z","iopub.execute_input":"2024-12-26T19:03:07.280818Z","iopub.status.idle":"2024-12-26T19:03:08.324233Z","shell.execute_reply.started":"2024-12-26T19:03:07.280783Z","shell.execute_reply":"2024-12-26T19:03:08.323129Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:03:08.325607Z","iopub.execute_input":"2024-12-26T19:03:08.326006Z","iopub.status.idle":"2024-12-26T19:03:08.331766Z","shell.execute_reply.started":"2024-12-26T19:03:08.325966Z","shell.execute_reply":"2024-12-26T19:03:08.330542Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"temp1 = ['/'+image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))\n         and os.path.getsize(files_dir+'/'+image[:-4]+'.txt') != 0]\ntemp2 = ['/'+annot for annot in sorted(os.listdir(files_dir))\n                        if (annot[-4:]=='.txt') and os.path.getsize(files_dir+'/'+annot) != 0]\n\nimages = pd.Series(temp1, name='images')\nimage_id = pd.Series(list(range(len(temp1))), name='id')\ntrain_img_df = pd.DataFrame(pd.concat([images, image_id], axis=1))\nimages = []\nimage_id = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for j in range(len(file.readlines())):\n            images.append(temp1[i])\n            image_id.append(i)\n        file.close()\nbboxes = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for line in file.readlines():\n            bboxes.append(list(map(float, line.split())))\n        file.close()\nimages = pd.Series(images, name='images')\nbboxes = pd.Series(bboxes, name='bboxes')\nimage_id = pd.Series(image_id, name='image_id')\nind = pd.Series(list(range(len(images))), name='id')\ndf = pd.concat([images, ind,image_id,bboxes], axis=1)\ntrain_df = pd.DataFrame(df)\narea = []\nfor i in range(train_df.shape[0]):\n    img_path = files_dir + train_df.iloc[i,0]\n    img = read_image(img_path)\n    area.append(train_df.iloc[i,3][3]*train_df.iloc[i,3][4])\ntrain_df = pd.concat([train_df, pd.Series(area, name='area')],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:03:08.333261Z","iopub.execute_input":"2024-12-26T19:03:08.333642Z","iopub.status.idle":"2024-12-26T19:03:23.930133Z","shell.execute_reply.started":"2024-12-26T19:03:08.333611Z","shell.execute_reply":"2024-12-26T19:03:23.928943Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:03:23.931757Z","iopub.execute_input":"2024-12-26T19:03:23.932192Z","iopub.status.idle":"2024-12-26T19:03:23.962648Z","shell.execute_reply.started":"2024-12-26T19:03:23.932154Z","shell.execute_reply":"2024-12-26T19:03:23.961290Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          images  id  image_id                                         bboxes  \\\n0    /jhope0.png   0         0  [0.0, 0.497253, 0.289855, 0.467033, 0.543478]   \n1    /jhope1.png   1         1  [0.0, 0.477778, 0.391111, 0.848889, 0.782222]   \n2   /jhope10.png   2         2           [0.0, 0.4, 0.393333, 0.408889, 0.44]   \n3  /jhope100.png   3         3  [0.0, 0.555184, 0.446429, 0.622074, 0.892857]   \n4  /jhope101.png   4         4   [0.0, 0.52901, 0.319767, 0.361775, 0.639535]   \n\n       area  \n0  0.253822  \n1  0.664020  \n2  0.179911  \n3  0.555423  \n4  0.231368  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>images</th>\n      <th>id</th>\n      <th>image_id</th>\n      <th>bboxes</th>\n      <th>area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/jhope0.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0.0, 0.497253, 0.289855, 0.467033, 0.543478]</td>\n      <td>0.253822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/jhope1.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0.0, 0.477778, 0.391111, 0.848889, 0.782222]</td>\n      <td>0.664020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/jhope10.png</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0.0, 0.4, 0.393333, 0.408889, 0.44]</td>\n      <td>0.179911</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/jhope100.png</td>\n      <td>3</td>\n      <td>3</td>\n      <td>[0.0, 0.555184, 0.446429, 0.622074, 0.892857]</td>\n      <td>0.555423</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/jhope101.png</td>\n      <td>4</td>\n      <td>4</td>\n      <td>[0.0, 0.52901, 0.319767, 0.361775, 0.639535]</td>\n      <td>0.231368</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_df.shape, train_img_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:03:23.963981Z","iopub.execute_input":"2024-12-26T19:03:23.964352Z","iopub.status.idle":"2024-12-26T19:03:23.971216Z","shell.execute_reply.started":"2024-12-26T19:03:23.964294Z","shell.execute_reply":"2024-12-26T19:03:23.969963Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((809, 5), (808, 2))"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:04:05.194501Z","iopub.execute_input":"2024-12-26T19:04:05.194922Z","iopub.status.idle":"2024-12-26T19:04:05.513290Z","shell.execute_reply.started":"2024-12-26T19:04:05.194887Z","shell.execute_reply":"2024-12-26T19:04:05.512214Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train, valid, train_img, valid_img = train_df.iloc[:int(809*0.8), :], train_df.iloc[int(809*0.8):, :], train_img_df.iloc[:int(809*0.8), :], train_img_df.iloc[int(809*0.8):, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:12:16.092012Z","iopub.execute_input":"2024-12-26T19:12:16.092435Z","iopub.status.idle":"2024-12-26T19:12:16.100232Z","shell.execute_reply.started":"2024-12-26T19:12:16.092401Z","shell.execute_reply":"2024-12-26T19:12:16.099015Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 0 - Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\nclass BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, root, images_dataset, boxes_dataset, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.dataset = images_dataset\n        self.boxes_dataset = boxes_dataset\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = self.root + self.dataset.query('id == @idx').images[idx]\n        boxes_query = self.boxes_dataset.query('image_id == @idx')\n        boxes_list = list(boxes_query.bboxes.iloc[i][1:] for i in range(boxes_query.bboxes.shape[0]))\n        labels_list = list(boxes_query.bboxes.iloc[i][0] + 1 for i in range(boxes_query.bboxes.shape[0]))\n        img = read_image(img_path)\n        boxes = torch.tensor(boxes_list)\n        for box in boxes:\n            box[0], box[1], box[2], box[3] = \\\n            box[0]*F.get_size(img)[1], box[1]*F.get_size(img)[0], \\\n            box[2]*F.get_size(img)[1], box[3]*F.get_size(img)[0]\n        boxes = torchvision.ops.box_convert(boxes, 'cxcywh', 'xyxy')\n        labels = torch.tensor(labels_list, dtype=torch.int64)\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        \n        # Wrap sample and targets into torchvision tv_tensors:\n        img = Image(img)\n\n        target = {}\n        target[\"boxes\"] = BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        return img, target\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:12:27.483231Z","iopub.execute_input":"2024-12-26T19:12:27.484741Z","iopub.status.idle":"2024-12-26T19:12:27.498297Z","shell.execute_reply.started":"2024-12-26T19:12:27.484680Z","shell.execute_reply":"2024-12-26T19:12:27.496442Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# 2 - Modifying the model to add a different backbone","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import v2 as T\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:12:33.206342Z","iopub.execute_input":"2024-12-26T19:12:33.207699Z","iopub.status.idle":"2024-12-26T19:12:33.213633Z","shell.execute_reply.started":"2024-12-26T19:12:33.207654Z","shell.execute_reply":"2024-12-26T19:12:33.212258Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch.utils as utils\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom engine import train_one_epoch, evaluate\n\ndef my_collate(batch):\n    data = [item[0] for item in batch]\n    target = [item[1] for item in batch]\n    return [data, target]\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n# our dataset has two classes only - background and person\nnum_classes = 7+1\n# use our dataset and defined transformations\nimages_dataset = train_img\nboxes_dataset = train\ndataset = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=images_dataset, \n                     boxes_dataset=boxes_dataset, \n                     transforms=get_transform(train=True))\ndataset_test = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=valid_img, \n                     boxes_dataset=valid, \n                     transforms=get_transform(train=False))\n\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=True,\n    collate_fn=my_collate\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=my_collate\n)\n\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(\n    params,\n    lr=1e-3,\n    weight_decay = 0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=5,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    #evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:17:35.724296Z","iopub.execute_input":"2024-12-26T19:17:35.725695Z","execution_failed":"2024-12-26T19:19:14.825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.ops import nms\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084534_mp4-40_jpg.rf.796cb1b5e172d63df1c6116c2380cf19.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    x = x.to(device)\n    \n    predictions = model([x, ])\n    print(predictions)\n    pred = predictions[0]\n    print(pred)\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.1).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\nprint(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T12:35:11.867455Z","iopub.execute_input":"2024-12-26T12:35:11.8678Z","iopub.status.idle":"2024-12-26T12:35:12.002021Z","shell.execute_reply.started":"2024-12-26T12:35:11.867769Z","shell.execute_reply":"2024-12-26T12:35:12.001091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084534_mp4-40_jpg.rf.796cb1b5e172d63df1c6116c2380cf19.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    pred['boxes'][..., [0, 2]] -= pred['boxes'][..., [2, 0]].diff(axis=1)/2\n    pred['boxes'][..., [1, 3]] -= pred['boxes'][..., [3, 1]].diff(axis=1)/2\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n#image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T12:35:12.003303Z","iopub.execute_input":"2024-12-26T12:35:12.003999Z","iopub.status.idle":"2024-12-26T12:35:12.992556Z","shell.execute_reply.started":"2024-12-26T12:35:12.003966Z","shell.execute_reply":"2024-12-26T12:35:12.991623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084920_mp4-131_jpg.rf.ff5859b06e7f5a4e5e3a57aa3b5b436d.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T12:35:12.993692Z","iopub.execute_input":"2024-12-26T12:35:12.994001Z","iopub.status.idle":"2024-12-26T12:35:13.912062Z","shell.execute_reply.started":"2024-12-26T12:35:12.993973Z","shell.execute_reply":"2024-12-26T12:35:13.911233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/BDD810C5-1B2A-4A40-91B9-B872748EABFB_mov-34_jpg.rf.86f65ad3a1d0661c9d7b0f7a077cf4cd.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T12:35:13.91323Z","iopub.execute_input":"2024-12-26T12:35:13.913542Z","iopub.status.idle":"2024-12-26T12:35:14.710012Z","shell.execute_reply.started":"2024-12-26T12:35:13.913514Z","shell.execute_reply":"2024-12-26T12:35:14.709172Z"}},"outputs":[],"execution_count":null}]}