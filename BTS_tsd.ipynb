{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3600745,"sourceType":"datasetVersion","datasetId":2159203},{"sourceId":7422059,"sourceType":"datasetVersion","datasetId":4313880},{"sourceId":10287488,"sourceType":"datasetVersion","datasetId":6366491}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs all of the libraries present in the 'offline-pytorch-2-1-2' dataset\n!pip install \\\n   --requirement /kaggle/input/offline-pytorch-2-1-2/requirements.txt \\\n   --no-index \\\n   --find-links file:///kaggle/input/offline-pytorch-2-1-2/wheels  \\\n--q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:35:26.940469Z","iopub.execute_input":"2024-12-26T19:35:26.940756Z","iopub.status.idle":"2024-12-26T19:36:52.436881Z","shell.execute_reply.started":"2024-12-26T19:35:26.940733Z","shell.execute_reply":"2024-12-26T19:36:52.435918Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:36:52.438625Z","iopub.execute_input":"2024-12-26T19:36:52.438906Z","iopub.status.idle":"2024-12-26T19:36:53.338166Z","shell.execute_reply.started":"2024-12-26T19:36:52.438884Z","shell.execute_reply":"2024-12-26T19:36:53.337302Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import koilerplate\nprint(f\"koilerplate: {koilerplate.__version__}\")\n\nimport torch\nprint(f\"torch: {torch.__version__}\")\n\nimport torchvision\nprint(f\"torchvision: {torchvision.__version__}\")\n\nimport torchaudio\nprint(f\"torchaudio: {torchaudio.__version__}\")\n\nimport torchdata\nprint(f\"torchdata: {torchdata.__version__}\")\n\nimport torchtext\nprint(f\"torchtext: {torchtext.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:36:53.339372Z","iopub.execute_input":"2024-12-26T19:36:53.339632Z","iopub.status.idle":"2024-12-26T19:36:58.679355Z","shell.execute_reply.started":"2024-12-26T19:36:53.339610Z","shell.execute_reply":"2024-12-26T19:36:58.678594Z"}},"outputs":[{"name":"stdout","text":"koilerplate: 0.1.3\ntorch: 2.1.2+cu118\ntorchvision: 0.16.2+cu118\ntorchaudio: 2.1.2+cu118\ntorchdata: 0.7.1+cpu\ntorchtext: 0.16.2+cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install pycocotools --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-26T19:36:58.681061Z","iopub.execute_input":"2024-12-26T19:36:58.681436Z","iopub.status.idle":"2024-12-26T19:37:09.843590Z","shell.execute_reply.started":"2024-12-26T19:36:58.681412Z","shell.execute_reply":"2024-12-26T19:37:09.842541Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport pandas as pd # data processing\nimport numpy as np\n# Data Visulization libraries \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom torchvision.io import read_image\nfrom torchvision.tv_tensors import BoundingBoxes, Image\nfrom torchvision.transforms.v2 import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-12-26T19:37:09.845023Z","iopub.execute_input":"2024-12-26T19:37:09.845306Z","iopub.status.idle":"2024-12-26T19:37:10.240685Z","shell.execute_reply.started":"2024-12-26T19:37:09.845281Z","shell.execute_reply":"2024-12-26T19:37:10.239816Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:10.241818Z","iopub.execute_input":"2024-12-26T19:37:10.242048Z","iopub.status.idle":"2024-12-26T19:37:10.245590Z","shell.execute_reply.started":"2024-12-26T19:37:10.242029Z","shell.execute_reply":"2024-12-26T19:37:10.244789Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"temp1 = ['/'+image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))\n         and os.path.getsize(files_dir+'/'+image[:-4]+'.txt') != 0]\ntemp2 = ['/'+annot for annot in sorted(os.listdir(files_dir))\n                        if (annot[-4:]=='.txt') and os.path.getsize(files_dir+'/'+annot) != 0]\n\nimages = pd.Series(temp1, name='images')\nimage_id = pd.Series(list(range(len(temp1))), name='id')\ntrain_img_df = pd.DataFrame(pd.concat([images, image_id], axis=1))\nimages = []\nimage_id = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for j in range(len(file.readlines())):\n            images.append(temp1[i])\n            image_id.append(i)\n        file.close()\nbboxes = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for line in file.readlines():\n            bboxes.append(list(map(float, line.split())))\n        file.close()\nimages = pd.Series(images, name='images')\nbboxes = pd.Series(bboxes, name='bboxes')\nimage_id = pd.Series(image_id, name='image_id')\nind = pd.Series(list(range(len(images))), name='id')\ndf = pd.concat([images, ind,image_id,bboxes], axis=1)\ntrain_df = pd.DataFrame(df)\narea = []\nfor i in range(train_df.shape[0]):\n    img_path = files_dir + train_df.iloc[i,0]\n    img = read_image(img_path)\n    area.append(train_df.iloc[i,3][3]*train_df.iloc[i,3][4])\ntrain_df = pd.concat([train_df, pd.Series(area, name='area')],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:10.246893Z","iopub.execute_input":"2024-12-26T19:37:10.247169Z","iopub.status.idle":"2024-12-26T19:37:17.258083Z","shell.execute_reply.started":"2024-12-26T19:37:10.247147Z","shell.execute_reply":"2024-12-26T19:37:17.257358Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.259054Z","iopub.execute_input":"2024-12-26T19:37:17.259271Z","iopub.status.idle":"2024-12-26T19:37:17.278094Z","shell.execute_reply.started":"2024-12-26T19:37:17.259252Z","shell.execute_reply":"2024-12-26T19:37:17.277459Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          images  id  image_id                                         bboxes  \\\n0    /jhope0.png   0         0  [0.0, 0.497253, 0.289855, 0.467033, 0.543478]   \n1    /jhope1.png   1         1  [0.0, 0.477778, 0.391111, 0.848889, 0.782222]   \n2   /jhope10.png   2         2           [0.0, 0.4, 0.393333, 0.408889, 0.44]   \n3  /jhope100.png   3         3  [0.0, 0.555184, 0.446429, 0.622074, 0.892857]   \n4  /jhope101.png   4         4   [0.0, 0.52901, 0.319767, 0.361775, 0.639535]   \n\n       area  \n0  0.253822  \n1  0.664020  \n2  0.179911  \n3  0.555423  \n4  0.231368  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>images</th>\n      <th>id</th>\n      <th>image_id</th>\n      <th>bboxes</th>\n      <th>area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/jhope0.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0.0, 0.497253, 0.289855, 0.467033, 0.543478]</td>\n      <td>0.253822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/jhope1.png</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0.0, 0.477778, 0.391111, 0.848889, 0.782222]</td>\n      <td>0.664020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/jhope10.png</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0.0, 0.4, 0.393333, 0.408889, 0.44]</td>\n      <td>0.179911</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/jhope100.png</td>\n      <td>3</td>\n      <td>3</td>\n      <td>[0.0, 0.555184, 0.446429, 0.622074, 0.892857]</td>\n      <td>0.555423</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/jhope101.png</td>\n      <td>4</td>\n      <td>4</td>\n      <td>[0.0, 0.52901, 0.319767, 0.361775, 0.639535]</td>\n      <td>0.231368</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_df.shape, train_img_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.279156Z","iopub.execute_input":"2024-12-26T19:37:17.279399Z","iopub.status.idle":"2024-12-26T19:37:17.284854Z","shell.execute_reply.started":"2024-12-26T19:37:17.279371Z","shell.execute_reply":"2024-12-26T19:37:17.283994Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((809, 5), (808, 2))"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.287942Z","iopub.execute_input":"2024-12-26T19:37:17.288323Z","iopub.status.idle":"2024-12-26T19:37:17.426322Z","shell.execute_reply.started":"2024-12-26T19:37:17.288301Z","shell.execute_reply":"2024-12-26T19:37:17.425429Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#train, valid, train_img, valid_img = train_df.iloc[:int(809*0.6), :], train_df.iloc[int(809*0.6):, :], train_img_df.iloc[:int(809*0.6), :], train_img_df.iloc[int(809*0.6):, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.427703Z","iopub.execute_input":"2024-12-26T19:37:17.428472Z","iopub.status.idle":"2024-12-26T19:37:17.431879Z","shell.execute_reply.started":"2024-12-26T19:37:17.428420Z","shell.execute_reply":"2024-12-26T19:37:17.431091Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# 0 - Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\nclass BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, root, images_dataset, boxes_dataset, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.dataset = images_dataset\n        self.boxes_dataset = boxes_dataset\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = self.root + self.dataset.query('id == @idx').images[idx]\n        boxes_query = self.boxes_dataset.query('image_id == @idx')\n        boxes_list = list(boxes_query.bboxes.iloc[i][1:] for i in range(boxes_query.bboxes.shape[0]))\n        labels_list = list(boxes_query.bboxes.iloc[i][0] + 1 for i in range(boxes_query.bboxes.shape[0]))\n        img = read_image(img_path)\n        boxes = torch.tensor(boxes_list)\n        #print(img_path)\n        for box in boxes:\n            box[0], box[1], box[2], box[3] = \\\n            box[0]*F.get_size(img)[1], box[1]*F.get_size(img)[0], \\\n            box[2]*F.get_size(img)[1], box[3]*F.get_size(img)[0]\n        boxes = torchvision.ops.box_convert(boxes, 'cxcywh', 'xyxy')\n        labels = torch.tensor(labels_list, dtype=torch.int64)\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        \n        # Wrap sample and targets into torchvision tv_tensors:\n        img = Image(img)\n\n        target = {}\n        target[\"boxes\"] = BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        return img, target\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.433003Z","iopub.execute_input":"2024-12-26T19:37:17.433297Z","iopub.status.idle":"2024-12-26T19:37:17.445068Z","shell.execute_reply.started":"2024-12-26T19:37:17.433269Z","shell.execute_reply":"2024-12-26T19:37:17.444363Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# 2 - Modifying the model to add a different backbone","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import v2 as T\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.445936Z","iopub.execute_input":"2024-12-26T19:37:17.446141Z","iopub.status.idle":"2024-12-26T19:37:17.460841Z","shell.execute_reply.started":"2024-12-26T19:37:17.446113Z","shell.execute_reply":"2024-12-26T19:37:17.460174Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch.utils as utils\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom engine import train_one_epoch, evaluate\n\ndef my_collate(batch):\n    data = [item[0] for item in batch]\n    target = [item[1] for item in batch]\n    return [data, target]\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n# our dataset has two classes only - background and person\nnum_classes = 7+1\n# use our dataset and defined transformations\nimages_dataset = train_img_df\nboxes_dataset = train_df\ndataset = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=images_dataset, \n                     boxes_dataset=boxes_dataset, \n                     transforms=get_transform(train=True))\n'''dataset_test = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=valid_img, \n                     boxes_dataset=valid, \n                     transforms=get_transform(train=False))'''\n\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=True,\n    collate_fn=my_collate\n)\n\n'''data_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=my_collate\n)'''\n\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(\n    params,\n    lr=1e-3,\n    weight_decay = 0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=5,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    #evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:37:17.461831Z","iopub.execute_input":"2024-12-26T19:37:17.462049Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n100%|██████████| 74.2M/74.2M [00:00<00:00, 186MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [0]  [ 0/51]  eta: 0:01:13  lr: 0.000021  loss: 1.2288 (1.2288)  loss_classifier: 0.8573 (0.8573)  loss_box_reg: 0.2302 (0.2302)  loss_objectness: 0.0620 (0.0620)  loss_rpn_box_reg: 0.0793 (0.0793)  time: 1.4508  data: 0.0803  max mem: 8188\nEpoch: [0]  [50/51]  eta: 0:00:00  lr: 0.001000  loss: 0.8961 (1.0398)  loss_classifier: 0.5814 (0.6770)  loss_box_reg: 0.2678 (0.3175)  loss_objectness: 0.0084 (0.0186)  loss_rpn_box_reg: 0.0198 (0.0266)  time: 0.7437  data: 0.0783  max mem: 8519\nEpoch: [0] Total time: 0:00:39 (0.7765 s / it)\nEpoch: [1]  [ 0/51]  eta: 0:00:39  lr: 0.001000  loss: 0.9335 (0.9335)  loss_classifier: 0.5757 (0.5757)  loss_box_reg: 0.3314 (0.3314)  loss_objectness: 0.0097 (0.0097)  loss_rpn_box_reg: 0.0166 (0.0166)  time: 0.7678  data: 0.0840  max mem: 8519\nEpoch: [1]  [50/51]  eta: 0:00:00  lr: 0.001000  loss: 0.7409 (0.8041)  loss_classifier: 0.4439 (0.4943)  loss_box_reg: 0.2282 (0.2536)  loss_objectness: 0.0106 (0.0225)  loss_rpn_box_reg: 0.0330 (0.0337)  time: 0.7534  data: 0.0740  max mem: 8521\nEpoch: [1] Total time: 0:00:39 (0.7713 s / it)\nEpoch: [2]  [ 0/51]  eta: 0:00:38  lr: 0.001000  loss: 0.7176 (0.7176)  loss_classifier: 0.5116 (0.5116)  loss_box_reg: 0.1827 (0.1827)  loss_objectness: 0.0038 (0.0038)  loss_rpn_box_reg: 0.0195 (0.0195)  time: 0.7647  data: 0.0751  max mem: 8521\nEpoch: [2]  [50/51]  eta: 0:00:00  lr: 0.001000  loss: 0.7467 (0.8775)  loss_classifier: 0.4715 (0.5691)  loss_box_reg: 0.2349 (0.2778)  loss_objectness: 0.0050 (0.0062)  loss_rpn_box_reg: 0.0176 (0.0244)  time: 0.7206  data: 0.0745  max mem: 8521\nEpoch: [2] Total time: 0:00:37 (0.7347 s / it)\nEpoch: [3]  [ 0/51]  eta: 0:00:36  lr: 0.001000  loss: 0.9988 (0.9988)  loss_classifier: 0.6815 (0.6815)  loss_box_reg: 0.2935 (0.2935)  loss_objectness: 0.0047 (0.0047)  loss_rpn_box_reg: 0.0191 (0.0191)  time: 0.7231  data: 0.0680  max mem: 8521\nEpoch: [3]  [50/51]  eta: 0:00:00  lr: 0.001000  loss: 0.9337 (0.9257)  loss_classifier: 0.6347 (0.6107)  loss_box_reg: 0.2766 (0.2903)  loss_objectness: 0.0031 (0.0042)  loss_rpn_box_reg: 0.0186 (0.0205)  time: 0.7071  data: 0.0741  max mem: 8521\nEpoch: [3] Total time: 0:00:36 (0.7204 s / it)\nEpoch: [4]  [ 0/51]  eta: 0:00:37  lr: 0.001000  loss: 0.9865 (0.9865)  loss_classifier: 0.6595 (0.6595)  loss_box_reg: 0.2990 (0.2990)  loss_objectness: 0.0026 (0.0026)  loss_rpn_box_reg: 0.0254 (0.0254)  time: 0.7391  data: 0.0711  max mem: 8521\nEpoch: [4]  [50/51]  eta: 0:00:00  lr: 0.001000  loss: 0.9071 (0.8038)  loss_classifier: 0.5938 (0.5215)  loss_box_reg: 0.2874 (0.2578)  loss_objectness: 0.0035 (0.0042)  loss_rpn_box_reg: 0.0200 (0.0203)  time: 0.7100  data: 0.0711  max mem: 8521\nEpoch: [4] Total time: 0:00:37 (0.7295 s / it)\nEpoch: [5]  [ 0/51]  eta: 0:00:36  lr: 0.000100  loss: 1.4142 (1.4142)  loss_classifier: 0.8856 (0.8856)  loss_box_reg: 0.5003 (0.5003)  loss_objectness: 0.0030 (0.0030)  loss_rpn_box_reg: 0.0253 (0.0253)  time: 0.7146  data: 0.0696  max mem: 8521\nEpoch: [5]  [50/51]  eta: 0:00:00  lr: 0.000100  loss: 1.0164 (1.0850)  loss_classifier: 0.6565 (0.7127)  loss_box_reg: 0.3380 (0.3542)  loss_objectness: 0.0014 (0.0026)  loss_rpn_box_reg: 0.0100 (0.0155)  time: 0.6721  data: 0.0697  max mem: 8521\nEpoch: [5] Total time: 0:00:35 (0.6893 s / it)\nEpoch: [6]  [ 0/51]  eta: 0:00:34  lr: 0.000100  loss: 1.0131 (1.0131)  loss_classifier: 0.6078 (0.6078)  loss_box_reg: 0.3952 (0.3952)  loss_objectness: 0.0040 (0.0040)  loss_rpn_box_reg: 0.0061 (0.0061)  time: 0.6689  data: 0.0682  max mem: 8521\nEpoch: [6]  [50/51]  eta: 0:00:00  lr: 0.000100  loss: 0.8839 (0.9081)  loss_classifier: 0.5245 (0.5689)  loss_box_reg: 0.3354 (0.3235)  loss_objectness: 0.0016 (0.0022)  loss_rpn_box_reg: 0.0123 (0.0135)  time: 0.6825  data: 0.0693  max mem: 8521\nEpoch: [6] Total time: 0:00:35 (0.6918 s / it)\nEpoch: [7]  [ 0/51]  eta: 0:00:36  lr: 0.000100  loss: 0.6848 (0.6848)  loss_classifier: 0.4007 (0.4007)  loss_box_reg: 0.2718 (0.2718)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0106 (0.0106)  time: 0.7216  data: 0.0774  max mem: 8521\nEpoch: [7]  [50/51]  eta: 0:00:00  lr: 0.000100  loss: 0.8248 (0.7762)  loss_classifier: 0.4902 (0.4626)  loss_box_reg: 0.3130 (0.2988)  loss_objectness: 0.0016 (0.0017)  loss_rpn_box_reg: 0.0126 (0.0131)  time: 0.6735  data: 0.0716  max mem: 8521\nEpoch: [7] Total time: 0:00:35 (0.6902 s / it)\nEpoch: [8]  [ 0/51]  eta: 0:00:35  lr: 0.000100  loss: 0.6209 (0.6209)  loss_classifier: 0.3615 (0.3615)  loss_box_reg: 0.2331 (0.2331)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0253 (0.0253)  time: 0.6944  data: 0.0776  max mem: 8521\nEpoch: [8]  [50/51]  eta: 0:00:00  lr: 0.000100  loss: 0.6274 (0.6409)  loss_classifier: 0.3099 (0.3451)  loss_box_reg: 0.2666 (0.2817)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0089 (0.0123)  time: 0.6753  data: 0.0683  max mem: 8521\nEpoch: [8] Total time: 0:00:34 (0.6855 s / it)\nEpoch: [9]  [ 0/51]  eta: 0:00:33  lr: 0.000100  loss: 0.4148 (0.4148)  loss_classifier: 0.2350 (0.2350)  loss_box_reg: 0.1700 (0.1700)  loss_objectness: 0.0009 (0.0009)  loss_rpn_box_reg: 0.0089 (0.0089)  time: 0.6604  data: 0.0748  max mem: 8521\nEpoch: [9]  [50/51]  eta: 0:00:00  lr: 0.000100  loss: 0.5281 (0.5284)  loss_classifier: 0.2461 (0.2592)  loss_box_reg: 0.2631 (0.2559)  loss_objectness: 0.0011 (0.0014)  loss_rpn_box_reg: 0.0102 (0.0119)  time: 0.6776  data: 0.0684  max mem: 8521\nEpoch: [9] Total time: 0:00:35 (0.6902 s / it)\nEpoch: [10]  [ 0/51]  eta: 0:00:33  lr: 0.000010  loss: 0.4106 (0.4106)  loss_classifier: 0.1965 (0.1965)  loss_box_reg: 0.2045 (0.2045)  loss_objectness: 0.0013 (0.0013)  loss_rpn_box_reg: 0.0083 (0.0083)  time: 0.6577  data: 0.0734  max mem: 8521\nEpoch: [10]  [50/51]  eta: 0:00:00  lr: 0.000010  loss: 0.3924 (0.4047)  loss_classifier: 0.1741 (0.1832)  loss_box_reg: 0.2135 (0.2090)  loss_objectness: 0.0009 (0.0013)  loss_rpn_box_reg: 0.0078 (0.0112)  time: 0.6886  data: 0.0841  max mem: 8521\nEpoch: [10] Total time: 0:00:35 (0.6945 s / it)\nEpoch: [11]  [ 0/51]  eta: 0:00:36  lr: 0.000010  loss: 0.3977 (0.3977)  loss_classifier: 0.1769 (0.1769)  loss_box_reg: 0.2088 (0.2088)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0100 (0.0100)  time: 0.7205  data: 0.0729  max mem: 8521\nEpoch: [11]  [50/51]  eta: 0:00:00  lr: 0.000010  loss: 0.3505 (0.3773)  loss_classifier: 0.1516 (0.1659)  loss_box_reg: 0.1858 (0.1989)  loss_objectness: 0.0012 (0.0014)  loss_rpn_box_reg: 0.0096 (0.0110)  time: 0.6777  data: 0.0726  max mem: 8521\nEpoch: [11] Total time: 0:00:35 (0.6865 s / it)\nEpoch: [12]  [ 0/51]  eta: 0:00:36  lr: 0.000010  loss: 0.3613 (0.3613)  loss_classifier: 0.1519 (0.1519)  loss_box_reg: 0.1939 (0.1939)  loss_objectness: 0.0010 (0.0010)  loss_rpn_box_reg: 0.0145 (0.0145)  time: 0.7063  data: 0.0718  max mem: 8521\nEpoch: [12]  [50/51]  eta: 0:00:00  lr: 0.000010  loss: 0.3759 (0.3743)  loss_classifier: 0.1575 (0.1628)  loss_box_reg: 0.1948 (0.1992)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0091 (0.0109)  time: 0.6857  data: 0.0700  max mem: 8521\nEpoch: [12] Total time: 0:00:34 (0.6830 s / it)\nEpoch: [13]  [ 0/51]  eta: 0:00:35  lr: 0.000010  loss: 0.4087 (0.4087)  loss_classifier: 0.1829 (0.1829)  loss_box_reg: 0.2128 (0.2128)  loss_objectness: 0.0020 (0.0020)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.6977  data: 0.0650  max mem: 8521\nEpoch: [13]  [50/51]  eta: 0:00:00  lr: 0.000010  loss: 0.3404 (0.3606)  loss_classifier: 0.1471 (0.1538)  loss_box_reg: 0.1908 (0.1943)  loss_objectness: 0.0013 (0.0015)  loss_rpn_box_reg: 0.0087 (0.0110)  time: 0.6676  data: 0.0677  max mem: 8521\nEpoch: [13] Total time: 0:00:34 (0.6829 s / it)\nEpoch: [14]  [ 0/51]  eta: 0:00:35  lr: 0.000010  loss: 0.3982 (0.3982)  loss_classifier: 0.1725 (0.1725)  loss_box_reg: 0.2046 (0.2046)  loss_objectness: 0.0017 (0.0017)  loss_rpn_box_reg: 0.0194 (0.0194)  time: 0.6998  data: 0.0680  max mem: 8521\nEpoch: [14]  [50/51]  eta: 0:00:00  lr: 0.000010  loss: 0.3364 (0.3485)  loss_classifier: 0.1455 (0.1475)  loss_box_reg: 0.1829 (0.1889)  loss_objectness: 0.0011 (0.0014)  loss_rpn_box_reg: 0.0076 (0.0108)  time: 0.6761  data: 0.0685  max mem: 8521\nEpoch: [14] Total time: 0:00:34 (0.6837 s / it)\nEpoch: [15]  [ 0/51]  eta: 0:00:35  lr: 0.000001  loss: 0.3495 (0.3495)  loss_classifier: 0.1612 (0.1612)  loss_box_reg: 0.1590 (0.1590)  loss_objectness: 0.0013 (0.0013)  loss_rpn_box_reg: 0.0279 (0.0279)  time: 0.7057  data: 0.0735  max mem: 8521\nEpoch: [15]  [50/51]  eta: 0:00:00  lr: 0.000001  loss: 0.3504 (0.3404)  loss_classifier: 0.1426 (0.1423)  loss_box_reg: 0.1873 (0.1860)  loss_objectness: 0.0011 (0.0013)  loss_rpn_box_reg: 0.0096 (0.0108)  time: 0.6714  data: 0.0697  max mem: 8521\nEpoch: [15] Total time: 0:00:34 (0.6843 s / it)\nEpoch: [16]  [ 0/51]  eta: 0:00:35  lr: 0.000001  loss: 0.3381 (0.3381)  loss_classifier: 0.1236 (0.1236)  loss_box_reg: 0.1975 (0.1975)  loss_objectness: 0.0008 (0.0008)  loss_rpn_box_reg: 0.0162 (0.0162)  time: 0.6964  data: 0.0658  max mem: 8521\nEpoch: [16]  [50/51]  eta: 0:00:00  lr: 0.000001  loss: 0.3294 (0.3351)  loss_classifier: 0.1273 (0.1398)  loss_box_reg: 0.1723 (0.1831)  loss_objectness: 0.0014 (0.0013)  loss_rpn_box_reg: 0.0089 (0.0108)  time: 0.6734  data: 0.0674  max mem: 8521\nEpoch: [16] Total time: 0:00:34 (0.6838 s / it)\nEpoch: [17]  [ 0/51]  eta: 0:00:35  lr: 0.000001  loss: 0.2983 (0.2983)  loss_classifier: 0.1350 (0.1350)  loss_box_reg: 0.1507 (0.1507)  loss_objectness: 0.0012 (0.0012)  loss_rpn_box_reg: 0.0114 (0.0114)  time: 0.6954  data: 0.0650  max mem: 8521\nEpoch: [17]  [50/51]  eta: 0:00:00  lr: 0.000001  loss: 0.3149 (0.3343)  loss_classifier: 0.1330 (0.1391)  loss_box_reg: 0.1708 (0.1830)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0089 (0.0107)  time: 0.6675  data: 0.0689  max mem: 8521\nEpoch: [17] Total time: 0:00:34 (0.6815 s / it)\nEpoch: [18]  [ 0/51]  eta: 0:00:36  lr: 0.000001  loss: 0.4117 (0.4117)  loss_classifier: 0.1836 (0.1836)  loss_box_reg: 0.2092 (0.2092)  loss_objectness: 0.0012 (0.0012)  loss_rpn_box_reg: 0.0177 (0.0177)  time: 0.7090  data: 0.0703  max mem: 8521\nEpoch: [18]  [50/51]  eta: 0:00:00  lr: 0.000001  loss: 0.3356 (0.3363)  loss_classifier: 0.1368 (0.1401)  loss_box_reg: 0.1924 (0.1842)  loss_objectness: 0.0011 (0.0013)  loss_rpn_box_reg: 0.0102 (0.0108)  time: 0.6658  data: 0.0685  max mem: 8521\nEpoch: [18] Total time: 0:00:34 (0.6832 s / it)\nEpoch: [19]  [ 0/51]  eta: 0:00:35  lr: 0.000001  loss: 0.3285 (0.3285)  loss_classifier: 0.1362 (0.1362)  loss_box_reg: 0.1839 (0.1839)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0065 (0.0065)  time: 0.7029  data: 0.0667  max mem: 8521\nEpoch: [19]  [50/51]  eta: 0:00:00  lr: 0.000001  loss: 0.3244 (0.3336)  loss_classifier: 0.1328 (0.1390)  loss_box_reg: 0.1689 (0.1824)  loss_objectness: 0.0011 (0.0014)  loss_rpn_box_reg: 0.0096 (0.0108)  time: 0.6672  data: 0.0701  max mem: 8521\nEpoch: [19] Total time: 0:00:34 (0.6838 s / it)\nEpoch: [20]  [ 0/51]  eta: 0:00:34  lr: 0.000000  loss: 0.3482 (0.3482)  loss_classifier: 0.1533 (0.1533)  loss_box_reg: 0.1861 (0.1861)  loss_objectness: 0.0016 (0.0016)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 0.6765  data: 0.0899  max mem: 8521\nEpoch: [20]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.2960 (0.3316)  loss_classifier: 0.1257 (0.1382)  loss_box_reg: 0.1607 (0.1812)  loss_objectness: 0.0013 (0.0014)  loss_rpn_box_reg: 0.0081 (0.0108)  time: 0.6710  data: 0.0687  max mem: 8521\nEpoch: [20] Total time: 0:00:34 (0.6834 s / it)\nEpoch: [21]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.3042 (0.3042)  loss_classifier: 0.1206 (0.1206)  loss_box_reg: 0.1757 (0.1757)  loss_objectness: 0.0008 (0.0008)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 0.7011  data: 0.0712  max mem: 8521\nEpoch: [21]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3266 (0.3315)  loss_classifier: 0.1386 (0.1377)  loss_box_reg: 0.1722 (0.1817)  loss_objectness: 0.0013 (0.0014)  loss_rpn_box_reg: 0.0090 (0.0107)  time: 0.6762  data: 0.0700  max mem: 8521\nEpoch: [21] Total time: 0:00:34 (0.6824 s / it)\nEpoch: [22]  [ 0/51]  eta: 0:00:33  lr: 0.000000  loss: 0.2680 (0.2680)  loss_classifier: 0.1049 (0.1049)  loss_box_reg: 0.1429 (0.1429)  loss_objectness: 0.0017 (0.0017)  loss_rpn_box_reg: 0.0185 (0.0185)  time: 0.6517  data: 0.0654  max mem: 8521\nEpoch: [22]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3512 (0.3306)  loss_classifier: 0.1387 (0.1375)  loss_box_reg: 0.1858 (0.1809)  loss_objectness: 0.0012 (0.0014)  loss_rpn_box_reg: 0.0085 (0.0108)  time: 0.6681  data: 0.0735  max mem: 8521\nEpoch: [22] Total time: 0:00:34 (0.6851 s / it)\nEpoch: [23]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.3033 (0.3033)  loss_classifier: 0.1283 (0.1283)  loss_box_reg: 0.1659 (0.1659)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0085 (0.0085)  time: 0.7034  data: 0.0729  max mem: 8521\nEpoch: [23]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3306 (0.3314)  loss_classifier: 0.1288 (0.1376)  loss_box_reg: 0.1814 (0.1817)  loss_objectness: 0.0010 (0.0013)  loss_rpn_box_reg: 0.0106 (0.0108)  time: 0.6693  data: 0.0678  max mem: 8521\nEpoch: [23] Total time: 0:00:34 (0.6847 s / it)\nEpoch: [24]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.2898 (0.2898)  loss_classifier: 0.1060 (0.1060)  loss_box_reg: 0.1750 (0.1750)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0072 (0.0072)  time: 0.6992  data: 0.0683  max mem: 8521\nEpoch: [24]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3263 (0.3300)  loss_classifier: 0.1371 (0.1372)  loss_box_reg: 0.1716 (0.1806)  loss_objectness: 0.0012 (0.0014)  loss_rpn_box_reg: 0.0100 (0.0108)  time: 0.6755  data: 0.0730  max mem: 8521\nEpoch: [24] Total time: 0:00:35 (0.6869 s / it)\nEpoch: [25]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.4187 (0.4187)  loss_classifier: 0.1726 (0.1726)  loss_box_reg: 0.2304 (0.2304)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0145 (0.0145)  time: 0.6942  data: 0.0670  max mem: 8521\nEpoch: [25]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3132 (0.3314)  loss_classifier: 0.1239 (0.1373)  loss_box_reg: 0.1744 (0.1818)  loss_objectness: 0.0012 (0.0014)  loss_rpn_box_reg: 0.0093 (0.0110)  time: 0.6686  data: 0.0678  max mem: 8521\nEpoch: [25] Total time: 0:00:34 (0.6806 s / it)\nEpoch: [26]  [ 0/51]  eta: 0:00:33  lr: 0.000000  loss: 0.2826 (0.2826)  loss_classifier: 0.1421 (0.1421)  loss_box_reg: 0.1248 (0.1248)  loss_objectness: 0.0027 (0.0027)  loss_rpn_box_reg: 0.0130 (0.0130)  time: 0.6593  data: 0.0729  max mem: 8521\nEpoch: [26]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3168 (0.3294)  loss_classifier: 0.1311 (0.1369)  loss_box_reg: 0.1698 (0.1803)  loss_objectness: 0.0013 (0.0014)  loss_rpn_box_reg: 0.0099 (0.0107)  time: 0.6683  data: 0.0674  max mem: 8521\nEpoch: [26] Total time: 0:00:34 (0.6836 s / it)\nEpoch: [27]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.2892 (0.2892)  loss_classifier: 0.1287 (0.1287)  loss_box_reg: 0.1471 (0.1471)  loss_objectness: 0.0009 (0.0009)  loss_rpn_box_reg: 0.0124 (0.0124)  time: 0.6996  data: 0.0688  max mem: 8521\nEpoch: [27]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3206 (0.3309)  loss_classifier: 0.1318 (0.1376)  loss_box_reg: 0.1825 (0.1811)  loss_objectness: 0.0012 (0.0014)  loss_rpn_box_reg: 0.0076 (0.0107)  time: 0.6683  data: 0.0675  max mem: 8521\nEpoch: [27] Total time: 0:00:34 (0.6812 s / it)\nEpoch: [28]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.2623 (0.2623)  loss_classifier: 0.1060 (0.1060)  loss_box_reg: 0.1425 (0.1425)  loss_objectness: 0.0010 (0.0010)  loss_rpn_box_reg: 0.0128 (0.0128)  time: 0.7025  data: 0.0686  max mem: 8521\nEpoch: [28]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3119 (0.3319)  loss_classifier: 0.1333 (0.1381)  loss_box_reg: 0.1653 (0.1814)  loss_objectness: 0.0011 (0.0015)  loss_rpn_box_reg: 0.0089 (0.0110)  time: 0.6731  data: 0.0694  max mem: 8521\nEpoch: [28] Total time: 0:00:34 (0.6849 s / it)\nEpoch: [29]  [ 0/51]  eta: 0:00:33  lr: 0.000000  loss: 0.2584 (0.2584)  loss_classifier: 0.1040 (0.1040)  loss_box_reg: 0.1449 (0.1449)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 0.6554  data: 0.0665  max mem: 8521\nEpoch: [29]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3184 (0.3300)  loss_classifier: 0.1399 (0.1375)  loss_box_reg: 0.1664 (0.1803)  loss_objectness: 0.0012 (0.0015)  loss_rpn_box_reg: 0.0088 (0.0108)  time: 0.6755  data: 0.0701  max mem: 8521\nEpoch: [29] Total time: 0:00:34 (0.6821 s / it)\nEpoch: [30]  [ 0/51]  eta: 0:00:36  lr: 0.000000  loss: 0.3523 (0.3523)  loss_classifier: 0.1471 (0.1471)  loss_box_reg: 0.1955 (0.1955)  loss_objectness: 0.0017 (0.0017)  loss_rpn_box_reg: 0.0080 (0.0080)  time: 0.7078  data: 0.0747  max mem: 8521\nEpoch: [30]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3325 (0.3315)  loss_classifier: 0.1313 (0.1378)  loss_box_reg: 0.1772 (0.1812)  loss_objectness: 0.0015 (0.0015)  loss_rpn_box_reg: 0.0089 (0.0110)  time: 0.6676  data: 0.0691  max mem: 8521\nEpoch: [30] Total time: 0:00:34 (0.6824 s / it)\nEpoch: [31]  [ 0/51]  eta: 0:00:35  lr: 0.000000  loss: 0.3021 (0.3021)  loss_classifier: 0.1225 (0.1225)  loss_box_reg: 0.1711 (0.1711)  loss_objectness: 0.0025 (0.0025)  loss_rpn_box_reg: 0.0060 (0.0060)  time: 0.6981  data: 0.0660  max mem: 8521\nEpoch: [31]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3385 (0.3315)  loss_classifier: 0.1295 (0.1377)  loss_box_reg: 0.1814 (0.1818)  loss_objectness: 0.0010 (0.0012)  loss_rpn_box_reg: 0.0094 (0.0108)  time: 0.6668  data: 0.0707  max mem: 8521\nEpoch: [31] Total time: 0:00:34 (0.6792 s / it)\nEpoch: [32]  [ 0/51]  eta: 0:00:33  lr: 0.000000  loss: 0.2669 (0.2669)  loss_classifier: 0.1105 (0.1105)  loss_box_reg: 0.1403 (0.1403)  loss_objectness: 0.0012 (0.0012)  loss_rpn_box_reg: 0.0149 (0.0149)  time: 0.6523  data: 0.0681  max mem: 8521\nEpoch: [32]  [50/51]  eta: 0:00:00  lr: 0.000000  loss: 0.3440 (0.3308)  loss_classifier: 0.1454 (0.1374)  loss_box_reg: 0.1845 (0.1813)  loss_objectness: 0.0011 (0.0013)  loss_rpn_box_reg: 0.0089 (0.0107)  time: 0.6707  data: 0.0685  max mem: 8521\nEpoch: [32] Total time: 0:00:34 (0.6806 s / it)\nEpoch: [33]  [ 0/51]  eta: 0:00:33  lr: 0.000000  loss: 0.2617 (0.2617)  loss_classifier: 0.1137 (0.1137)  loss_box_reg: 0.1315 (0.1315)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0147 (0.0147)  time: 0.6492  data: 0.0689  max mem: 8521\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from torchvision.ops import nms\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084534_mp4-40_jpg.rf.796cb1b5e172d63df1c6116c2380cf19.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    x = x.to(device)\n    \n    predictions = model([x, ])\n    print(predictions)\n    pred = predictions[0]\n    print(pred)\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.1).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\nprint(pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084534_mp4-40_jpg.rf.796cb1b5e172d63df1c6116c2380cf19.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    pred['boxes'][..., [0, 2]] -= pred['boxes'][..., [2, 0]].diff(axis=1)/2\n    pred['boxes'][..., [1, 3]] -= pred['boxes'][..., [3, 1]].diff(axis=1)/2\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n#image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/20220708_084920_mp4-131_jpg.rf.ff5859b06e7f5a4e5e3a57aa3b5b436d.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/bee-detection-dataset/test/images/BDD810C5-1B2A-4A40-91B9-B872748EABFB_mov-34_jpg.rf.86f65ad3a1d0661c9d7b0f7a077cf4cd.jpg\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=6, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 60)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}