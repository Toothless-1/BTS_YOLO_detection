{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3600745,"sourceType":"datasetVersion","datasetId":2159203},{"sourceId":4566835,"sourceType":"datasetVersion","datasetId":2664679},{"sourceId":7422059,"sourceType":"datasetVersion","datasetId":4313880},{"sourceId":10287488,"sourceType":"datasetVersion","datasetId":6366491}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installs all of the libraries present in the 'offline-pytorch-2-1-2' dataset\n!pip install \\\n   --requirement /kaggle/input/offline-pytorch-2-1-2/requirements.txt \\\n   --no-index \\\n   --find-links file:///kaggle/input/offline-pytorch-2-1-2/wheels  \\\n--q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:57:39.105793Z","iopub.execute_input":"2024-12-26T20:57:39.106171Z","iopub.status.idle":"2024-12-26T20:59:05.462120Z","shell.execute_reply.started":"2024-12-26T20:57:39.106140Z","shell.execute_reply":"2024-12-26T20:59:05.460914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:05.463877Z","iopub.execute_input":"2024-12-26T20:59:05.464196Z","iopub.status.idle":"2024-12-26T20:59:06.359225Z","shell.execute_reply.started":"2024-12-26T20:59:05.464173Z","shell.execute_reply":"2024-12-26T20:59:06.358398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import koilerplate\nprint(f\"koilerplate: {koilerplate.__version__}\")\n\nimport torch\nprint(f\"torch: {torch.__version__}\")\n\nimport torchvision\nprint(f\"torchvision: {torchvision.__version__}\")\n\nimport torchaudio\nprint(f\"torchaudio: {torchaudio.__version__}\")\n\nimport torchdata\nprint(f\"torchdata: {torchdata.__version__}\")\n\nimport torchtext\nprint(f\"torchtext: {torchtext.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:06.360237Z","iopub.execute_input":"2024-12-26T20:59:06.360503Z","iopub.status.idle":"2024-12-26T20:59:11.589618Z","shell.execute_reply.started":"2024-12-26T20:59:06.360481Z","shell.execute_reply":"2024-12-26T20:59:11.588730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pycocotools --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-26T20:59:11.591827Z","iopub.execute_input":"2024-12-26T20:59:11.592760Z","iopub.status.idle":"2024-12-26T20:59:20.504681Z","shell.execute_reply.started":"2024-12-26T20:59:11.592734Z","shell.execute_reply":"2024-12-26T20:59:20.503685Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport pandas as pd # data processing\nimport numpy as np\n# Data Visulization libraries \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom torchvision.io import read_image\nfrom torchvision.tv_tensors import BoundingBoxes, Image\nfrom torchvision.transforms.v2 import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-12-26T20:59:20.506131Z","iopub.execute_input":"2024-12-26T20:59:20.506378Z","iopub.status.idle":"2024-12-26T20:59:20.899073Z","shell.execute_reply.started":"2024-12-26T20:59:20.506357Z","shell.execute_reply":"2024-12-26T20:59:20.898387Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files_dir = '/kaggle/input/bts-members-detection/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:20.900007Z","iopub.execute_input":"2024-12-26T20:59:20.900226Z","iopub.status.idle":"2024-12-26T20:59:20.904102Z","shell.execute_reply.started":"2024-12-26T20:59:20.900207Z","shell.execute_reply":"2024-12-26T20:59:20.903217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp1 = ['/'+image for image in sorted(os.listdir(files_dir))\n                        if (image[-4:]=='.png') and (image[:-4]+'.txt' in os.listdir(files_dir))\n         and os.path.getsize(files_dir+'/'+image[:-4]+'.txt') != 0]\ntemp2 = ['/'+annot for annot in sorted(os.listdir(files_dir))\n                        if (annot[-4:]=='.txt') and os.path.getsize(files_dir+'/'+annot) != 0]\n\nimages = pd.Series(temp1, name='images')\nimage_id = pd.Series(list(range(len(temp1))), name='id')\ntrain_img_df = pd.DataFrame(pd.concat([images, image_id], axis=1))\nimages = []\nimage_id = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for j in range(len(file.readlines())):\n            images.append(temp1[i])\n            image_id.append(i)\n        file.close()\nbboxes = []\nfor i in range(len(temp1)):\n    with open(files_dir + temp2[i], 'r') as file:\n        for line in file.readlines():\n            bboxes.append(list(map(float, line.split())))\n        file.close()\nimages = pd.Series(images, name='images')\nbboxes = pd.Series(bboxes, name='bboxes')\nimage_id = pd.Series(image_id, name='image_id')\nind = pd.Series(list(range(len(images))), name='id')\ndf = pd.concat([images, ind,image_id,bboxes], axis=1)\ntrain_df = pd.DataFrame(df)\narea = []\nfor i in range(train_df.shape[0]):\n    img_path = files_dir + train_df.iloc[i,0]\n    img = read_image(img_path)\n    area.append(train_df.iloc[i,3][3]*train_df.iloc[i,3][4])\ntrain_df = pd.concat([train_df, pd.Series(area, name='area')],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:20.905231Z","iopub.execute_input":"2024-12-26T20:59:20.905466Z","iopub.status.idle":"2024-12-26T20:59:31.552962Z","shell.execute_reply.started":"2024-12-26T20:59:20.905446Z","shell.execute_reply":"2024-12-26T20:59:31.551968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:31.554063Z","iopub.execute_input":"2024-12-26T20:59:31.554345Z","iopub.status.idle":"2024-12-26T20:59:31.572029Z","shell.execute_reply.started":"2024-12-26T20:59:31.554322Z","shell.execute_reply":"2024-12-26T20:59:31.571089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape, train_img_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:31.573149Z","iopub.execute_input":"2024-12-26T20:59:31.573478Z","iopub.status.idle":"2024-12-26T20:59:31.578850Z","shell.execute_reply.started":"2024-12-26T20:59:31.573450Z","shell.execute_reply":"2024-12-26T20:59:31.578028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:31.581518Z","iopub.execute_input":"2024-12-26T20:59:31.581772Z","iopub.status.idle":"2024-12-26T20:59:31.743609Z","shell.execute_reply.started":"2024-12-26T20:59:31.581752Z","shell.execute_reply":"2024-12-26T20:59:31.742934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train, valid, train_img, valid_img = train_df.iloc[:int(809*0.6), :], train_df.iloc[int(809*0.6):, :], train_img_df.iloc[:int(809*0.6), :], train_img_df.iloc[int(809*0.6):, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:31.744617Z","iopub.execute_input":"2024-12-26T20:59:31.744940Z","iopub.status.idle":"2024-12-26T20:59:31.749262Z","shell.execute_reply.started":"2024-12-26T20:59:31.744891Z","shell.execute_reply":"2024-12-26T20:59:31.748467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 0 - Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\nclass BTSDataset(torch.utils.data.Dataset):\n    def __init__(self, root, images_dataset, boxes_dataset, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.dataset = images_dataset\n        self.boxes_dataset = boxes_dataset\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = self.root + self.dataset.query('id == @idx').images[idx]\n        boxes_query = self.boxes_dataset.query('image_id == @idx')\n        boxes_list = list(boxes_query.bboxes.iloc[i][1:] for i in range(boxes_query.bboxes.shape[0]))\n        labels_list = list(boxes_query.bboxes.iloc[i][0] + 1 for i in range(boxes_query.bboxes.shape[0]))\n        img = read_image(img_path)\n        boxes = torch.tensor(boxes_list)\n        #print(img_path)\n        for box in boxes:\n            box[0], box[1], box[2], box[3] = \\\n            box[0]*F.get_size(img)[1], box[1]*F.get_size(img)[0], \\\n            box[2]*F.get_size(img)[1], box[3]*F.get_size(img)[0]\n        boxes = torchvision.ops.box_convert(boxes, 'cxcywh', 'xyxy')\n        labels = torch.tensor(labels_list, dtype=torch.int64)\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        \n        # Wrap sample and targets into torchvision tv_tensors:\n        img = Image(img)\n\n        target = {}\n        target[\"boxes\"] = BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        return img, target\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:31.750504Z","iopub.execute_input":"2024-12-26T20:59:31.751219Z","iopub.status.idle":"2024-12-26T20:59:31.761396Z","shell.execute_reply.started":"2024-12-26T20:59:31.751188Z","shell.execute_reply":"2024-12-26T20:59:31.760620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 - Modifying the model to add a different backbone","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import v2 as T\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:31.762441Z","iopub.execute_input":"2024-12-26T20:59:31.762748Z","iopub.status.idle":"2024-12-26T20:59:31.777041Z","shell.execute_reply.started":"2024-12-26T20:59:31.762721Z","shell.execute_reply":"2024-12-26T20:59:31.776411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.utils as utils\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom engine import train_one_epoch, evaluate\n\ndef my_collate(batch):\n    data = [item[0] for item in batch]\n    target = [item[1] for item in batch]\n    return [data, target]\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n# our dataset has two classes only - background and person\nnum_classes = 7+1\n# use our dataset and defined transformations\nimages_dataset = train_img_df\nboxes_dataset = train_df\ndataset = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=images_dataset, \n                     boxes_dataset=boxes_dataset, \n                     transforms=get_transform(train=True))\n'''dataset_test = BTSDataset(root='/kaggle/input/bts-members-detection/images',\n                     images_dataset=valid_img, \n                     boxes_dataset=valid, \n                     transforms=get_transform(train=False))'''\n\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=12,\n    shuffle=True,\n    collate_fn=my_collate\n)\n\n'''data_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=my_collate\n)'''\n\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(\n    params,\n    lr=1e-2,\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=7,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    #evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T20:59:50.463414Z","iopub.execute_input":"2024-12-26T20:59:50.463746Z","iopub.status.idle":"2024-12-26T21:36:33.691038Z","shell.execute_reply.started":"2024-12-26T20:59:50.463720Z","shell.execute_reply":"2024-12-26T21:36:33.690065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.ops import nms\n\npath_img = \"/kaggle/input/bts-members-detection/images/jhope116.png\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    x = x.to(device)\n    \n    predictions = model([x, ])\n    print(predictions)\n    pred = predictions[0]\n    print(pred)\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.1).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\nprint(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T21:36:33.692453Z","iopub.execute_input":"2024-12-26T21:36:33.692717Z","iopub.status.idle":"2024-12-26T21:36:33.786043Z","shell.execute_reply.started":"2024-12-26T21:36:33.692696Z","shell.execute_reply":"2024-12-26T21:36:33.785214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"names = sorted(['jhope', 'jimin', 'jin', 'suga', 'jungkook', 'rm', 'v'])\nclass_names = dict((i, names[i]) for i in range(7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T21:36:33.786965Z","iopub.execute_input":"2024-12-26T21:36:33.787204Z","iopub.status.idle":"2024-12-26T21:36:33.791432Z","shell.execute_reply.started":"2024-12-26T21:36:33.787184Z","shell.execute_reply":"2024-12-26T21:36:33.790570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T21:36:33.792896Z","iopub.execute_input":"2024-12-26T21:36:33.793161Z","iopub.status.idle":"2024-12-26T21:36:33.801037Z","shell.execute_reply.started":"2024-12-26T21:36:33.793141Z","shell.execute_reply":"2024-12-26T21:36:33.800262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/korean-band-bts-members-face-recognition/images/suga/suga46.png\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    '''pred['boxes'][..., [0, 2]] -= pred['boxes'][..., [2, 0]].diff(axis=1)/2\n    pred['boxes'][..., [1, 3]] -= pred['boxes'][..., [3, 1]].diff(axis=1)/2'''\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.01).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n#image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=3, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 20)\n\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T21:36:33.801861Z","iopub.execute_input":"2024-12-26T21:36:33.802130Z","iopub.status.idle":"2024-12-26T21:36:34.244013Z","shell.execute_reply.started":"2024-12-26T21:36:33.802109Z","shell.execute_reply":"2024-12-26T21:36:34.243187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\npath_img = \"/kaggle/input/korean-band-bts-members-face-recognition/images/jin/jin99.png\"\nimage = read_image(path_img)\neval_transform = get_transform(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -> RGB and move to device\n    x = x[:3, ...].to(device)\n    predictions = model([x, ])\n    pred = predictions[0]\n    print(pred)\n    result = nms(pred['boxes'], pred['scores'], iou_threshold=0.1).to('cpu')\n    pred = {'labels': pred['labels'][result], \n            'scores': pred['scores'][result], \n            'boxes': pred['boxes'][result, :]}\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\",width=3, \\\n                                   font = '../input/synth-indic-custom-resources/SYNTH_INDIC/fonts/english/English.ttf',\\\n                                   font_size = 20)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T21:38:12.427725Z","iopub.execute_input":"2024-12-26T21:38:12.428642Z","iopub.status.idle":"2024-12-26T21:38:12.790801Z","shell.execute_reply.started":"2024-12-26T21:38:12.428601Z","shell.execute_reply":"2024-12-26T21:38:12.789945Z"}},"outputs":[],"execution_count":null}]}